---
File: tests/__init__.py
---
# tests/__init__.py
"""LLM Archive test suite."""



---
File: tests/conftest.py
---
# tests/conftest.py
"""Root pytest configuration."""

import pytest


def pytest_configure(config):
    """Configure pytest markers."""
    config.addinivalue_line(
        "markers", "integration: marks tests as requiring database (deselect with '-m \"not integration\"')"
    )


def pytest_collection_modifyitems(config, items):
    """Automatically mark tests in integration folder."""
    for item in items:
        if "integration" in str(item.fspath):
            item.add_marker(pytest.mark.integration)



---
File: tests/integration/__init__.py
---




---
File: tests/integration/conftest.py
---
# tests/integration/conftest.py
"""Pytest configuration for integration tests - CORRECTED FIXTURES."""

import os
from typing import Generator

import pytest
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine
from sqlalchemy.orm import Session, sessionmaker


def get_test_db_url() -> str:
    """Get test database URL from environment."""
    url = os.getenv('TEST_DATABASE_URL', 'postgresql://localhost:5432/llm_archive_test')
    return url


@pytest.fixture(scope="session")
def db_engine() -> Generator[Engine, None, None]:
    """Create database engine for tests."""
    url = get_test_db_url()
    engine = create_engine(url, echo=False)
    yield engine
    engine.dispose()


@pytest.fixture(scope="session")
def setup_schemas(db_engine):
    """Initialize schemas once per test session."""
    from pathlib import Path
    
    # Find schema directory relative to this file
    tests_dir = Path(__file__).parent.parent
    project_dir = tests_dir.parent
    schema_dir = project_dir / "schema"
    
    with db_engine.connect() as conn:
        # Drop and recreate schemas
        conn.execute(text("DROP SCHEMA IF EXISTS derived CASCADE"))
        conn.execute(text("DROP SCHEMA IF EXISTS raw CASCADE"))
        conn.commit()
        
        # Execute schema files in order
        for sql_file in sorted(schema_dir.glob("*.sql")):
            print(f"Executing {sql_file.name}")
            sql = sql_file.read_text()
            
            try:
                conn.execute(text(sql))
                conn.commit()
            except Exception as e:
                if "already exists" in str(e).lower():
                    conn.rollback()
                    print(f"Note: {sql_file.name} - {e}")
                else:
                    print(f"ERROR in {sql_file.name}: {e}")
                    conn.rollback()
                    raise
    
    yield
    
    # Cleanup
    with db_engine.connect() as conn:
        conn.execute(text("DROP SCHEMA IF EXISTS derived CASCADE"))
        conn.execute(text("DROP SCHEMA IF EXISTS raw CASCADE"))
        conn.commit()


@pytest.fixture
def db_session(db_engine, setup_schemas) -> Generator[Session, None, None]:
    """Create a database session with transaction rollback."""
    connection = db_engine.connect()
    transaction = connection.begin()
    SessionFactory = sessionmaker(bind=connection)
    session = SessionFactory()
    
    yield session
    
    session.close()
    transaction.rollback()
    connection.close()


@pytest.fixture
def clean_db_session(db_session) -> Session:
    """Alias for db_session."""
    return db_session


# ============================================================
# ChatGPT Test Fixtures
# ============================================================

@pytest.fixture
def chatgpt_simple_conversation() -> dict:
    """Simple linear ChatGPT conversation."""
    return {
        "conversation_id": "conv-simple-001",
        "title": "Simple Test Conversation",
        "create_time": 1700000000.0,
        "update_time": 1700001000.0,
        "mapping": {
            "root": {
                "id": "root",
                "parent": None,
                "children": ["node-1"],
                "message": None,
            },
            "node-1": {
                "id": "node-1",
                "parent": "root",
                "children": ["node-2"],
                "message": {
                    "id": "msg-1",
                    "author": {"role": "user"},
                    "create_time": 1700000100.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Hello, how are you?"]
                    }
                }
            },
            "node-2": {
                "id": "node-2",
                "parent": "node-1",
                "children": ["node-3"],
                "message": {
                    "id": "msg-2",
                    "author": {"role": "assistant"},
                    "create_time": 1700000200.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["I'm doing well, thank you!"]
                    }
                }
            },
            "node-3": {
                "id": "node-3",
                "parent": "node-2",
                "children": ["node-4"],
                "message": {
                    "id": "msg-3",
                    "author": {"role": "user"},
                    "create_time": 1700000300.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["What's the weather like?"]
                    }
                }
            },
            "node-4": {
                "id": "node-4",
                "parent": "node-3",
                "children": [],
                "message": {
                    "id": "msg-4",
                    "author": {"role": "assistant"},
                    "create_time": 1700000400.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["It's sunny and warm today."]
                    }
                }
            }
        }
    }


@pytest.fixture
def chatgpt_branched_conversation() -> dict:
    """ChatGPT conversation: 1 user message with 2 assistant responses that each have continuation messages."""
    return {
        "conversation_id": "conv-branched-001",
        "title": "Branched Test Conversation",
        "create_time": 1700000000.0,
        "update_time": 1700002000.0,
        "mapping": {
            "root": {
                "id": "root",
                "parent": None,
                "children": ["user1"],
                "message": None,
            },
            # THE ONLY USER MESSAGE - has 2 assistant children (regenerations)
            "user1": {
                "id": "user1",
                "parent": "root",
                "children": ["asst1a", "asst1b"],  # BRANCH POINT
                "message": {
                    "id": "msg-user1",
                    "author": {"role": "user"},
                    "create_time": 1700000100.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Tell me a story"]
                    }
                }
            },
            # First branch
            "asst1a": {
                "id": "asst1a",
                "parent": "user1",
                "children": ["asst2a"],
                "message": {
                    "id": "msg-asst1a",
                    "author": {"role": "assistant"},
                    "create_time": 1700000200.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Once upon a time..."]
                    }
                }
            },
            "asst2a": {
                "id": "asst2a",
                "parent": "asst1a",
                "children": [],
                "message": {
                    "id": "msg-asst2a",
                    "author": {"role": "assistant"},
                    "create_time": 1700000300.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["There was a brave knight..."]
                    }
                }
            },
            # Second branch  
            "asst1b": {
                "id": "asst1b",
                "parent": "user1",
                "children": ["asst2b"],
                "message": {
                    "id": "msg-asst1b",
                    "author": {"role": "assistant"},
                    "create_time": 1700000250.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["In a galaxy far away..."]
                    }
                }
            },
            "asst2b": {
                "id": "asst2b",
                "parent": "asst1b",
                "children": [],
                "message": {
                    "id": "msg-asst2b",
                    "author": {"role": "assistant"},
                    "create_time": 1700000350.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["A spaceship landed..."]
                    }
                }
            }
        }
    }


@pytest.fixture
def chatgpt_conversation_with_code() -> dict:
    """ChatGPT conversation with code content - uses nested parts structure."""
    return {
        "conversation_id": "conv-code-001",
        "title": "Code Example",
        "create_time": 1700000000.0,
        "update_time": 1700001000.0,
        "mapping": {
            "root": {
                "id": "root",
                "parent": None,
                "children": ["node-1"],
                "message": None,
            },
            "node-1": {
                "id": "node-1",
                "parent": "root",
                "children": ["node-2"],
                "message": {
                    "id": "msg-1",
                    "author": {"role": "user"},
                    "create_time": 1700000100.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Write a Python function to calculate fibonacci numbers"]
                    }
                }
            },
            "node-2": {
                "id": "node-2",
                "parent": "node-1",
                "children": [],
                "message": {
                    "id": "msg-2",
                    "author": {"role": "assistant"},
                    "create_time": 1700000200.0,
                    "content": {
                        "content_type": "text",
                        "parts": [
                            "Here's a Python function:",
                            {
                                "content_type": "code",
                                "language": "python",
                                "text": "def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)"
                            },
                            "This is a recursive implementation."
                        ]
                    }
                }
            }
        }
    }


@pytest.fixture
def chatgpt_conversation_with_image() -> dict:
    """ChatGPT conversation with image content - uses nested parts structure."""
    return {
        "conversation_id": "conv-image-001",
        "title": "Image Example",
        "create_time": 1700000000.0,
        "update_time": 1700001000.0,
        "mapping": {
            "root": {
                "id": "root",
                "parent": None,
                "children": ["node-1"],
                "message": None,
            },
            "node-1": {
                "id": "node-1",
                "parent": "root",
                "children": ["node-2"],
                "message": {
                    "id": "msg-1",
                    "author": {"role": "user"},
                    "create_time": 1700000100.0,
                    "content": {
                        "content_type": "multimodal_text",
                        "parts": [
                            "What's in this image?",
                            {
                                "content_type": "image/png",
                                "asset_pointer": "file-service://dalle-gen-abc123"
                            }
                        ]
                    }
                }
            },
            "node-2": {
                "id": "node-2",
                "parent": "node-1",
                "children": [],
                "message": {
                    "id": "msg-2",
                    "author": {"role": "assistant"},
                    "create_time": 1700000200.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["This image shows a cat."]
                    }
                }
            }
        }
    }


@pytest.fixture
def chatgpt_conversations(
    chatgpt_simple_conversation,
    chatgpt_branched_conversation,
) -> list[dict]:
    """List of ChatGPT test conversations."""
    third_conversation = {
        "conversation_id": "conv-third-001",
        "title": "Third Conversation",
        "create_time": 1700003000.0,
        "update_time": 1700003000.0,
        "mapping": {
            "root": {
                "id": "root",
                "parent": None,
                "children": ["node-1"],
                "message": None,
            },
            "node-1": {
                "id": "node-1",
                "parent": "root",
                "children": [],
                "message": {
                    "id": "msg-third-1",
                    "author": {"role": "user"},
                    "create_time": 1700003100.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Hello"]
                    }
                }
            }
        }
    }
    return [
        chatgpt_simple_conversation,
        chatgpt_branched_conversation,
        third_conversation,
    ]


# ============================================================
# Claude Test Fixtures
# ============================================================

@pytest.fixture
def claude_simple_conversation() -> dict:
    """Simple Claude conversation."""
    return {
        "uuid": "claude-conv-001",
        "name": "Claude Test Conversation",
        "created_at": "2024-01-15T10:00:00Z",
        "updated_at": "2024-01-15T10:30:00Z",
        "chat_messages": [
            {
                "uuid": "claude-msg-001",
                "sender": "human",
                "created_at": "2024-01-15T10:00:00Z",
                "content": [
                    {"type": "text", "text": "Hello Claude"}
                ]
            },
            {
                "uuid": "claude-msg-002",
                "sender": "assistant",
                "created_at": "2024-01-15T10:01:00Z",
                "content": [
                    {"type": "text", "text": "Hello! How can I help you today?"}
                ]
            },
            {
                "uuid": "claude-msg-003",
                "sender": "human",
                "created_at": "2024-01-15T10:05:00Z",
                "content": [
                    {"type": "text", "text": "What's 5 + 3?"}
                ]
            },
            {
                "uuid": "claude-msg-004",
                "sender": "assistant",
                "created_at": "2024-01-15T10:06:00Z",
                "content": [
                    {"type": "text", "text": "5 + 3 = 8"}
                ]
            }
        ]
    }


@pytest.fixture
def claude_conversation_with_thinking() -> dict:
    """Claude conversation with thinking blocks."""
    return {
        "uuid": "claude-conv-002",
        "name": "Claude Thinking Test",
        "created_at": "2024-01-15T11:00:00Z",
        "updated_at": "2024-01-15T11:05:00Z",
        "chat_messages": [
            {
                "uuid": "claude-msg-005",
                "sender": "human",
                "created_at": "2024-01-15T11:00:00Z",
                "content": [
                    {"type": "text", "text": "What is 15 * 23?"}
                ]
            },
            {
                "uuid": "claude-msg-006",
                "sender": "assistant",
                "created_at": "2024-01-15T11:01:00Z",
                "content": [
                    {"type": "thinking", "thinking": "Let me calculate: 15 * 23 = 345"},
                    {"type": "text", "text": "15 multiplied by 23 equals 345."}
                ]
            }
        ]
    }


@pytest.fixture
def claude_conversation_with_tool_use() -> dict:
    """Claude conversation with tool use."""
    return {
        "uuid": "claude-conv-003",
        "name": "Claude Tool Use Test",
        "created_at": "2024-01-15T12:00:00Z",
        "updated_at": "2024-01-15T12:10:00Z",
        "chat_messages": [
            {
                "uuid": "claude-msg-007",
                "sender": "human",
                "created_at": "2024-01-15T12:00:00Z",
                "content": [
                    {"type": "text", "text": "Search for recent news about AI."}
                ]
            },
            {
                "uuid": "claude-msg-008",
                "sender": "assistant",
                "created_at": "2024-01-15T12:01:00Z",
                "content": [
                    {
                        "type": "tool_use",
                        "id": "tool-001",
                        "name": "web_search",
                        "input": {"query": "recent AI news 2024"}
                    }
                ]
            },
            {
                "uuid": "claude-msg-009",
                "sender": "human",
                "created_at": "2024-01-15T12:02:00Z",
                "content": [
                    {
                        "type": "tool_result",
                        "tool_use_id": "tool-001",
                        "content": "AI advances in 2024 include..."
                    }
                ]
            },
            {
                "uuid": "claude-msg-010",
                "sender": "assistant",
                "created_at": "2024-01-15T12:03:00Z",
                "content": [
                    {"type": "text", "text": "Based on my search, here are the recent developments in AI..."}
                ]
            }
        ]
    }


@pytest.fixture
def claude_conversations(
    claude_simple_conversation,
    claude_conversation_with_thinking,
    claude_conversation_with_tool_use,
) -> list[dict]:
    """List of Claude test conversations."""
    return [
        claude_simple_conversation,
        claude_conversation_with_thinking,
        claude_conversation_with_tool_use,
    ]


# ============================================================
# Populated Database Fixtures
# ============================================================

@pytest.fixture
def populated_chatgpt_db(clean_db_session, chatgpt_simple_conversation):
    """Database with a single ChatGPT conversation imported."""
    from llm_archive.extractors import ChatGPTExtractor
    
    extractor = ChatGPTExtractor(clean_db_session)
    extractor.extract_dialogue(chatgpt_simple_conversation)
    clean_db_session.commit()
    
    return clean_db_session


@pytest.fixture
def populated_claude_db(clean_db_session, claude_simple_conversation):
    """Database with a single Claude conversation imported."""
    from llm_archive.extractors import ClaudeExtractor
    
    extractor = ClaudeExtractor(clean_db_session)
    extractor.extract_dialogue(claude_simple_conversation)
    clean_db_session.commit()
    
    return clean_db_session


@pytest.fixture
def fully_populated_db(
    clean_db_session,
    chatgpt_simple_conversation,
    chatgpt_branched_conversation,
    claude_simple_conversation,
):
    """Database with multiple conversations and derived data."""
    from llm_archive.extractors import ChatGPTExtractor, ClaudeExtractor
    from llm_archive.builders.prompt_response import PromptResponseBuilder
    
    chatgpt_extractor = ChatGPTExtractor(clean_db_session)
    chatgpt_extractor.extract_dialogue(chatgpt_simple_conversation)
    chatgpt_extractor.extract_dialogue(chatgpt_branched_conversation)
    
    claude_extractor = ClaudeExtractor(clean_db_session)
    claude_extractor.extract_dialogue(claude_simple_conversation)
    
    clean_db_session.commit()
    
    pr_builder = PromptResponseBuilder(clean_db_session)
    pr_builder.build_all()
    
    clean_db_session.commit()
    
    return clean_db_session



---
File: tests/integration/test_annotations.py
---
# tests/integration/test_annotations.py
"""Integration tests for typed annotation system."""

import pytest
from uuid import uuid4

from llm_archive.annotations.core import (
    AnnotationWriter,
    AnnotationReader,
    EntityType,
    ValueType,
    AnnotationResult,
)
from llm_archive.extractors.chatgpt import ChatGPTExtractor
from llm_archive.builders.prompt_response import PromptResponseBuilder
from llm_archive.annotators.prompt_response import (
    WikiCandidateAnnotator,
    NaiveTitleAnnotator,
)
from llm_archive.models import Message, PromptResponse


class TestAnnotationWriterIntegration:
    """Integration tests for AnnotationWriter."""
    
    def test_write_flag_creates_record(self, clean_db_session, chatgpt_simple_conversation):
        """Writing a flag creates a record in flag table."""
        # Get a real message ID
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        writer = AnnotationWriter(clean_db_session)
        result = writer.write_flag(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='test_flag',
            source='test',
        )
        clean_db_session.commit()
        
        assert result is True
        
        # Verify record exists
        reader = AnnotationReader(clean_db_session)
        assert reader.has_flag(EntityType.MESSAGE, message.id, 'test_flag')
    
    def test_write_string_creates_record(self, clean_db_session, chatgpt_simple_conversation):
        """Writing a string creates a record in string table."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        writer = AnnotationWriter(clean_db_session)
        result = writer.write_string(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='category',
            value='greeting',
            source='test',
        )
        clean_db_session.commit()
        
        assert result is True
        
        reader = AnnotationReader(clean_db_session)
        values = reader.get_string(EntityType.MESSAGE, message.id, 'category')
        assert 'greeting' in values
    
    def test_write_numeric_creates_record(self, clean_db_session, chatgpt_simple_conversation):
        """Writing a numeric creates a record in numeric table."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        writer = AnnotationWriter(clean_db_session)
        result = writer.write_numeric(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='word_count',
            value=42,
            source='test',
        )
        clean_db_session.commit()
        
        assert result is True
        
        reader = AnnotationReader(clean_db_session)
        values = reader.get_numeric(EntityType.MESSAGE, message.id, 'word_count')
        assert 42 in values
    
    def test_write_json_creates_record(self, clean_db_session, chatgpt_simple_conversation):
        """Writing JSON creates a record in json table."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        writer = AnnotationWriter(clean_db_session)
        result = writer.write_json(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='metadata',
            value={'tags': ['test', 'example'], 'score': 0.95},
            source='test',
        )
        clean_db_session.commit()
        
        assert result is True
        
        reader = AnnotationReader(clean_db_session)
        value = reader.get_json(EntityType.MESSAGE, message.id, 'metadata')
        assert value == {'tags': ['test', 'example'], 'score': 0.95}
    
    def test_write_duplicate_flag_returns_false(self, clean_db_session, chatgpt_simple_conversation):
        """Writing duplicate flag returns False (no new record)."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        writer = AnnotationWriter(clean_db_session)
        
        # First write succeeds
        result1 = writer.write_flag(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='test_flag',
            source='test',
        )
        clean_db_session.commit()
        assert result1 is True
        
        # Duplicate returns False
        result2 = writer.write_flag(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='test_flag',
            source='test',
        )
        assert result2 is False
    
    def test_write_multi_value_string(self, clean_db_session, chatgpt_simple_conversation):
        """Can write multiple values for same string key."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        writer = AnnotationWriter(clean_db_session)
        writer.write_string(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='tag',
            value='coding',
            source='test',
        )
        writer.write_string(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='tag',
            value='python',
            source='test',
        )
        clean_db_session.commit()
        
        reader = AnnotationReader(clean_db_session)
        values = reader.get_string(EntityType.MESSAGE, message.id, 'tag')
        assert set(values) == {'coding', 'python'}
    
    def test_write_from_annotation_result(self, clean_db_session, chatgpt_simple_conversation):
        """Can write from AnnotationResult object."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        result = AnnotationResult(
            key='exchange_type',
            value='wiki_article',
            value_type=ValueType.STRING,
            confidence=0.9,
            reason='wiki_links_detected',
        )
        
        writer = AnnotationWriter(clean_db_session)
        written = writer.write(EntityType.MESSAGE, message.id, result)
        clean_db_session.commit()
        
        assert written is True
        
        reader = AnnotationReader(clean_db_session)
        values = reader.get_string(EntityType.MESSAGE, message.id, 'exchange_type')
        assert 'wiki_article' in values


class TestAnnotationReaderIntegration:
    """Integration tests for AnnotationReader."""
    
    def test_find_entities_with_flag(self, clean_db_session, chatgpt_simple_conversation):
        """Can find all entities with a specific flag."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        messages = clean_db_session.query(Message).all()
        assert len(messages) >= 2
        
        writer = AnnotationWriter(clean_db_session)
        
        # Flag first two messages with 'has_code'
        writer.write_flag(EntityType.MESSAGE, messages[0].id, 'has_code', source='test')
        writer.write_flag(EntityType.MESSAGE, messages[1].id, 'has_code', source='test')
        
        # Flag third message with something else (if exists)
        if len(messages) > 2:
            writer.write_flag(EntityType.MESSAGE, messages[2].id, 'has_attachment', source='test')
        
        clean_db_session.commit()
        
        reader = AnnotationReader(clean_db_session)
        results = reader.find_entities_with_flag(EntityType.MESSAGE, 'has_code')
        
        assert messages[0].id in results
        assert messages[1].id in results
        if len(messages) > 2:
            assert messages[2].id not in results
    
    def test_find_entities_with_string_value(self, clean_db_session, chatgpt_simple_conversation):
        """Can find entities with specific string value."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        messages = clean_db_session.query(Message).all()
        
        writer = AnnotationWriter(clean_db_session)
        writer.write_string(EntityType.MESSAGE, messages[0].id, 'topic', 'coding', source='test')
        writer.write_string(EntityType.MESSAGE, messages[1].id, 'topic', 'general', source='test')
        clean_db_session.commit()
        
        reader = AnnotationReader(clean_db_session)
        
        # Find by specific value
        coding_results = reader.find_entities_with_string(EntityType.MESSAGE, 'topic', 'coding')
        assert messages[0].id in coding_results
        assert messages[1].id not in coding_results
        
        # Find by key only (any value)
        all_results = reader.find_entities_with_string(EntityType.MESSAGE, 'topic', None)
        assert messages[0].id in all_results
        assert messages[1].id in all_results
    
    def test_get_all_keys(self, clean_db_session, chatgpt_simple_conversation):
        """Can get all annotations for an entity."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        writer = AnnotationWriter(clean_db_session)
        writer.write_flag(EntityType.MESSAGE, message.id, 'has_code', source='test')
        writer.write_string(EntityType.MESSAGE, message.id, 'language', 'python', source='test')
        writer.write_numeric(EntityType.MESSAGE, message.id, 'line_count', 50, source='test')
        clean_db_session.commit()
        
        reader = AnnotationReader(clean_db_session)
        all_keys = reader.get_all_keys(EntityType.MESSAGE, message.id)
        
        assert 'has_code' in all_keys
        assert 'language' in all_keys
        assert 'line_count' in all_keys


class TestPromptResponseAnnotatorIntegration:
    """Integration tests for prompt-response annotators."""
    
    @pytest.fixture
    def wiki_conversation(self):
        """Conversation with wiki-style content."""
        return {
            'conversation_id': 'conv-wiki',
            'title': 'Wiki Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-user': {
                    'id': 'node-user',
                    'message': {
                        'id': 'msg-user',
                        'author': {'role': 'user'},
                        'content': {'parts': ['Write about cats']},
                        'create_time': 1700000000,
                    },
                    'parent': None,
                },
                'node-asst': {
                    'id': 'node-asst',
                    'message': {
                        'id': 'msg-asst',
                        'author': {'role': 'assistant'},
                        'content': {'parts': [
                            '# The Domestic Cat\n\n'
                            '[[Cats]] are [[mammals]] that belong to the family [[Felidae]]. '
                            'They are known for their [[hunting]] abilities.'
                        ]},
                        'create_time': 1700000001,
                    },
                    'parent': 'node-user',
                },
            },
        }
    
    def test_wiki_candidate_annotator_end_to_end(self, clean_db_session, wiki_conversation):
        """Test WikiCandidateAnnotator with real database."""
        # Import and build
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(wiki_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        builder.build_all()
        clean_db_session.commit()
        
        # Run annotator
        annotator = WikiCandidateAnnotator(clean_db_session)
        count = annotator.compute()
        clean_db_session.commit()
        
        assert count > 0
        
        # Verify annotations exist
        reader = AnnotationReader(clean_db_session)
        pr = clean_db_session.query(PromptResponse).first()
        
        values = reader.get_string(EntityType.PROMPT_RESPONSE, pr.id, 'exchange_type')
        assert 'wiki_article' in values
        
        counts = reader.get_numeric(EntityType.PROMPT_RESPONSE, pr.id, 'wiki_link_count')
        assert len(counts) > 0
        assert counts[0] >= 4  # At least 4 wiki links in our test data
    
    def test_naive_title_annotator_end_to_end(self, clean_db_session, wiki_conversation):
        """Test NaiveTitleAnnotator with real database."""
        # Import and build
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(wiki_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        builder.build_all()
        clean_db_session.commit()
        
        # Run wiki candidate annotator first (prerequisite)
        wiki_annotator = WikiCandidateAnnotator(clean_db_session)
        wiki_annotator.compute()
        clean_db_session.commit()
        
        # Run title annotator
        title_annotator = NaiveTitleAnnotator(clean_db_session)
        count = title_annotator.compute()
        clean_db_session.commit()
        
        assert count > 0
        
        # Verify title was extracted
        reader = AnnotationReader(clean_db_session)
        pr = clean_db_session.query(PromptResponse).first()
        
        values = reader.get_string(EntityType.PROMPT_RESPONSE, pr.id, 'proposed_title')
        assert 'The Domestic Cat' in values
    
    def test_annotator_prerequisite_filtering(self, clean_db_session, chatgpt_simple_conversation):
        """Test that NaiveTitleAnnotator respects REQUIRES_STRINGS."""
        # Import conversation that won't be marked as wiki
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        builder.build_all()
        clean_db_session.commit()
        
        # Skip wiki annotator - no wiki_article annotations will exist
        
        # Run title annotator
        title_annotator = NaiveTitleAnnotator(clean_db_session)
        count = title_annotator.compute()
        clean_db_session.commit()
        
        # Should process nothing because prerequisite not met
        assert count == 0


class TestGizmoAnnotationIntegration:
    """Integration tests for gizmo annotation writing during extraction."""
    
    def test_gizmo_annotation_written_during_extraction(self, clean_db_session):
        """Test that gizmo_id is written as annotation during extraction."""
        conversation = {
            'conversation_id': 'conv-gizmo',
            'title': 'Gizmo Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': {
                        'id': 'msg-1',
                        'author': {'role': 'assistant'},
                        'content': {'parts': ['Response from custom GPT']},
                        'metadata': {
                            'gizmo_id': 'g-wiki-generator',
                            'model_slug': 'gpt-4',
                        },
                        'create_time': 1700000000,
                    },
                    'parent': None,
                },
            },
        }
        
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        reader = AnnotationReader(clean_db_session)
        
        # Check gizmo_id annotation
        gizmo_values = reader.get_string(EntityType.MESSAGE, message.id, 'gizmo_id')
        assert 'g-wiki-generator' in gizmo_values
        
        # Check has_gizmo flag
        assert reader.has_flag(EntityType.MESSAGE, message.id, 'has_gizmo')
        
        # Check model_slug annotation
        model_values = reader.get_string(EntityType.MESSAGE, message.id, 'model_slug')
        assert 'gpt-4' in model_values



---
File: tests/integration/test_extraction_diagnostics.py
---
# tests/integration/test_extraction_diagnostics.py
"""Diagnostic tests to understand what's happening during extraction."""

import pytest
from llm_archive.extractors import ChatGPTExtractor
from llm_archive.models import Message


def test_branched_conversation_diagnostic(db_session, chatgpt_branched_conversation):
    """Diagnostic test to see what's actually extracted."""
    extractor = ChatGPTExtractor(db_session)
    result = extractor.extract_dialogue(chatgpt_branched_conversation)
    
    print(f"\n=== EXTRACTION RESULT: {result} ===")
    
    # Get all messages
    messages = db_session.query(Message).order_by(Message.created_at).all()
    
    print(f"\n=== TOTAL MESSAGES: {len(messages)} ===")
    for i, msg in enumerate(messages):
        print(f"{i+1}. source_id={msg.source_id}, role={msg.role}, parent_id={msg.parent_id}")
    
    # Check the mapping structure
    mapping = chatgpt_branched_conversation['mapping']
    print(f"\n=== MAPPING STRUCTURE ===")
    for node_id, node in mapping.items():
        msg_data = node.get('message')
        if msg_data:
            msg_id = msg_data.get('id')
            role = msg_data.get('author', {}).get('role')
            parent_node = node.get('parent')
            children = node.get('children', [])
            print(f"Node: {node_id}, msg_id: {msg_id}, role: {role}, parent: {parent_node}, children: {children}")
    
    # Find the user message
    user_msg = db_session.query(Message).filter(Message.role == 'user').first()
    print(f"\n=== USER MESSAGE ===")
    print(f"source_id: {user_msg.source_id}")
    print(f"parent_id: {user_msg.parent_id}")
    
    # Find children
    children = db_session.query(Message).filter(Message.parent_id == user_msg.id).all()
    print(f"\n=== CHILDREN OF USER MESSAGE ===")
    print(f"Count: {len(children)}")
    for child in children:
        print(f"  - source_id={child.source_id}, role={child.role}")
    
    # Check if parent_id is set at all
    messages_with_parent = db_session.query(Message).filter(Message.parent_id.isnot(None)).all()
    print(f"\n=== MESSAGES WITH PARENT_ID SET ===")
    print(f"Count: {len(messages_with_parent)}")
    for msg in messages_with_parent:
        parent = db_session.get(Message, msg.parent_id)
        parent_source = parent.source_id if parent else "NOT FOUND"
        print(f"  - {msg.source_id} -> parent: {parent_source}")
    
    # The actual assertion
    assert len(children) == 2, f"Expected 2 children, found {len(children)}"


def test_simple_parent_child_relationship(db_session):
    """Test if parent-child relationships work at all."""
    simple_conv = {
        "conversation_id": "test-parent-child",
        "title": "Parent Child Test",
        "create_time": 1700000000.0,
        "update_time": 1700000000.0,
        "mapping": {
            "root": {
                "id": "root",
                "parent": None,
                "children": ["node1"],
                "message": None
            },
            "node1": {
                "id": "node1", 
                "parent": "root",
                "children": ["node2"],
                "message": {
                    "id": "msg1",
                    "author": {"role": "user"},
                    "create_time": 1700000100.0,
                    "content": {"content_type": "text", "parts": ["Hello"]}
                }
            },
            "node2": {
                "id": "node2",
                "parent": "node1",
                "children": [],
                "message": {
                    "id": "msg2",
                    "author": {"role": "assistant"},
                    "create_time": 1700000200.0,
                    "content": {"content_type": "text", "parts": ["Hi"]}
                }
            }
        }
    }
    
    extractor = ChatGPTExtractor(db_session)
    extractor.extract_dialogue(simple_conv)
    
    msg1 = db_session.query(Message).filter(Message.source_id == "msg1").first()
    msg2 = db_session.query(Message).filter(Message.source_id == "msg2").first()
    
    print(f"\n=== SIMPLE TEST ===")
    print(f"msg1 (user): source_id={msg1.source_id}, id={msg1.id}, parent_id={msg1.parent_id}")
    print(f"msg2 (asst): source_id={msg2.source_id}, id={msg2.id}, parent_id={msg2.parent_id}")
    
    # Check if msg2's parent_id points to msg1's id
    if msg2.parent_id:
        print(f"msg2.parent_id == msg1.id? {msg2.parent_id == msg1.id}")
    else:
        print("msg2.parent_id is None!")
    
    assert msg2.parent_id == msg1.id, "Child should have parent_id pointing to parent"


def test_branched_extraction_step_by_step(db_session):
    """Test extraction with a known branched structure."""
    conv = {
        "conversation_id": "test-branch",
        "title": "Branch Test",
        "create_time": 1700000000.0,
        "update_time": 1700000000.0,
        "mapping": {
            "root": {
                "id": "root",
                "parent": None,
                "children": ["user1"],
                "message": None
            },
            "user1": {
                "id": "user1",
                "parent": "root", 
                "children": ["asst1", "asst2"],  # TWO CHILDREN
                "message": {
                    "id": "msg-user1",
                    "author": {"role": "user"},
                    "create_time": 1700000100.0,
                    "content": {"content_type": "text", "parts": ["Question"]}
                }
            },
            "asst1": {
                "id": "asst1",
                "parent": "user1",
                "children": [],
                "message": {
                    "id": "msg-asst1",
                    "author": {"role": "assistant"},
                    "create_time": 1700000200.0,
                    "content": {"content_type": "text", "parts": ["Answer 1"]}
                }
            },
            "asst2": {
                "id": "asst2",
                "parent": "user1",
                "children": [],
                "message": {
                    "id": "msg-asst2",
                    "author": {"role": "assistant"},
                    "create_time": 1700000250.0,
                    "content": {"content_type": "text", "parts": ["Answer 2"]}
                }
            }
        }
    }
    
    extractor = ChatGPTExtractor(db_session)
    extractor.extract_dialogue(conv)
    
    user_msg = db_session.query(Message).filter(Message.source_id == "msg-user1").first()
    asst1 = db_session.query(Message).filter(Message.source_id == "msg-asst1").first()
    asst2 = db_session.query(Message).filter(Message.source_id == "msg-asst2").first()
    
    print(f"\n=== BRANCHED TEST ===")
    print(f"user: id={user_msg.id}, source_id={user_msg.source_id}")
    print(f"asst1: id={asst1.id}, parent_id={asst1.parent_id}")
    print(f"asst2: id={asst2.id}, parent_id={asst2.parent_id}")
    
    # Both assistants should have user as parent
    assert asst1.parent_id == user_msg.id, "asst1 parent should be user"
    assert asst2.parent_id == user_msg.id, "asst2 parent should be user"
    
    # User should have 2 children
    children = db_session.query(Message).filter(Message.parent_id == user_msg.id).all()
    print(f"Children count: {len(children)}")
    
    assert len(children) == 2, f"User should have 2 children, got {len(children)}"



---
File: tests/integration/test_extractors.py
---
# tests/integration/test_extractors.py
"""Tests for conversation extractors."""

import pytest
from uuid import UUID

from llm_archive.extractors import ChatGPTExtractor, ClaudeExtractor
from llm_archive.models import Dialogue, Message, ContentPart


class TestChatGPTExtractor:
    """Tests for ChatGPT extractor."""
    
    def test_extract_simple_conversation(self, db_session, chatgpt_simple_conversation):
        """Test extracting a simple linear conversation."""
        extractor = ChatGPTExtractor(db_session)
        result = extractor.extract_dialogue(chatgpt_simple_conversation)
        
        assert result == 'new'
        
        # Check dialogue was created
        dialogue = db_session.query(Dialogue).filter(
            Dialogue.source_id == "conv-simple-001"
        ).first()
        
        assert dialogue is not None
        assert dialogue.source == 'chatgpt'
        assert dialogue.title == "Simple Test Conversation"
    
    def test_extract_messages(self, db_session, chatgpt_simple_conversation):
        """Test that messages are extracted correctly."""
        extractor = ChatGPTExtractor(db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        
        dialogue = db_session.query(Dialogue).filter(
            Dialogue.source_id == "conv-simple-001"
        ).first()
        
        messages = db_session.query(Message).filter(
            Message.dialogue_id == dialogue.id
        ).all()
        
        # Should have 4 messages (excluding root node which has no message)
        assert len(messages) == 4
        
        # Check roles
        roles = sorted([m.role for m in messages])
        assert roles == ['assistant', 'assistant', 'user', 'user']
    
    def test_extract_content_parts(self, db_session, chatgpt_simple_conversation):
        """Test that content parts are extracted."""
        extractor = ChatGPTExtractor(db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        
        # Get a user message
        message = db_session.query(Message).filter(
            Message.role == 'user'
        ).first()
        
        parts = db_session.query(ContentPart).filter(
            ContentPart.message_id == message.id
        ).all()
        
        assert len(parts) >= 1
        assert parts[0].part_type == 'text'
        assert parts[0].text_content is not None
    
    def test_extract_branched_conversation(self, db_session, chatgpt_branched_conversation):
        """Test extracting a conversation with branches."""
        extractor = ChatGPTExtractor(db_session)
        result = extractor.extract_dialogue(chatgpt_branched_conversation)
        
        assert result == 'new'
        
        dialogue = db_session.query(Dialogue).filter(
            Dialogue.source_id == "conv-branched-001"
        ).first()
        
        messages = db_session.query(Message).filter(
            Message.dialogue_id == dialogue.id
        ).all()
        
        # Should have 5 messages (including both branches)
        assert len(messages) == 5
        
        # Check for branch point (message with multiple children)
        user_msg = db_session.query(Message).filter(
            Message.dialogue_id == dialogue.id,
            Message.role == 'user'
        ).first()
        
        # Count children
        children = db_session.query(Message).filter(
            Message.parent_id == user_msg.id
        ).all()
        
        # First user message should have 2 children (regeneration)
        assert len(children) == 2
    
    def test_extract_code_content(self, db_session, chatgpt_conversation_with_code):
        """Test extracting code execution content with language."""
        extractor = ChatGPTExtractor(db_session)
        extractor.extract_dialogue(chatgpt_conversation_with_code)
        
        dialogue = db_session.query(Dialogue).filter(
            Dialogue.source_id == "conv-code-001"
        ).first()
        
        assert dialogue is not None
        
        # Check for code content part with language
        code_parts = db_session.query(ContentPart).filter(
            ContentPart.part_type == 'code'
        ).all()
        
        assert len(code_parts) >= 1
        
        code_part = code_parts[0]
        assert code_part.language == 'python'
        assert 'fibonacci' in code_part.text_content
    
    def test_extract_image_content(self, db_session, chatgpt_conversation_with_image):
        """Test extracting image content with media type and URL."""
        extractor = ChatGPTExtractor(db_session)
        extractor.extract_dialogue(chatgpt_conversation_with_image)
        
        dialogue = db_session.query(Dialogue).filter(
            Dialogue.source_id == "conv-image-001"
        ).first()
        
        assert dialogue is not None
        
        # Check for image content part
        image_parts = db_session.query(ContentPart).filter(
            ContentPart.part_type == 'image'
        ).all()
        
        assert len(image_parts) >= 1
        
        image_part = image_parts[0]
        assert image_part.media_type == 'image/png'
        assert image_part.url is not None
        assert 'dalle-gen-abc123' in image_part.url or 'example.com' in image_part.url
    
    def test_missing_conversation_id(self, db_session):
        """Test handling of conversation without ID."""
        extractor = ChatGPTExtractor(db_session)
        result = extractor.extract_dialogue({"title": "No ID"})
        
        assert result is None
    
    def test_extract_all(self, db_session, chatgpt_conversations):
        """Test extracting multiple conversations."""
        extractor = ChatGPTExtractor(db_session)
        counts = extractor.extract_all(chatgpt_conversations)
        
        assert counts['dialogues_new'] == 3
        assert counts['failed'] == 0


class TestClaudeExtractor:
    """Tests for Claude extractor."""
    
    def test_extract_simple_conversation(self, db_session, claude_simple_conversation):
        """Test extracting a simple Claude conversation."""
        extractor = ClaudeExtractor(db_session)
        result = extractor.extract_dialogue(claude_simple_conversation)
        
        assert result == 'new'
        
        dialogue = db_session.query(Dialogue).filter(
            Dialogue.source_id == "claude-conv-001"
        ).first()
        
        assert dialogue is not None
        assert dialogue.source == 'claude'
        assert dialogue.title == "Claude Test Conversation"
    
    def test_extract_messages_linear(self, db_session, claude_simple_conversation):
        """Test that Claude messages form a linear chain."""
        extractor = ClaudeExtractor(db_session)
        extractor.extract_dialogue(claude_simple_conversation)
        
        dialogue = db_session.query(Dialogue).filter(
            Dialogue.source_id == "claude-conv-001"
        ).first()
        
        messages = db_session.query(Message).filter(
            Message.dialogue_id == dialogue.id
        ).order_by(Message.created_at).all()
        
        assert len(messages) == 4
        
        # Check linear structure
        for i in range(1, len(messages)):
            assert messages[i].parent_id == messages[i-1].id
    
    def test_role_normalization(self, db_session, claude_simple_conversation):
        """Test that 'human' role is normalized to 'user'."""
        extractor = ClaudeExtractor(db_session)
        extractor.extract_dialogue(claude_simple_conversation)
        
        messages = db_session.query(Message).all()
        roles = set(m.role for m in messages)
        
        assert 'human' not in roles
        assert 'user' in roles
        assert 'assistant' in roles
    
    def test_extract_thinking_blocks(self, db_session, claude_conversation_with_thinking):
        """Test extracting thinking content."""
        extractor = ClaudeExtractor(db_session)
        extractor.extract_dialogue(claude_conversation_with_thinking)
        
        # Check for thinking content part
        thinking_parts = db_session.query(ContentPart).filter(
            ContentPart.part_type == 'thinking'
        ).all()
        
        assert len(thinking_parts) >= 1
        assert thinking_parts[0].text_content is not None
    
    def test_extract_tool_use(self, db_session, claude_conversation_with_tool_use):
        """Test extracting tool use content with all fields."""
        extractor = ClaudeExtractor(db_session)
        extractor.extract_dialogue(claude_conversation_with_tool_use)
        
        tool_use_parts = db_session.query(ContentPart).filter(
            ContentPart.part_type == 'tool_use'
        ).all()
        
        assert len(tool_use_parts) >= 1
        
        # Verify tool use fields are extracted
        tool_use = tool_use_parts[0]
        assert tool_use.tool_name == 'web_search'
        assert tool_use.tool_use_id == 'tool-001'
        assert tool_use.tool_input == {'query': 'recent AI news 2024'}
        assert tool_use.text_content == 'recent AI news 2024'  # Extracted from input.query
    
    def test_extract_tool_result(self, db_session, claude_conversation_with_tool_use):
        """Test extracting tool result content with linked tool_use_id."""
        extractor = ClaudeExtractor(db_session)
        extractor.extract_dialogue(claude_conversation_with_tool_use)
        
        tool_result_parts = db_session.query(ContentPart).filter(
            ContentPart.part_type == 'tool_result'
        ).all()
        
        assert len(tool_result_parts) >= 1
        
        # Verify tool result fields
        tool_result = tool_result_parts[0]
        assert tool_result.tool_use_id == 'tool-001'  # Links back to tool_use
        assert tool_result.text_content == 'AI advances in 2024 include...'
    
    def test_missing_uuid(self, db_session):
        """Test handling of conversation without UUID."""
        extractor = ClaudeExtractor(db_session)
        result = extractor.extract_dialogue({"name": "No UUID"})
        
        assert result is None
    
    def test_extract_all(self, db_session, claude_conversations):
        """Test extracting multiple Claude conversations."""
        extractor = ClaudeExtractor(db_session)
        counts = extractor.extract_all(claude_conversations)
        
        assert counts['dialogues_new'] == 3
        assert counts['failed'] == 0


class TestExtractorTimestamps:
    """Tests for timestamp parsing."""
    
    def test_chatgpt_epoch_timestamps(self, db_session, chatgpt_simple_conversation):
        """Test parsing ChatGPT epoch timestamps."""
        extractor = ChatGPTExtractor(db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        
        dialogue = db_session.query(Dialogue).first()
        
        assert dialogue.created_at is not None
        assert dialogue.updated_at is not None
        assert dialogue.created_at.tzinfo is not None  # Timezone aware
    
    def test_claude_iso_timestamps(self, db_session, claude_simple_conversation):
        """Test parsing Claude ISO timestamps."""
        extractor = ClaudeExtractor(db_session)
        extractor.extract_dialogue(claude_simple_conversation)
        
        dialogue = db_session.query(Dialogue).first()
        
        assert dialogue.created_at is not None
        assert dialogue.updated_at is not None
        assert dialogue.created_at.tzinfo is not None  # Timezone aware



---
File: tests/integration/test_idempotency.py
---
# tests/integration/test_idempotency.py
"""Tests for idempotent import behavior with incremental updates."""

import copy
import pytest
from datetime import datetime, timezone

from llm_archive.extractors import ChatGPTExtractor, ClaudeExtractor
from llm_archive.models import Dialogue, Message


class TestChatGPTIdempotency:
    """Tests for ChatGPT idempotent import."""
    
    def test_reimport_unchanged_skips(self, clean_db_session, chatgpt_simple_conversation):
        """Test that reimporting unchanged conversation is skipped."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        result1 = extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        assert result1 == 'new'
        
        # Second import - same data
        result2 = extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        assert result2 == 'skipped'
        
        # Should still have only one dialogue
        count = clean_db_session.query(Dialogue).count()
        assert count == 1
    
    def test_reimport_updated_updates(self, clean_db_session, chatgpt_simple_conversation):
        """Test that reimporting updated conversation updates it."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        result1 = extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        assert result1 == 'new'
        
        original_title = clean_db_session.query(Dialogue).first().title
        
        # Modify and reimport
        updated = copy.deepcopy(chatgpt_simple_conversation)
        updated['update_time'] = 1700002000.0  # Later timestamp
        updated['title'] = "Updated Title"
        
        result2 = extractor.extract_dialogue(updated)
        clean_db_session.commit()
        assert result2 == 'updated'
        
        # Should still have only one dialogue
        count = clean_db_session.query(Dialogue).count()
        assert count == 1
        
        # Title should be updated
        dialogue = clean_db_session.query(Dialogue).first()
        assert dialogue.title == "Updated Title"
    
    def test_reimport_messages_refreshed(self, clean_db_session, chatgpt_simple_conversation):
        """Test that messages are refreshed on update."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_msg_count = clean_db_session.query(Message).count()
        
        # Modify and reimport
        updated = copy.deepcopy(chatgpt_simple_conversation)
        updated['update_time'] = 1700002000.0
        
        extractor.extract_dialogue(updated)
        clean_db_session.commit()
        
        # Message count should be the same (refreshed, not duplicated)
        new_msg_count = clean_db_session.query(Message).count()
        assert new_msg_count == original_msg_count
    
    def test_extract_all_mixed_results(self, clean_db_session, chatgpt_simple_conversation):
        """Test extract_all with mix of new, updated, and skipped."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import one conversation
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Create variations
        unchanged = copy.deepcopy(chatgpt_simple_conversation)
        
        updated = copy.deepcopy(chatgpt_simple_conversation)
        updated['update_time'] = 1700005000.0
        updated['title'] = "Updated"
        
        new_conv = copy.deepcopy(chatgpt_simple_conversation)
        new_conv['conversation_id'] = "conv-new-001"
        
        # Extract all
        counts = extractor.extract_all([unchanged, updated, new_conv])
        
        # Note: after the first was already imported, we have:
        # - unchanged: skipped (no update since update_time same)
        # - updated: but we already updated it above with unchanged, 
        #   so this depends on whether unchanged came first
        # Let's just check we have correct totals
        assert counts['dialogues_skipped'] + counts['dialogues_new'] + counts['dialogues_updated'] == 3


class TestClaudeIdempotency:
    """Tests for Claude idempotent import."""
    
    def test_reimport_unchanged_skips(self, clean_db_session, claude_simple_conversation):
        """Test that reimporting unchanged conversation is skipped."""
        extractor = ClaudeExtractor(clean_db_session)
        
        # First import
        result1 = extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        assert result1 == 'new'
        
        # Second import - same data
        result2 = extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        assert result2 == 'skipped'
    
    def test_reimport_updated_updates(self, clean_db_session, claude_simple_conversation):
        """Test that reimporting updated conversation updates it."""
        extractor = ClaudeExtractor(clean_db_session)
        
        # First import
        result1 = extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        
        # Modify and reimport
        updated = copy.deepcopy(claude_simple_conversation)
        updated['updated_at'] = "2024-01-16T10:00:00Z"  # Later timestamp
        updated['name'] = "Updated Title"
        
        result2 = extractor.extract_dialogue(updated)
        clean_db_session.commit()
        assert result2 == 'updated'
        
        # Title should be updated
        dialogue = clean_db_session.query(Dialogue).first()
        assert dialogue.title == "Updated Title"


class TestCrossSourceIdempotency:
    """Tests for idempotency across sources."""
    
    def test_same_content_different_sources(self, clean_db_session):
        """Test that same content from different sources creates separate records."""
        chatgpt_conv = {
            "conversation_id": "cross-001",
            "title": "Cross Source Test",
            "create_time": 1700000000.0,
            "update_time": 1700001000.0,
            "mapping": {
                "root": {"id": "root", "parent": None, "children": ["m1"], "message": None},
                "m1": {
                    "id": "m1",
                    "parent": "root",
                    "children": [],
                    "message": {
                        "id": "m1",
                        "author": {"role": "user"},
                        "create_time": 1700000100.0,
                        "content": {"content_type": "text", "parts": ["Hello"]}
                    }
                }
            }
        }
        
        claude_conv = {
            "uuid": "cross-001",  # Same ID!
            "name": "Cross Source Test",
            "created_at": "2024-01-15T10:00:00Z",
            "updated_at": "2024-01-15T10:30:00Z",
            "chat_messages": [
                {
                    "uuid": "msg-001",
                    "sender": "human",
                    "created_at": "2024-01-15T10:00:00Z",
                    "content": [{"type": "text", "text": "Hello"}]
                }
            ]
        }
        
        # Import both
        ChatGPTExtractor(clean_db_session).extract_dialogue(chatgpt_conv)
        ClaudeExtractor(clean_db_session).extract_dialogue(claude_conv)
        clean_db_session.commit()
        
        # Should have two dialogues (different sources)
        dialogues = clean_db_session.query(Dialogue).all()
        assert len(dialogues) == 2
        
        sources = set(d.source for d in dialogues)
        assert sources == {'chatgpt', 'claude'}


class TestPartialUpdate:
    """Tests for partial update scenarios."""
    
    def test_conversation_extended(self, clean_db_session, chatgpt_simple_conversation):
        """Test handling of conversation that has been extended."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # Import original
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_msg_count = clean_db_session.query(Message).count()
        
        # Extend conversation (add new messages)
        extended = copy.deepcopy(chatgpt_simple_conversation)
        extended['update_time'] = 1700005000.0
        
        # Add new message to mapping
        new_msg_id = "new-msg-001"
        last_msg_id = list(extended['mapping'].keys())[-1]
        
        extended['mapping'][new_msg_id] = {
            "id": new_msg_id,
            "parent": last_msg_id,
            "children": [],
            "message": {
                "id": new_msg_id,
                "author": {"role": "user"},
                "create_time": 1700004000.0,
                "content": {"content_type": "text", "parts": ["One more question"]}
            }
        }
        # Update parent's children
        for node_id, node in extended['mapping'].items():
            if node.get('message') and node['children'] == [] and node_id != new_msg_id:
                if node_id == last_msg_id:
                    node['children'] = [new_msg_id]
        
        # Reimport
        result = extractor.extract_dialogue(extended)
        clean_db_session.commit()
        
        assert result == 'updated'
        
        # Should have more messages now
        new_msg_count = clean_db_session.query(Message).count()
        assert new_msg_count == original_msg_count + 1


class TestUUIDPreservation:
    """Tests for message UUID preservation during updates."""
    
    def test_unchanged_messages_keep_uuids(self, clean_db_session, chatgpt_simple_conversation):
        """Test that unchanged messages keep their UUIDs."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Record original UUIDs
        original_messages = {m.source_id: m.id for m in clean_db_session.query(Message).all()}
        
        # Update with later timestamp but same content
        updated = copy.deepcopy(chatgpt_simple_conversation)
        updated['update_time'] = 1700005000.0
        updated['title'] = "New Title"  # Only title changed, not messages
        
        extractor.extract_dialogue(updated)
        clean_db_session.commit()
        
        # Check UUIDs are preserved
        new_messages = {m.source_id: m.id for m in clean_db_session.query(Message).all()}
        
        for source_id, original_uuid in original_messages.items():
            assert source_id in new_messages, f"Message {source_id} should still exist"
            assert new_messages[source_id] == original_uuid, f"Message {source_id} UUID changed"
    
    def test_changed_message_keeps_uuid_updates_content(self, clean_db_session, chatgpt_simple_conversation):
        """Test that changed messages keep their UUID but update content."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Find a message to modify
        first_user_msg = clean_db_session.query(Message).filter(
            Message.role == 'user'
        ).first()
        original_uuid = first_user_msg.id
        original_source_id = first_user_msg.source_id
        
        # Modify the message content
        updated = copy.deepcopy(chatgpt_simple_conversation)
        updated['update_time'] = 1700005000.0
        
        for node_id, node in updated['mapping'].items():
            msg = node.get('message')
            if msg and msg.get('id') == original_source_id:
                msg['content']['parts'] = ['MODIFIED MESSAGE CONTENT']
                break
        
        extractor.extract_dialogue(updated)
        clean_db_session.commit()
        
        # UUID should be preserved
        modified_msg = clean_db_session.query(Message).filter(
            Message.source_id == original_source_id
        ).first()
        
        assert modified_msg is not None
        assert modified_msg.id == original_uuid, "UUID should be preserved"
        assert 'MODIFIED' in str(modified_msg.source_json), "Content should be updated"
    
    def test_claude_unchanged_messages_keep_uuids(self, clean_db_session, claude_simple_conversation):
        """Test UUID preservation for Claude extractor."""
        extractor = ClaudeExtractor(clean_db_session)
        
        # First import
        extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        
        # Record original UUIDs
        original_messages = {m.source_id: m.id for m in clean_db_session.query(Message).all()}
        
        # Update with later timestamp but same messages
        updated = copy.deepcopy(claude_simple_conversation)
        updated['updated_at'] = "2024-01-20T10:00:00Z"
        updated['name'] = "New Title"
        
        extractor.extract_dialogue(updated)
        clean_db_session.commit()
        
        # Check UUIDs are preserved
        new_messages = {m.source_id: m.id for m in clean_db_session.query(Message).all()}
        
        for source_id, original_uuid in original_messages.items():
            assert source_id in new_messages
            assert new_messages[source_id] == original_uuid


class TestSoftDelete:
    """Tests for soft-delete behavior when messages are removed from source."""
    
    def test_removed_message_soft_deleted(self, clean_db_session, chatgpt_simple_conversation):
        """Test that messages removed from source are soft-deleted."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_count = clean_db_session.query(Message).count()
        
        # Remove a message from the conversation
        truncated = copy.deepcopy(chatgpt_simple_conversation)
        truncated['update_time'] = 1700005000.0
        
        # Remove the last message
        mapping_keys = list(truncated['mapping'].keys())
        last_msg_key = mapping_keys[-1]
        del truncated['mapping'][last_msg_key]
        
        # Update parent to not have children pointing to deleted msg
        for node_id, node in truncated['mapping'].items():
            if last_msg_key in node.get('children', []):
                node['children'].remove(last_msg_key)
        
        extractor.extract_dialogue(truncated)
        clean_db_session.commit()
        
        # Total message count should be the same (soft-deleted, not hard-deleted)
        total_count = clean_db_session.query(Message).count()
        assert total_count == original_count
        
        # Active message count should be one less
        active_count = clean_db_session.query(Message).filter(
            Message.deleted_at.is_(None)
        ).count()
        assert active_count == original_count - 1
        
        # Should have one soft-deleted message
        deleted_count = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).count()
        assert deleted_count == 1
    
    def test_soft_deleted_message_restored_on_reappear(self, clean_db_session, chatgpt_simple_conversation):
        """Test that soft-deleted message is restored if it reappears."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Remove a message
        truncated = copy.deepcopy(chatgpt_simple_conversation)
        truncated['update_time'] = 1700005000.0
        
        mapping_keys = list(truncated['mapping'].keys())
        removed_msg_key = mapping_keys[-1]
        del truncated['mapping'][removed_msg_key]
        
        for node_id, node in truncated['mapping'].items():
            if removed_msg_key in node.get('children', []):
                node['children'].remove(removed_msg_key)
        
        extractor.extract_dialogue(truncated)
        clean_db_session.commit()
        
        # Verify it's soft-deleted
        deleted_msg = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).first()
        assert deleted_msg is not None
        deleted_uuid = deleted_msg.id
        
        # Now restore by importing original again with newer timestamp
        restored = copy.deepcopy(chatgpt_simple_conversation)
        restored['update_time'] = 1700010000.0
        
        extractor.extract_dialogue(restored)
        clean_db_session.commit()
        
        # Message should be restored
        restored_msg = clean_db_session.query(Message).filter(
            Message.id == deleted_uuid
        ).first()
        
        assert restored_msg is not None
        assert restored_msg.deleted_at is None, "Message should be restored (deleted_at = None)"
    
    def test_content_hash_detects_changes(self, clean_db_session, chatgpt_simple_conversation):
        """Test that content hash correctly detects changed messages."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Get a message's content hash
        msg = clean_db_session.query(Message).filter(Message.role == 'user').first()
        original_hash = msg.content_hash
        
        assert original_hash is not None, "Content hash should be computed"
        
        # Modify the message content
        modified = copy.deepcopy(chatgpt_simple_conversation)
        modified['update_time'] = 1800000000.0  # Much later timestamp to ensure update
        
        for node_id, node in modified['mapping'].items():
            msg_data = node.get('message')
            if msg_data and msg_data.get('id') == msg.source_id:
                msg_data['content']['parts'] = ['Completely different content']
                break
        
        extractor.extract_dialogue(modified)
        clean_db_session.commit()
        
        # Hash should have changed
        clean_db_session.refresh(msg)
        assert msg.content_hash != original_hash, "Content hash should change when content changes"


class TestAssumeImmutableFlag:
    """Tests for assume_immutable optimization flag."""
    
    def test_immutable_mode_skips_hash_check(self, clean_db_session, chatgpt_simple_conversation):
        """Test that immutable mode skips content hash comparison."""
        extractor = ChatGPTExtractor(clean_db_session, assume_immutable=True)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Get a message
        msg = clean_db_session.query(Message).filter(Message.role == 'user').first()
        original_hash = msg.content_hash
        original_uuid = msg.id
        
        # "Modify" content in source (simulating what we'd do in mutable mode)
        # In immutable mode, this shouldn't trigger an update
        modified = copy.deepcopy(chatgpt_simple_conversation)
        modified['update_time'] = 1800000000.0
        
        for node_id, node in modified['mapping'].items():
            msg_data = node.get('message')
            if msg_data and msg_data.get('id') == msg.source_id:
                msg_data['content']['parts'] = ['MODIFIED CONTENT']
                break
        
        extractor.extract_dialogue(modified)
        clean_db_session.commit()
        
        # In immutable mode, hash should NOT change (we didn't check it)
        clean_db_session.refresh(msg)
        assert msg.id == original_uuid, "UUID should be preserved"
        assert msg.content_hash == original_hash, "Hash should NOT change in immutable mode"
    
    def test_mutable_mode_detects_changes(self, clean_db_session, chatgpt_simple_conversation):
        """Test that mutable mode (default) detects content changes."""
        extractor = ChatGPTExtractor(clean_db_session, assume_immutable=False)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Get a message
        msg = clean_db_session.query(Message).filter(Message.role == 'user').first()
        original_hash = msg.content_hash
        
        # Modify content
        modified = copy.deepcopy(chatgpt_simple_conversation)
        modified['update_time'] = 1800000000.0
        
        for node_id, node in modified['mapping'].items():
            msg_data = node.get('message')
            if msg_data and msg_data.get('id') == msg.source_id:
                msg_data['content']['parts'] = ['MODIFIED CONTENT']
                break
        
        extractor.extract_dialogue(modified)
        clean_db_session.commit()
        
        # In mutable mode, hash SHOULD change
        clean_db_session.refresh(msg)
        assert msg.content_hash != original_hash, "Hash SHOULD change in mutable mode"
    
    def test_immutable_mode_still_creates_new_messages(self, clean_db_session, chatgpt_simple_conversation):
        """Test that immutable mode still creates new messages properly."""
        extractor = ChatGPTExtractor(clean_db_session, assume_immutable=True)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_count = clean_db_session.query(Message).count()
        
        # Add a new message
        extended = copy.deepcopy(chatgpt_simple_conversation)
        extended['update_time'] = 1800000000.0
        
        new_msg_id = "new-immutable-msg"
        last_node_id = list(extended['mapping'].keys())[-1]
        
        extended['mapping'][new_msg_id] = {
            "id": new_msg_id,
            "parent": last_node_id,
            "children": [],
            "message": {
                "id": new_msg_id,
                "author": {"role": "user"},
                "create_time": 1700006000.0,
                "content": {"content_type": "text", "parts": ["New message"]}
            }
        }
        
        extractor.extract_dialogue(extended)
        clean_db_session.commit()
        
        # New message should be created
        new_count = clean_db_session.query(Message).count()
        assert new_count == original_count + 1
        
        # And it should have a content hash
        new_msg = clean_db_session.query(Message).filter(
            Message.source_id == new_msg_id
        ).first()
        assert new_msg is not None
        assert new_msg.content_hash is not None
    
    def test_immutable_mode_still_soft_deletes(self, clean_db_session, chatgpt_simple_conversation):
        """Test that immutable mode still soft-deletes removed messages."""
        extractor = ChatGPTExtractor(clean_db_session, assume_immutable=True)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Remove a message
        truncated = copy.deepcopy(chatgpt_simple_conversation)
        truncated['update_time'] = 1800000000.0
        
        mapping_keys = list(truncated['mapping'].keys())
        removed_key = mapping_keys[-1]
        del truncated['mapping'][removed_key]
        
        for node_id, node in truncated['mapping'].items():
            if removed_key in node.get('children', []):
                node['children'].remove(removed_key)
        
        extractor.extract_dialogue(truncated)
        clean_db_session.commit()
        
        # Message should be soft-deleted
        deleted_count = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).count()
        assert deleted_count == 1
    
    def test_immutable_mode_restores_soft_deleted(self, clean_db_session, chatgpt_simple_conversation):
        """Test that immutable mode restores soft-deleted messages without re-hashing."""
        extractor = ChatGPTExtractor(clean_db_session, assume_immutable=True)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Remove and soft-delete a message
        truncated = copy.deepcopy(chatgpt_simple_conversation)
        truncated['update_time'] = 1800000000.0
        
        mapping_keys = list(truncated['mapping'].keys())
        removed_key = mapping_keys[-1]
        del truncated['mapping'][removed_key]
        
        for node_id, node in truncated['mapping'].items():
            if removed_key in node.get('children', []):
                node['children'].remove(removed_key)
        
        extractor.extract_dialogue(truncated)
        clean_db_session.commit()
        
        # Verify soft-deleted
        deleted_msg = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).first()
        assert deleted_msg is not None
        original_hash = deleted_msg.content_hash
        
        # Restore by importing original with newer timestamp
        restored = copy.deepcopy(chatgpt_simple_conversation)
        restored['update_time'] = 1900000000.0
        
        extractor.extract_dialogue(restored)
        clean_db_session.commit()
        
        # Should be restored
        clean_db_session.refresh(deleted_msg)
        assert deleted_msg.deleted_at is None
        # Hash should be unchanged (we didn't re-hash)
        assert deleted_msg.content_hash == original_hash
    
    def test_claude_immutable_mode(self, clean_db_session, claude_simple_conversation):
        """Test that assume_immutable works for Claude extractor too."""
        extractor = ClaudeExtractor(clean_db_session, assume_immutable=True)
        
        # First import
        extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        
        # Get a message
        msg = clean_db_session.query(Message).filter(Message.role == 'user').first()
        original_hash = msg.content_hash
        
        # "Modify" content
        modified = copy.deepcopy(claude_simple_conversation)
        modified['updated_at'] = "2025-01-01T00:00:00Z"
        
        for m in modified['chat_messages']:
            if m.get('uuid') == msg.source_id:
                m['content'] = [{'type': 'text', 'text': 'MODIFIED'}]
                break
        
        extractor.extract_dialogue(modified)
        clean_db_session.commit()
        
        # In immutable mode, hash should NOT change
        clean_db_session.refresh(msg)
        assert msg.content_hash == original_hash


class TestIncrementalMode:
    """Tests for incremental (delta import) mode."""
    
    def test_incremental_mode_skips_soft_delete(self, clean_db_session, chatgpt_simple_conversation):
        """Test that incremental mode doesn't soft-delete missing messages."""
        extractor = ChatGPTExtractor(clean_db_session, incremental=True)
        
        # First import - full conversation
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_count = clean_db_session.query(Message).count()
        
        # Second import - partial conversation (remove a message)
        partial = copy.deepcopy(chatgpt_simple_conversation)
        partial['update_time'] = 1800000000.0
        
        mapping_keys = list(partial['mapping'].keys())
        removed_key = mapping_keys[-1]
        del partial['mapping'][removed_key]
        
        for node_id, node in partial['mapping'].items():
            if removed_key in node.get('children', []):
                node['children'].remove(removed_key)
        
        extractor.extract_dialogue(partial)
        clean_db_session.commit()
        
        # In incremental mode, no messages should be soft-deleted
        deleted_count = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).count()
        assert deleted_count == 0, "Incremental mode should not soft-delete"
        
        # Total count should be unchanged
        assert clean_db_session.query(Message).count() == original_count
    
    def test_non_incremental_mode_does_soft_delete(self, clean_db_session, chatgpt_simple_conversation):
        """Test that non-incremental mode (default) does soft-delete missing messages."""
        extractor = ChatGPTExtractor(clean_db_session, incremental=False)
        
        # First import - full conversation
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_count = clean_db_session.query(Message).count()
        
        # Second import - partial conversation
        partial = copy.deepcopy(chatgpt_simple_conversation)
        partial['update_time'] = 1800000000.0
        
        mapping_keys = list(partial['mapping'].keys())
        removed_key = mapping_keys[-1]
        del partial['mapping'][removed_key]
        
        for node_id, node in partial['mapping'].items():
            if removed_key in node.get('children', []):
                node['children'].remove(removed_key)
        
        extractor.extract_dialogue(partial)
        clean_db_session.commit()
        
        # In non-incremental mode, missing message should be soft-deleted
        deleted_count = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).count()
        assert deleted_count == 1, "Non-incremental mode should soft-delete"
    
    def test_incremental_mode_still_adds_new_messages(self, clean_db_session, chatgpt_simple_conversation):
        """Test that incremental mode still adds new messages."""
        extractor = ChatGPTExtractor(clean_db_session, incremental=True)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_count = clean_db_session.query(Message).count()
        
        # Add a new message
        extended = copy.deepcopy(chatgpt_simple_conversation)
        extended['update_time'] = 1800000000.0
        
        new_msg_id = "new-incremental-msg"
        last_node_id = list(extended['mapping'].keys())[-1]
        
        extended['mapping'][new_msg_id] = {
            "id": new_msg_id,
            "parent": last_node_id,
            "children": [],
            "message": {
                "id": new_msg_id,
                "author": {"role": "user"},
                "create_time": 1700006000.0,
                "content": {"content_type": "text", "parts": ["New message"]}
            }
        }
        
        extractor.extract_dialogue(extended)
        clean_db_session.commit()
        
        # New message should be created
        assert clean_db_session.query(Message).count() == original_count + 1
    
    def test_incremental_mode_still_updates_changed_messages(self, clean_db_session, chatgpt_simple_conversation):
        """Test that incremental mode still updates changed messages."""
        # Use mutable + incremental mode
        extractor = ChatGPTExtractor(clean_db_session, assume_immutable=False, incremental=True)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        msg = clean_db_session.query(Message).filter(Message.role == 'user').first()
        original_hash = msg.content_hash
        
        # Modify message content
        modified = copy.deepcopy(chatgpt_simple_conversation)
        modified['update_time'] = 1800000000.0
        
        for node_id, node in modified['mapping'].items():
            msg_data = node.get('message')
            if msg_data and msg_data.get('id') == msg.source_id:
                msg_data['content']['parts'] = ['MODIFIED CONTENT']
                break
        
        extractor.extract_dialogue(modified)
        clean_db_session.commit()
        
        # Content should be updated
        clean_db_session.refresh(msg)
        assert msg.content_hash != original_hash
    
    def test_claude_incremental_mode(self, clean_db_session, claude_simple_conversation):
        """Test that incremental mode works for Claude extractor."""
        extractor = ClaudeExtractor(clean_db_session, incremental=True)
        
        # First import
        extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        
        original_count = clean_db_session.query(Message).count()
        
        # Partial import (remove first message)
        partial = copy.deepcopy(claude_simple_conversation)
        partial['updated_at'] = "2025-01-01T00:00:00Z"
        partial['chat_messages'] = partial['chat_messages'][1:]  # Remove first message
        
        extractor.extract_dialogue(partial)
        clean_db_session.commit()
        
        # No messages should be soft-deleted
        deleted_count = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).count()
        assert deleted_count == 0
    
    def test_combined_immutable_and_incremental(self, clean_db_session, chatgpt_simple_conversation):
        """Test combining immutable and incremental modes for fastest delta imports."""
        extractor = ChatGPTExtractor(
            clean_db_session, 
            assume_immutable=True, 
            incremental=True
        )
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_count = clean_db_session.query(Message).count()
        msg = clean_db_session.query(Message).filter(Message.role == 'user').first()
        original_hash = msg.content_hash
        
        # Partial import with "modified" content
        partial = copy.deepcopy(chatgpt_simple_conversation)
        partial['update_time'] = 1800000000.0
        
        # Remove a message
        mapping_keys = list(partial['mapping'].keys())
        removed_key = mapping_keys[-1]
        del partial['mapping'][removed_key]
        
        # "Modify" existing message (should be ignored in immutable mode)
        for node_id, node in partial['mapping'].items():
            msg_data = node.get('message')
            if msg_data and msg_data.get('id') == msg.source_id:
                msg_data['content']['parts'] = ['SHOULD BE IGNORED']
            if removed_key in node.get('children', []):
                node['children'].remove(removed_key)
        
        extractor.extract_dialogue(partial)
        clean_db_session.commit()
        
        # No soft-deletes (incremental mode)
        deleted_count = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).count()
        assert deleted_count == 0
        
        # Hash unchanged (immutable mode)
        clean_db_session.refresh(msg)
        assert msg.content_hash == original_hash



---
File: tests/integration/test_models.py
---
# tests/integration/test_models.py
"""Integration tests for SQLAlchemy models with database."""

import pytest
from uuid import uuid4
from datetime import datetime, timezone

from llm_archive.models import (
    Dialogue, Message, ContentPart,
)


class TestRawModels:
    """Tests for raw schema models with database persistence."""
    
    def test_create_dialogue(self, db_session):
        """Test creating and persisting a dialogue."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='test-001',
            title='Test Dialogue',
            created_at=datetime.now(timezone.utc),
            source_json={'test': True},
        )
        db_session.add(dialogue)
        db_session.flush()
        
        assert dialogue.id is not None
        assert dialogue.source == 'chatgpt'
    
    def test_create_message_with_parent(self, db_session):
        """Test creating messages with parent relationship."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='test-002',
            source_json={'conversation_id': 'test-002'},
        )
        db_session.add(dialogue)
        db_session.flush()
        
        msg1 = Message(
            dialogue_id=dialogue.id,
            source_id='msg-001',
            role='user',
            source_json={'id': 'msg-001', 'role': 'user'},
        )
        db_session.add(msg1)
        db_session.flush()
        
        msg2 = Message(
            dialogue_id=dialogue.id,
            source_id='msg-002',
            role='assistant',
            parent_id=msg1.id,
            source_json={'id': 'msg-002', 'role': 'assistant'},
        )
        db_session.add(msg2)
        db_session.flush()
        
        assert msg2.parent_id == msg1.id
    
    def test_create_content_part(self, db_session):
        """Test creating content parts."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='test-003',
            source_json={},
        )
        db_session.add(dialogue)
        db_session.flush()
        
        message = Message(
            dialogue_id=dialogue.id,
            source_id='msg-003',
            role='assistant',
            source_json={'id': 'msg-003'},
        )
        db_session.add(message)
        db_session.flush()
        
        part = ContentPart(
            message_id=message.id,
            sequence=0,
            part_type='text',
            text_content='Hello world',
            source_json={'type': 'text'},
        )
        db_session.add(part)
        db_session.flush()
        
        assert part.id is not None
        assert part.message_id == message.id
    
    def test_dialogue_messages_relationship(self, db_session):
        """Test dialogue to messages relationship."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='test-004',
            source_json={},
        )
        db_session.add(dialogue)
        db_session.flush()
        
        msg1 = Message(
            dialogue_id=dialogue.id,
            source_id='m1',
            role='user',
            source_json={},
        )
        msg2 = Message(
            dialogue_id=dialogue.id,
            source_id='m2',
            role='assistant',
            source_json={},
        )
        db_session.add_all([msg1, msg2])
        db_session.flush()
        
        # Refresh to load relationship
        db_session.refresh(dialogue)
        
        assert len(dialogue.messages) == 2



class TestCascadeDeletes:
    """Tests for cascade delete behavior."""
    
    def test_delete_dialogue_cascades_to_messages(self, db_session):
        """Test that deleting dialogue deletes messages."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='test-cascade-001',
            source_json={},
        )
        db_session.add(dialogue)
        db_session.flush()
        
        msg = Message(
            dialogue_id=dialogue.id,
            source_id='m1',
            role='user',
            source_json={},
        )
        db_session.add(msg)
        db_session.flush()
        
        dialogue_id = dialogue.id
        
        # Delete dialogue
        db_session.delete(dialogue)
        db_session.flush()
        
        # Message should be gone
        remaining = db_session.query(Message).filter(
            Message.dialogue_id == dialogue_id
        ).count()
        assert remaining == 0
    
    def test_delete_message_cascades_to_content(self, db_session):
        """Test that deleting message deletes content parts."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='test-cascade-002',
            source_json={},
        )
        db_session.add(dialogue)
        db_session.flush()
        
        msg = Message(
            dialogue_id=dialogue.id,
            source_id='m1',
            role='user',
            source_json={},
        )
        db_session.add(msg)
        db_session.flush()
        
        part = ContentPart(
            message_id=msg.id,
            sequence=0,
            part_type='text',
            text_content='Hello',
            source_json={},
        )
        db_session.add(part)
        db_session.flush()
        
        msg_id = msg.id
        
        # Delete message
        db_session.delete(msg)
        db_session.flush()
        
        # Content part should be gone
        remaining = db_session.query(ContentPart).filter(
            ContentPart.message_id == msg_id
        ).count()
        assert remaining == 0



---
File: tests/integration/test_prompt_response_builder.py
---
# tests/integration/test_prompt_response_builder.py
"""Integration tests for PromptResponseBuilder."""

import pytest
from uuid import UUID

from llm_archive.extractors.chatgpt import ChatGPTExtractor
from llm_archive.extractors.claude import ClaudeExtractor
from llm_archive.builders.prompt_response import PromptResponseBuilder
from llm_archive.models import Dialogue, Message, PromptResponse


class TestPromptResponseBuilderBasic:
    """Basic tests for PromptResponseBuilder."""
    
    def test_build_for_simple_conversation(self, clean_db_session, chatgpt_simple_conversation):
        """Test building prompt-responses for a simple conversation."""
        # Import conversation
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Build prompt-responses
        builder = PromptResponseBuilder(clean_db_session)
        stats = builder.build_all()
        
        assert stats['prompt_responses'] > 0
        
        # Verify records exist
        prs = clean_db_session.query(PromptResponse).all()
        assert len(prs) > 0
    
    def test_pairs_user_with_assistant(self, clean_db_session, chatgpt_simple_conversation):
        """Test that user messages are paired with assistant responses."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        builder.build_all()
        
        # Get all prompt-responses
        prs = clean_db_session.query(PromptResponse).all()
        
        for pr in prs:
            prompt_msg = clean_db_session.get(Message, pr.prompt_message_id)
            response_msg = clean_db_session.get(Message, pr.response_message_id)
            
            assert prompt_msg.role == 'user'
            assert response_msg.role == 'assistant'
    
    def test_response_position_ordering(self, clean_db_session, chatgpt_simple_conversation):
        """Test that response_position reflects message order."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        builder.build_all()
        
        dialogue = clean_db_session.query(Dialogue).first()
        prs = clean_db_session.query(PromptResponse).filter(
            PromptResponse.dialogue_id == dialogue.id
        ).order_by(PromptResponse.response_position).all()
        
        # Positions should be monotonically increasing
        positions = [pr.response_position for pr in prs]
        assert positions == sorted(positions)
        assert len(set(positions)) == len(positions)  # No duplicates


class TestPromptResponseBuilderClaude:
    """Tests specific to Claude conversations."""
    
    def test_build_for_claude_conversation(self, clean_db_session, claude_simple_conversation):
        """Test building prompt-responses for Claude conversation."""
        extractor = ClaudeExtractor(clean_db_session)
        extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        stats = builder.build_all()
        
        assert stats['prompt_responses'] > 0
    
    def test_linear_chain_pairing(self, clean_db_session, claude_simple_conversation):
        """Test that linear chains are paired correctly."""
        extractor = ClaudeExtractor(clean_db_session)
        extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        builder.build_all()
        
        # Each assistant message should be paired with preceding user message
        prs = clean_db_session.query(PromptResponse).all()
        
        for pr in prs:
            assert pr.prompt_position < pr.response_position


class TestPromptResponseBuilderBranched:
    """Tests for branched conversations."""
    
    def test_build_for_branched_conversation(self, clean_db_session, chatgpt_branched_conversation):
        """Test building prompt-responses for branched conversation."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_branched_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        stats = builder.build_all()
        
        # Should handle branches without error
        assert stats['prompt_responses'] > 0
    
    def test_uses_parent_id_for_pairing(self, clean_db_session, chatgpt_branched_conversation):
        """Test that parent_id is used to find the correct prompt."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_branched_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        builder.build_all()
        
        prs = clean_db_session.query(PromptResponse).all()
        
        for pr in prs:
            response_msg = clean_db_session.get(Message, pr.response_message_id)
            prompt_msg = clean_db_session.get(Message, pr.prompt_message_id)
            
            # If response has a parent, verify the relationship
            if response_msg.parent_id:
                # The prompt should be the parent or an ancestor
                # (For regenerations, multiple responses may share a prompt)
                pass  # Complex to verify without tree traversal


class TestPromptResponseBuilderIdempotency:
    """Tests for idempotent building."""
    
    def test_rebuild_clears_existing(self, clean_db_session, chatgpt_simple_conversation):
        """Test that rebuilding clears and recreates records."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        
        # Build first time
        stats1 = builder.build_all()
        first_count = stats1['prompt_responses']
        
        # Build again
        stats2 = builder.build_all()
        second_count = stats2['prompt_responses']
        
        # Should have same count (cleared and rebuilt)
        assert first_count == second_count
        
        # Total records should equal one build's worth
        total = clean_db_session.query(PromptResponse).count()
        assert total == first_count
    
    def test_build_for_single_dialogue(self, clean_db_session, chatgpt_simple_conversation, chatgpt_branched_conversation):
        """Test building for a single dialogue doesn't affect others."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        extractor.extract_dialogue(chatgpt_branched_conversation)
        clean_db_session.commit()
        
        dialogues = clean_db_session.query(Dialogue).all()
        assert len(dialogues) == 2
        
        builder = PromptResponseBuilder(clean_db_session)
        
        # Build for first dialogue only
        builder.build_for_dialogue(dialogues[0].id)
        
        # Should only have records for first dialogue
        prs = clean_db_session.query(PromptResponse).all()
        dialogue_ids = {pr.dialogue_id for pr in prs}
        
        assert dialogues[0].id in dialogue_ids
        # Second dialogue may or may not be present depending on implementation


class TestPromptResponseBuilderEdgeCases:
    """Edge case tests."""
    
    def test_handles_system_messages(self, clean_db_session):
        """Test handling of conversations with system messages."""
        conversation = {
            'conversation_id': 'conv-system',
            'title': 'System Message Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-sys': {
                    'id': 'node-sys',
                    'message': {
                        'id': 'msg-sys',
                        'author': {'role': 'system'},
                        'content': {'parts': ['You are a helpful assistant.']},
                        'create_time': 1700000000,
                    },
                    'parent': None,
                },
                'node-user': {
                    'id': 'node-user',
                    'message': {
                        'id': 'msg-user',
                        'author': {'role': 'user'},
                        'content': {'parts': ['Hello']},
                        'create_time': 1700000001,
                    },
                    'parent': 'node-sys',
                },
                'node-asst': {
                    'id': 'node-asst',
                    'message': {
                        'id': 'msg-asst',
                        'author': {'role': 'assistant'},
                        'content': {'parts': ['Hi there!']},
                        'create_time': 1700000002,
                    },
                    'parent': 'node-user',
                },
            },
        }
        
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(conversation)
        clean_db_session.commit()
        
        # Get the dialogue we just created
        dialogue = clean_db_session.query(Dialogue).filter(
            Dialogue.source_id == 'conv-system'
        ).one()
        
        builder = PromptResponseBuilder(clean_db_session)
        stats = builder.build_for_dialogue(dialogue.id)
        
        # Should create one prompt-response (user -> assistant)
        # System message should not be part of a pair
        assert stats['prompt_responses'] == 1
        
        prs = clean_db_session.query(PromptResponse).filter(
            PromptResponse.dialogue_id == dialogue.id
        ).all()
        assert len(prs) == 1
        
        pr = prs[0]
        prompt = clean_db_session.get(Message, pr.prompt_message_id)
        assert prompt.role == 'user'
    
    def test_handles_empty_dialogue(self, clean_db_session):
        """Test handling of dialogue with no messages."""
        conversation = {
            'conversation_id': 'conv-empty',
            'title': 'Empty',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {},
        }
        
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(conversation)
        clean_db_session.commit()
        
        # Get the dialogue we just created
        dialogue = clean_db_session.query(Dialogue).filter(
            Dialogue.source_id == 'conv-empty'
        ).one()
        
        builder = PromptResponseBuilder(clean_db_session)
        stats = builder.build_for_dialogue(dialogue.id)
        
        assert stats['prompt_responses'] == 0
    
    def test_handles_user_only_dialogue(self, clean_db_session):
        """Test handling of dialogue with only user messages."""
        conversation = {
            'conversation_id': 'conv-user-only',
            'title': 'User Only',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': {
                        'id': 'msg-1',
                        'author': {'role': 'user'},
                        'content': {'parts': ['Hello?']},
                        'create_time': 1700000000,
                    },
                    'parent': None,
                },
            },
        }
        
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(conversation)
        clean_db_session.commit()
        
        # Get the dialogue we just created
        dialogue = clean_db_session.query(Dialogue).filter(
            Dialogue.source_id == 'conv-user-only'
        ).one()
        
        builder = PromptResponseBuilder(clean_db_session)
        stats = builder.build_for_dialogue(dialogue.id)
        
        # No assistant responses means no prompt-response pairs
        assert stats['prompt_responses'] == 0



---
File: tests/unit/__init__.py
---
# tests/unit/__init__.py
"""Unit tests - no database required."""



---
File: tests/unit/conftest.py
---
# tests/unit/conftest.py
"""Fixtures for unit tests - no database required."""

import uuid
from datetime import datetime, timezone
from unittest.mock import MagicMock, patch

import pytest


# ============================================================
# Sample Data Fixtures - ChatGPT Format
# ============================================================

@pytest.fixture
def chatgpt_simple_conversation() -> dict:
    """Simple linear ChatGPT conversation (no branches)."""
    root_id = str(uuid.uuid4())
    msg1_id = str(uuid.uuid4())
    msg2_id = str(uuid.uuid4())
    msg3_id = str(uuid.uuid4())
    msg4_id = str(uuid.uuid4())
    
    return {
        "conversation_id": "conv-simple-001",
        "title": "Simple Test Conversation",
        "create_time": 1700000000.0,
        "update_time": 1700001000.0,
        "mapping": {
            root_id: {
                "id": root_id,
                "parent": None,
                "children": [msg1_id],
                "message": None
            },
            msg1_id: {
                "id": msg1_id,
                "parent": root_id,
                "children": [msg2_id],
                "message": {
                    "id": msg1_id,
                    "author": {"role": "user"},
                    "create_time": 1700000100.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Hello, how are you?"]
                    }
                }
            },
            msg2_id: {
                "id": msg2_id,
                "parent": msg1_id,
                "children": [msg3_id],
                "message": {
                    "id": msg2_id,
                    "author": {"role": "assistant"},
                    "create_time": 1700000200.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["I'm doing well, thank you!"]
                    }
                }
            },
            msg3_id: {
                "id": msg3_id,
                "parent": msg2_id,
                "children": [msg4_id],
                "message": {
                    "id": msg3_id,
                    "author": {"role": "user"},
                    "create_time": 1700000300.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Explain Python decorators."]
                    }
                }
            },
            msg4_id: {
                "id": msg4_id,
                "parent": msg3_id,
                "children": [],
                "message": {
                    "id": msg4_id,
                    "author": {"role": "assistant"},
                    "create_time": 1700000400.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Python decorators are functions that modify other functions.\n\n```python\ndef decorator(func):\n    pass\n```"]
                    }
                }
            }
        }
    }


@pytest.fixture
def claude_simple_conversation() -> dict:
    """Simple Claude conversation."""
    return {
        "uuid": "claude-conv-001",
        "name": "Claude Test Conversation",
        "created_at": "2024-01-15T10:00:00Z",
        "updated_at": "2024-01-15T10:30:00Z",
        "chat_messages": [
            {
                "uuid": "claude-msg-001",
                "sender": "human",
                "created_at": "2024-01-15T10:00:00Z",
                "content": [
                    {"type": "text", "text": "Hello Claude!"}
                ]
            },
            {
                "uuid": "claude-msg-002",
                "sender": "assistant",
                "created_at": "2024-01-15T10:01:00Z",
                "content": [
                    {"type": "text", "text": "Hello! How can I help you today?"}
                ]
            },
        ]
    }


# ============================================================
# Mock Session Fixture
# ============================================================

@pytest.fixture
def mock_session():
    """Create a mock SQLAlchemy session for unit tests."""
    session = MagicMock()
    session.query.return_value.filter.return_value.first.return_value = None
    session.query.return_value.filter.return_value.all.return_value = []
    session.query.return_value.count.return_value = 0
    return session



---
File: tests/unit/test_annotations.py
---
# tests/unit/test_annotations.py
"""Unit tests for annotation infrastructure."""

import pytest
from uuid import uuid4

from llm_archive.annotations.core import (
    EntityType,
    ValueType,
    AnnotationResult,
    AnnotationWriter,
    AnnotationReader,
)


# ============================================================
# AnnotationResult Tests
# ============================================================

class TestAnnotationResult:
    """Test AnnotationResult dataclass."""
    
    def test_create_flag_result(self):
        """Flag results only need key."""
        result = AnnotationResult(
            key='has_code',
            value_type=ValueType.FLAG,
        )
        assert result.key == 'has_code'
        assert result.value is None
        assert result.value_type == ValueType.FLAG
    
    def test_create_string_result(self):
        """String results need key and value."""
        result = AnnotationResult(
            key='exchange_type',
            value='wiki_article',
            value_type=ValueType.STRING,
            confidence=0.9,
            reason='wiki_links_detected',
        )
        assert result.key == 'exchange_type'
        assert result.value == 'wiki_article'
        assert result.value_type == ValueType.STRING
        assert result.confidence == 0.9
        assert result.reason == 'wiki_links_detected'
    
    def test_create_numeric_result(self):
        """Numeric results need key and numeric value."""
        result = AnnotationResult(
            key='wiki_link_count',
            value=5,
            value_type=ValueType.NUMERIC,
        )
        assert result.key == 'wiki_link_count'
        assert result.value == 5
        assert result.value_type == ValueType.NUMERIC
    
    def test_create_json_result(self):
        """JSON results can store complex data."""
        result = AnnotationResult(
            key='metadata',
            value={'domains': ['example.com', 'test.org']},
            value_type=ValueType.JSON,
        )
        assert result.key == 'metadata'
        assert result.value == {'domains': ['example.com', 'test.org']}
        assert result.value_type == ValueType.JSON
    
    def test_default_value_type_is_string(self):
        """Default value_type should be STRING."""
        result = AnnotationResult(key='title', value='Test Title')
        assert result.value_type == ValueType.STRING
    
    def test_default_source_is_heuristic(self):
        """Default source should be 'heuristic'."""
        result = AnnotationResult(key='test', value='value')
        assert result.source == 'heuristic'


# ============================================================
# EntityType and ValueType Enum Tests
# ============================================================

class TestEnums:
    """Test EntityType and ValueType enums."""
    
    def test_entity_types(self):
        """All expected entity types exist."""
        assert EntityType.CONTENT_PART.value == 'content_part'
        assert EntityType.MESSAGE.value == 'message'
        assert EntityType.PROMPT_RESPONSE.value == 'prompt_response'
        assert EntityType.DIALOGUE.value == 'dialogue'
    
    def test_value_types(self):
        """All expected value types exist."""
        assert ValueType.FLAG.value == 'flag'
        assert ValueType.STRING.value == 'string'
        assert ValueType.NUMERIC.value == 'numeric'
        assert ValueType.JSON.value == 'json'


# ============================================================
# AnnotationWriter Tests (mock-based, no DB)
# ============================================================

class TestAnnotationWriterInterface:
    """Test AnnotationWriter interface without database."""
    
    def test_table_name_generation(self):
        """Test table name generation for entity/value type combos."""
        # Can't instantiate without session, but can test the pattern
        template = "derived.{entity}_annotations_{value_type}"
        
        assert template.format(
            entity='message', value_type='string'
        ) == 'derived.message_annotations_string'
        
        assert template.format(
            entity='content_part', value_type='flag'
        ) == 'derived.content_part_annotations_flag'
        
        assert template.format(
            entity='prompt_response', value_type='numeric'
        ) == 'derived.prompt_response_annotations_numeric'


# ============================================================
# Integration test fixtures (require database)
# ============================================================

@pytest.fixture
def db_session():
    """
    Create a database session for integration tests.
    
    This fixture is a placeholder - actual implementation would
    need a test database setup.
    """
    pytest.skip("Requires database setup")


class TestAnnotationWriterIntegration:
    """Integration tests for AnnotationWriter (require database)."""
    
    def test_write_flag_creates_record(self, db_session):
        """Writing a flag creates a record in flag table."""
        writer = AnnotationWriter(db_session)
        entity_id = uuid4()
        
        result = writer.write_flag(
            entity_type=EntityType.MESSAGE,
            entity_id=entity_id,
            key='has_code',
            source='test',
        )
        
        assert result is True
        # Verify record exists
        reader = AnnotationReader(db_session)
        assert reader.has_flag(EntityType.MESSAGE, entity_id, 'has_code')
    
    def test_write_string_creates_record(self, db_session):
        """Writing a string creates a record in string table."""
        writer = AnnotationWriter(db_session)
        entity_id = uuid4()
        
        result = writer.write_string(
            entity_type=EntityType.MESSAGE,
            entity_id=entity_id,
            key='gizmo_id',
            value='g-12345',
            source='test',
        )
        
        assert result is True
        reader = AnnotationReader(db_session)
        values = reader.get_string(EntityType.MESSAGE, entity_id, 'gizmo_id')
        assert 'g-12345' in values
    
    def test_write_duplicate_flag_returns_false(self, db_session):
        """Writing duplicate flag returns False (no new record)."""
        writer = AnnotationWriter(db_session)
        entity_id = uuid4()
        
        # First write succeeds
        result1 = writer.write_flag(
            entity_type=EntityType.MESSAGE,
            entity_id=entity_id,
            key='has_code',
            source='test',
        )
        assert result1 is True
        
        # Duplicate returns False
        result2 = writer.write_flag(
            entity_type=EntityType.MESSAGE,
            entity_id=entity_id,
            key='has_code',
            source='test',
        )
        assert result2 is False
    
    def test_write_multi_value_string(self, db_session):
        """Can write multiple values for same string key."""
        writer = AnnotationWriter(db_session)
        entity_id = uuid4()
        
        writer.write_string(
            entity_type=EntityType.MESSAGE,
            entity_id=entity_id,
            key='tag',
            value='coding',
            source='test',
        )
        writer.write_string(
            entity_type=EntityType.MESSAGE,
            entity_id=entity_id,
            key='tag',
            value='python',
            source='test',
        )
        
        reader = AnnotationReader(db_session)
        values = reader.get_string(EntityType.MESSAGE, entity_id, 'tag')
        assert set(values) == {'coding', 'python'}


class TestAnnotationReaderIntegration:
    """Integration tests for AnnotationReader (require database)."""
    
    def test_find_entities_with_flag(self, db_session):
        """Can find all entities with a specific flag."""
        writer = AnnotationWriter(db_session)
        
        # Create some flagged entities
        id1, id2, id3 = uuid4(), uuid4(), uuid4()
        writer.write_flag(EntityType.MESSAGE, id1, 'has_code', source='test')
        writer.write_flag(EntityType.MESSAGE, id2, 'has_code', source='test')
        writer.write_flag(EntityType.MESSAGE, id3, 'has_attachment', source='test')
        
        reader = AnnotationReader(db_session)
        results = reader.find_entities_with_flag(EntityType.MESSAGE, 'has_code')
        
        assert id1 in results
        assert id2 in results
        assert id3 not in results
    
    def test_find_entities_with_string_value(self, db_session):
        """Can find entities with specific string value."""
        writer = AnnotationWriter(db_session)
        
        id1, id2 = uuid4(), uuid4()
        writer.write_string(EntityType.MESSAGE, id1, 'gizmo_id', 'g-wiki', source='test')
        writer.write_string(EntityType.MESSAGE, id2, 'gizmo_id', 'g-other', source='test')
        
        reader = AnnotationReader(db_session)
        results = reader.find_entities_with_string(
            EntityType.MESSAGE, 'gizmo_id', 'g-wiki'
        )
        
        assert id1 in results
        assert id2 not in results



---
File: tests/unit/test_chatgpt_extractor_annotations.py
---
# tests/unit/test_chatgpt_extractor_annotations.py
"""Unit tests for ChatGPT extractor annotation integration.

These tests verify that the ChatGPT extractor correctly writes
annotations during ingestion for:
- gizmo_id (string annotation on messages)
- has_gizmo (flag annotation on messages)
- model_slug (string annotation on messages)
- Canvas metadata (annotations on content_parts)
"""

import pytest
from uuid import uuid4


# ============================================================
# Test data fixtures
# ============================================================

def make_message_data(
    msg_id: str = None,
    role: str = 'assistant',
    content: str = 'Test content',
    gizmo_id: str = None,
    model_slug: str = None,
    canvas: dict = None,
) -> dict:
    """Create mock ChatGPT message data."""
    msg_id = msg_id or str(uuid4())
    
    metadata = {}
    if gizmo_id:
        metadata['gizmo_id'] = gizmo_id
    if model_slug:
        metadata['model_slug'] = model_slug
    if canvas:
        metadata['canvas'] = canvas
    
    return {
        'id': msg_id,
        'author': {
            'role': role,
            'name': None,
            'metadata': {},
        },
        'content': {
            'content_type': 'text',
            'parts': [content] if content else [],
        },
        'metadata': metadata,
        'create_time': 1700000000,
        'update_time': 1700000000,
        'status': 'finished',
        'end_turn': True,
    }


def make_canvas_data(
    textdoc_id: str = 'doc-123',
    version: int = 1,
    title: str = 'Test Canvas',
    textdoc_type: str = 'document',
    content: str = 'Canvas content here',
) -> dict:
    """Create mock canvas data."""
    return {
        'textdoc_id': textdoc_id,
        'version': version,
        'title': title,
        'textdoc_type': textdoc_type,
        'content': content,
        'from_version': version - 1 if version > 1 else None,
        'textdoc_content_length': len(content) if content else 0,
        'has_user_edit': False,
    }


# ============================================================
# Unit tests (no database required)
# ============================================================

class TestMessageDataConstruction:
    """Test message data fixture construction."""
    
    def test_basic_message_data(self):
        """Basic message data should have required fields."""
        data = make_message_data()
        
        assert 'id' in data
        assert data['author']['role'] == 'assistant'
        assert data['content']['parts'] == ['Test content']
    
    def test_message_with_gizmo(self):
        """Message with gizmo_id should have it in metadata."""
        data = make_message_data(gizmo_id='g-wiki-generator')
        
        assert data['metadata']['gizmo_id'] == 'g-wiki-generator'
    
    def test_message_with_model(self):
        """Message with model_slug should have it in metadata."""
        data = make_message_data(model_slug='gpt-4')
        
        assert data['metadata']['model_slug'] == 'gpt-4'
    
    def test_message_with_canvas(self):
        """Message with canvas should have canvas in metadata."""
        canvas = make_canvas_data(title='My Document')
        data = make_message_data(canvas=canvas)
        
        assert data['metadata']['canvas']['title'] == 'My Document'


class TestCanvasDataConstruction:
    """Test canvas data fixture construction."""
    
    def test_basic_canvas_data(self):
        """Basic canvas data should have required fields."""
        canvas = make_canvas_data()
        
        assert canvas['textdoc_id'] == 'doc-123'
        assert canvas['version'] == 1
        assert canvas['title'] == 'Test Canvas'
        assert canvas['textdoc_type'] == 'document'
    
    def test_canvas_version_tracking(self):
        """Canvas with version > 1 should have from_version."""
        canvas = make_canvas_data(version=3)
        
        assert canvas['from_version'] == 2
    
    def test_canvas_first_version(self):
        """First version canvas should have from_version=None."""
        canvas = make_canvas_data(version=1)
        
        assert canvas['from_version'] is None


# ============================================================
# Integration tests (require database)
# ============================================================

@pytest.fixture
def db_session():
    """
    Create a database session for integration tests.
    
    This fixture is a placeholder - actual implementation would
    need a test database setup with schema applied.
    """
    pytest.skip("Requires database setup with schema")


class TestChatGPTExtractorGizmoAnnotations:
    """Test gizmo annotation writing during extraction."""
    
    def test_extracts_gizmo_id_annotation(self, db_session):
        """Gizmo ID should be written as message string annotation."""
        from llm_archive.extractors.chatgpt import ChatGPTExtractor
        from llm_archive.annotations import AnnotationReader, EntityType
        
        extractor = ChatGPTExtractor(db_session)
        
        # Create a dialogue with a message using a gizmo
        dialogue_data = {
            'conversation_id': 'test-conv-1',
            'title': 'Test Conversation',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(
                        msg_id='msg-1',
                        gizmo_id='g-wiki-generator',
                    ),
                },
            },
        }
        
        result = extractor.extract_dialogue(dialogue_data)
        db_session.commit()
        
        assert result == 'new'
        
        # Verify annotation was written
        reader = AnnotationReader(db_session)
        # Would need to resolve the message ID to verify
    
    def test_extracts_has_gizmo_flag(self, db_session):
        """has_gizmo flag should be written for messages with gizmo."""
        from llm_archive.extractors.chatgpt import ChatGPTExtractor
        
        extractor = ChatGPTExtractor(db_session)
        
        dialogue_data = {
            'conversation_id': 'test-conv-2',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(
                        msg_id='msg-1',
                        gizmo_id='g-test',
                    ),
                },
            },
        }
        
        result = extractor.extract_dialogue(dialogue_data)
        assert result == 'new'
    
    def test_no_gizmo_annotation_when_missing(self, db_session):
        """Messages without gizmo should not have gizmo annotations."""
        from llm_archive.extractors.chatgpt import ChatGPTExtractor
        
        extractor = ChatGPTExtractor(db_session)
        
        dialogue_data = {
            'conversation_id': 'test-conv-3',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(
                        msg_id='msg-1',
                        gizmo_id=None,  # No gizmo
                    ),
                },
            },
        }
        
        result = extractor.extract_dialogue(dialogue_data)
        assert result == 'new'


class TestChatGPTExtractorCanvasAnnotations:
    """Test canvas annotation writing during extraction."""
    
    def test_extracts_canvas_as_content_part(self, db_session):
        """Canvas should be created as content_part with type='canvas'."""
        from llm_archive.extractors.chatgpt import ChatGPTExtractor
        from llm_archive.models import ContentPart
        
        extractor = ChatGPTExtractor(db_session)
        
        canvas = make_canvas_data(
            textdoc_id='doc-abc',
            version=1,
            title='My Wiki Article',
            content='Article content here',
        )
        
        dialogue_data = {
            'conversation_id': 'test-conv-canvas',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(
                        msg_id='msg-1',
                        content='Here is your document',
                        canvas=canvas,
                    ),
                },
            },
        }
        
        result = extractor.extract_dialogue(dialogue_data)
        db_session.commit()
        
        assert result == 'new'
        
        # Verify canvas content_part was created
        canvas_parts = db_session.query(ContentPart).filter(
            ContentPart.part_type == 'canvas'
        ).all()
        
        assert len(canvas_parts) >= 1
    
    def test_canvas_title_annotation(self, db_session):
        """Canvas title should be written as content_part annotation."""
        from llm_archive.extractors.chatgpt import ChatGPTExtractor
        from llm_archive.annotations import AnnotationReader, EntityType
        
        extractor = ChatGPTExtractor(db_session)
        
        canvas = make_canvas_data(title='Important Document')
        
        dialogue_data = {
            'conversation_id': 'test-conv-canvas-title',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(canvas=canvas),
                },
            },
        }
        
        result = extractor.extract_dialogue(dialogue_data)
        db_session.commit()
        
        # Would verify title annotation exists
    
    def test_canvas_version_annotation(self, db_session):
        """Canvas version should be written as numeric annotation."""
        from llm_archive.extractors.chatgpt import ChatGPTExtractor
        
        extractor = ChatGPTExtractor(db_session)
        
        canvas = make_canvas_data(version=5)
        
        dialogue_data = {
            'conversation_id': 'test-conv-canvas-version',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(canvas=canvas),
                },
            },
        }
        
        result = extractor.extract_dialogue(dialogue_data)
        assert result == 'new'


class TestMarkLatestCanvasVersions:
    """Test the mark_latest_canvas_versions utility."""
    
    def test_marks_single_version_as_latest(self, db_session):
        """Single canvas version should be marked as latest."""
        from llm_archive.extractors.chatgpt import (
            ChatGPTExtractor,
            mark_latest_canvas_versions,
        )
        
        extractor = ChatGPTExtractor(db_session)
        
        canvas = make_canvas_data(textdoc_id='doc-single', version=1)
        
        dialogue_data = {
            'conversation_id': 'test-single-version',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(canvas=canvas),
                },
            },
        }
        
        extractor.extract_dialogue(dialogue_data)
        db_session.commit()
        
        count = mark_latest_canvas_versions(db_session)
        assert count >= 1
    
    def test_marks_highest_version_as_latest(self, db_session):
        """With multiple versions, only highest should be marked latest."""
        from llm_archive.extractors.chatgpt import (
            ChatGPTExtractor,
            mark_latest_canvas_versions,
        )
        
        extractor = ChatGPTExtractor(db_session)
        
        # Create messages with different canvas versions
        mapping = {}
        for i, version in enumerate([1, 3, 2]):  # Out of order
            canvas = make_canvas_data(
                textdoc_id='doc-multi',
                version=version,
            )
            mapping[f'node-{i}'] = {
                'id': f'node-{i}',
                'message': make_message_data(
                    msg_id=f'msg-{i}',
                    canvas=canvas,
                ),
            }
        
        dialogue_data = {
            'conversation_id': 'test-multi-version',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': mapping,
        }
        
        extractor.extract_dialogue(dialogue_data)
        db_session.commit()
        
        count = mark_latest_canvas_versions(db_session)
        
        # Should mark version 3 as latest (only one per textdoc_id)
        # Would need to verify the correct one is marked


class TestFindWikiGizmoMessages:
    """Test the find_wiki_gizmo_messages utility."""
    
    def test_finds_messages_by_gizmo(self, db_session):
        """Should find all messages with specific gizmo_id."""
        from llm_archive.extractors.chatgpt import (
            ChatGPTExtractor,
            find_wiki_gizmo_messages,
        )
        
        extractor = ChatGPTExtractor(db_session)
        
        # Create messages with different gizmos
        dialogue_data = {
            'conversation_id': 'test-find-gizmo',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(
                        msg_id='msg-1',
                        gizmo_id='g-wiki',
                    ),
                },
                'node-2': {
                    'id': 'node-2',
                    'message': make_message_data(
                        msg_id='msg-2',
                        gizmo_id='g-other',
                    ),
                },
                'node-3': {
                    'id': 'node-3',
                    'message': make_message_data(
                        msg_id='msg-3',
                        gizmo_id='g-wiki',
                    ),
                },
            },
        }
        
        extractor.extract_dialogue(dialogue_data)
        db_session.commit()
        
        wiki_messages = find_wiki_gizmo_messages(db_session, 'g-wiki')
        other_messages = find_wiki_gizmo_messages(db_session, 'g-other')
        
        assert len(wiki_messages) == 2
        assert len(other_messages) == 1
    
    def test_returns_empty_for_unknown_gizmo(self, db_session):
        """Should return empty list for unknown gizmo_id."""
        from llm_archive.extractors.chatgpt import find_wiki_gizmo_messages
        
        messages = find_wiki_gizmo_messages(db_session, 'g-nonexistent')
        assert messages == []



---
File: tests/unit/test_cli.py
---
# tests/unit/test_cli.py
"""Unit tests for CLI interface."""

import json
import tempfile
from pathlib import Path

import pytest

from llm_archive.cli import CLI


class TestCLIInit:
    """Tests for CLI initialization."""
    
    def test_cli_default_db_url(self):
        """Test CLI uses default database URL."""
        cli = CLI()
        assert 'postgresql://' in cli.db_url
    
    def test_cli_custom_db_url(self):
        """Test CLI accepts custom database URL."""
        custom_url = "postgresql://user:pass@host:5432/mydb"
        cli = CLI(db_url=custom_url)
        assert cli.db_url == custom_url


class TestCLILoadJSON:
    """Tests for JSON loading."""
    
    def test_load_json_valid(self):
        """Test loading valid JSON file."""
        cli = CLI()
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump([{"id": "test"}], f)
            temp_path = f.name
        
        try:
            data = cli._load_json(temp_path)
            assert data == [{"id": "test"}]
        finally:
            Path(temp_path).unlink()
    
    def test_load_json_missing_file(self):
        """Test loading missing file raises error."""
        cli = CLI()
        
        with pytest.raises(FileNotFoundError):
            cli._load_json("/nonexistent/path.json")
    
    def test_load_json_invalid_format(self):
        """Test loading non-array JSON raises error."""
        cli = CLI()
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump({"not": "an array"}, f)
            temp_path = f.name
        
        try:
            with pytest.raises(ValueError, match="Expected JSON array"):
                cli._load_json(temp_path)
        finally:
            Path(temp_path).unlink()
    
    def test_load_json_empty_array(self):
        """Test loading empty array."""
        cli = CLI()
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump([], f)
            temp_path = f.name
        
        try:
            data = cli._load_json(temp_path)
            assert data == []
        finally:
            Path(temp_path).unlink()
    
    def test_load_json_multiple_items(self):
        """Test loading array with multiple items."""
        cli = CLI()
        
        items = [{"id": "1"}, {"id": "2"}, {"id": "3"}]
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(items, f)
            temp_path = f.name
        
        try:
            data = cli._load_json(temp_path)
            assert data == items
            assert len(data) == 3
        finally:
            Path(temp_path).unlink()



---
File: tests/unit/test_content_classification.py
---
# tests/unit/test_content_classification.py
"""Unit tests for content part classification logic."""

import pytest

from llm_archive.extractors.chatgpt import ChatGPTExtractor
from llm_archive.extractors.claude import ClaudeExtractor


class TestChatGPTClassifyContentPart:
    """Tests for ChatGPT content part classification."""
    
    @pytest.fixture
    def extractor(self, mock_session):
        """Create extractor with mock session."""
        return ChatGPTExtractor(mock_session)
    
    def test_classify_string_text(self, extractor):
        """Test classifying a plain string as text."""
        result = extractor._classify_content_part("Hello world")
        
        assert result['part_type'] == 'text'
        assert result['text_content'] == "Hello world"
        assert result['source_json'] == {'text': "Hello world"}
    
    def test_classify_dict_text(self, extractor):
        """Test classifying a dict with text."""
        part = {'text': 'Some text content'}
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'text'
        assert result['text_content'] == 'Some text content'
    
    def test_classify_image(self, extractor):
        """Test classifying image content."""
        part = {
            'content_type': 'image/png',
            'asset_pointer': 'file-service://abc123',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'image'
        assert result['media_type'] == 'image/png'
        assert result['url'] == 'file-service://abc123'
    
    def test_classify_image_with_url(self, extractor):
        """Test classifying image with direct URL."""
        part = {
            'content_type': 'image/jpeg',
            'url': 'https://example.com/image.jpg',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'image'
        assert result['media_type'] == 'image/jpeg'
        assert result['url'] == 'https://example.com/image.jpg'
    
    def test_classify_audio(self, extractor):
        """Test classifying audio content."""
        part = {
            'content_type': 'audio/mp3',
            'url': 'https://example.com/audio.mp3',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'audio'
        assert result['media_type'] == 'audio/mp3'
        assert result['url'] == 'https://example.com/audio.mp3'
    
    def test_classify_video(self, extractor):
        """Test classifying video content."""
        part = {
            'content_type': 'video/mp4',
            'asset_pointer': 'file-service://video123',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'video'
        assert result['media_type'] == 'video/mp4'
        assert result['url'] == 'file-service://video123'
    
    def test_classify_code(self, extractor):
        """Test classifying code content."""
        part = {
            'content_type': 'code',
            'language': 'python',
            'text': 'print("hello")',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'code'
        assert result['language'] == 'python'
        assert result['text_content'] == 'print("hello")'
    
    def test_classify_code_by_language(self, extractor):
        """Test classifying code by presence of language field."""
        part = {
            'language': 'javascript',
            'code': 'console.log("hi")',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'code'
        assert result['language'] == 'javascript'
    
    def test_classify_unknown_type(self, extractor):
        """Test classifying unknown content type."""
        part = {'content_type': 'exotic/type', 'data': 'something'}
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'exotic/type'
    
    def test_classify_non_dict(self, extractor):
        """Test classifying non-dict, non-string content."""
        result = extractor._classify_content_part(12345)
        
        assert result['part_type'] == 'unknown'
        assert result['source_json'] == {'raw': '12345'}


class TestClaudeClassifyContentPart:
    """Tests for Claude content part classification."""
    
    @pytest.fixture
    def extractor(self, mock_session):
        """Create extractor with mock session."""
        return ClaudeExtractor(mock_session)
    
    def test_classify_text(self, extractor):
        """Test classifying text content."""
        part = {'type': 'text', 'text': 'Hello from Claude'}
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'text'
        assert result['text_content'] == 'Hello from Claude'
    
    def test_classify_thinking(self, extractor):
        """Test classifying thinking content."""
        part = {'type': 'thinking', 'thinking': 'Let me consider this...'}
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'thinking'
        assert result['text_content'] == 'Let me consider this...'
    
    def test_classify_tool_use(self, extractor):
        """Test classifying tool_use content."""
        part = {
            'type': 'tool_use',
            'name': 'web_search',
            'id': 'tool-abc123',
            'input': {'query': 'climate change'},
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'tool_use'
        assert result['tool_name'] == 'web_search'
        assert result['tool_use_id'] == 'tool-abc123'
        assert result['tool_input'] == {'query': 'climate change'}
        assert result['text_content'] == 'climate change'  # Extracted from input.query
    
    def test_classify_tool_use_text_input(self, extractor):
        """Test classifying tool_use with text input."""
        part = {
            'type': 'tool_use',
            'name': 'code_executor',
            'id': 'tool-xyz',
            'input': {'text': 'print(1+1)'},
        }
        result = extractor._classify_content_part(part)
        
        assert result['tool_name'] == 'code_executor'
        assert result['text_content'] == 'print(1+1)'
    
    def test_classify_tool_result_string(self, extractor):
        """Test classifying tool_result with string content."""
        part = {
            'type': 'tool_result',
            'tool_use_id': 'tool-abc123',
            'content': 'Search results: AI news...',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'tool_result'
        assert result['tool_use_id'] == 'tool-abc123'
        assert result['text_content'] == 'Search results: AI news...'
    
    def test_classify_tool_result_list(self, extractor):
        """Test classifying tool_result with list content."""
        part = {
            'type': 'tool_result',
            'tool_use_id': 'tool-abc123',
            'content': [
                {'text': 'First result'},
                {'text': 'Second result'},
            ],
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'tool_result'
        assert result['text_content'] == 'First result\nSecond result'
    
    def test_classify_tool_result_mixed_list(self, extractor):
        """Test classifying tool_result with mixed list content."""
        part = {
            'type': 'tool_result',
            'tool_use_id': 'tool-abc123',
            'content': [
                'Plain string',
                {'text': 'Dict with text'},
            ],
        }
        result = extractor._classify_content_part(part)
        
        assert result['text_content'] == 'Plain string\nDict with text'
    
    def test_classify_tool_result_error(self, extractor):
        """Test classifying tool_result with error flag."""
        part = {
            'type': 'tool_result',
            'tool_use_id': 'tool-abc123',
            'is_error': True,
            'content': 'Error: Something went wrong',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'tool_result'
        assert result['is_error'] is True
    
    def test_classify_image(self, extractor):
        """Test classifying image content."""
        part = {
            'type': 'image',
            'media_type': 'image/png',
            'source': {'type': 'url', 'url': 'https://example.com/img.png'},
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'image'
        assert result['media_type'] == 'image/png'
        assert result['url'] == 'https://example.com/img.png'
    
    def test_classify_image_base64(self, extractor):
        """Test classifying base64 image (no URL)."""
        part = {
            'type': 'image',
            'media_type': 'image/jpeg',
            'source': {'type': 'base64', 'data': 'abc123...'},
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'image'
        assert result['media_type'] == 'image/jpeg'
        assert 'url' not in result or result.get('url') is None
    
    def test_classify_unknown_type(self, extractor):
        """Test classifying unknown content type."""
        part = {'type': 'custom_widget', 'data': {'foo': 'bar'}}
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'custom_widget'



---
File: tests/unit/test_content_part_annotators.py
---
# tests/unit/test_content_part_annotators.py
"""Unit tests for content-part level annotators."""

import pytest
from datetime import datetime, timezone
from uuid import uuid4

from llm_archive.annotators.content_part import (
    ContentPartData,
    ContentPartAnnotator,
    CodeBlockAnnotator,
    ScriptHeaderAnnotator,
    LatexContentAnnotator,
    WikiLinkContentAnnotator,
    CONTENT_PART_ANNOTATORS,
)
from llm_archive.annotations.core import ValueType, EntityType


# ============================================================
# Test Fixtures
# ============================================================

@pytest.fixture
def content_part_id():
    """Generate a content-part ID."""
    return uuid4()


def make_content_part_data(
    text_content: str = "Test content",
    part_type: str = "text",
    language: str | None = None,
    role: str = "assistant",
    content_part_id: uuid4 = None,
) -> ContentPartData:
    """Create ContentPartData for testing."""
    return ContentPartData(
        content_part_id=content_part_id or uuid4(),
        message_id=uuid4(),
        dialogue_id=uuid4(),
        sequence=0,
        part_type=part_type,
        text_content=text_content,
        language=language,
        role=role,
        created_at=datetime.now(timezone.utc),
    )


# ============================================================
# CodeBlockAnnotator Tests
# ============================================================

class TestCodeBlockAnnotator:
    """Test code block detection at content-part level."""
    
    def test_detects_simple_code_block(self, content_part_id):
        """Should detect basic code blocks."""
        data = make_content_part_data(
            text_content="Here's some code:\n```\nprint('hello')\n```",
            content_part_id=content_part_id,
        )
        
        annotator = CodeBlockAnnotator.__new__(CodeBlockAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_code_block' for r in results)
        assert any(r.key == 'code_block_count' for r in results)
        
        count_result = next(r for r in results if r.key == 'code_block_count')
        assert count_result.value == 1
    
    def test_detects_code_block_with_language(self, content_part_id):
        """Should detect code blocks with language specification."""
        data = make_content_part_data(
            text_content="```python\ndef hello():\n    pass\n```",
            content_part_id=content_part_id,
        )
        
        annotator = CodeBlockAnnotator.__new__(CodeBlockAnnotator)
        results = annotator.annotate(data)
        
        lang_results = [r for r in results if r.key == 'code_language']
        assert len(lang_results) == 1
        assert lang_results[0].value == 'python'
    
    def test_counts_multiple_code_blocks(self, content_part_id):
        """Should count multiple code blocks."""
        data = make_content_part_data(
            text_content="```python\ncode1\n```\n\n```javascript\ncode2\n```\n\n```sql\ncode3\n```",
            content_part_id=content_part_id,
        )
        
        annotator = CodeBlockAnnotator.__new__(CodeBlockAnnotator)
        results = annotator.annotate(data)
        
        count_result = next(r for r in results if r.key == 'code_block_count')
        assert count_result.value == 3
        
        lang_results = [r for r in results if r.key == 'code_language']
        langs = {r.value for r in lang_results}
        assert langs == {'python', 'javascript', 'sql'}
    
    def test_no_code_blocks(self, content_part_id):
        """Should return empty for text without code blocks."""
        data = make_content_part_data(
            text_content="This is plain text without any code.",
            content_part_id=content_part_id,
        )
        
        annotator = CodeBlockAnnotator.__new__(CodeBlockAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_skips_non_text_parts(self, content_part_id):
        """Should only process text part_type."""
        # Note: The base class handles this via PART_TYPE_FILTER
        # but the annotate method should also be robust
        data = make_content_part_data(
            text_content="```code```",
            part_type="image",  # Not text
            content_part_id=content_part_id,
        )
        
        # The filter is applied in _iter_content_parts, not annotate
        # So annotate itself will still process, but in real use it won't be called
        annotator = CodeBlockAnnotator.__new__(CodeBlockAnnotator)
        # This should still work since text_content is provided
        results = annotator.annotate(data)
        assert any(r.key == 'has_code_block' for r in results)
    
    def test_empty_text_content(self, content_part_id):
        """Should handle empty text content."""
        data = make_content_part_data(
            text_content="",
            content_part_id=content_part_id,
        )
        
        annotator = CodeBlockAnnotator.__new__(CodeBlockAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_none_text_content(self, content_part_id):
        """Should handle None text content."""
        data = make_content_part_data(
            text_content="placeholder",
            content_part_id=content_part_id,
        )
        data.text_content = None
        
        annotator = CodeBlockAnnotator.__new__(CodeBlockAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0


# ============================================================
# ScriptHeaderAnnotator Tests
# ============================================================

class TestScriptHeaderAnnotator:
    """Test script header detection."""
    
    def test_detects_python_shebang(self, content_part_id):
        """Should detect Python shebang."""
        data = make_content_part_data(
            text_content="#!/usr/bin/env python3\nimport sys",
            content_part_id=content_part_id,
        )
        
        annotator = ScriptHeaderAnnotator.__new__(ScriptHeaderAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_script_header' for r in results)
        
        type_result = next(r for r in results if r.key == 'script_type')
        assert type_result.value == 'python3'
    
    def test_detects_bash_shebang(self, content_part_id):
        """Should detect Bash shebang."""
        data = make_content_part_data(
            text_content="#!/bin/bash\necho hello",
            content_part_id=content_part_id,
        )
        
        annotator = ScriptHeaderAnnotator.__new__(ScriptHeaderAnnotator)
        results = annotator.annotate(data)
        
        type_result = next(r for r in results if r.key == 'script_type')
        assert type_result.value == 'bash'
    
    def test_detects_c_include(self, content_part_id):
        """Should detect C/C++ includes."""
        data = make_content_part_data(
            text_content='#include <stdio.h>\nint main() {}',
            content_part_id=content_part_id,
        )
        
        annotator = ScriptHeaderAnnotator.__new__(ScriptHeaderAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_script_header' for r in results)
        
        type_result = next(r for r in results if r.key == 'script_type')
        assert type_result.value == 'c'
    
    def test_detects_c_include_quotes(self, content_part_id):
        """Should detect C includes with quotes."""
        data = make_content_part_data(
            text_content='#include "myheader.h"',
            content_part_id=content_part_id,
        )
        
        annotator = ScriptHeaderAnnotator.__new__(ScriptHeaderAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_script_header' for r in results)
    
    def test_detects_php_tag(self, content_part_id):
        """Should detect PHP opening tag."""
        data = make_content_part_data(
            text_content="<?php\necho 'Hello';",
            content_part_id=content_part_id,
        )
        
        annotator = ScriptHeaderAnnotator.__new__(ScriptHeaderAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_script_header' for r in results)
        
        type_result = next(r for r in results if r.key == 'script_type')
        assert type_result.value == 'php'
    
    def test_no_script_header(self, content_part_id):
        """Should not detect in plain text."""
        data = make_content_part_data(
            text_content="Just some plain text about programming.",
            content_part_id=content_part_id,
        )
        
        annotator = ScriptHeaderAnnotator.__new__(ScriptHeaderAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0


# ============================================================
# LatexContentAnnotator Tests
# ============================================================

class TestLatexContentAnnotator:
    """Test LaTeX detection at content-part level."""
    
    def test_detects_display_math(self, content_part_id):
        """Should detect $$ display math."""
        data = make_content_part_data(
            text_content="The equation is: $$E = mc^2$$",
            content_part_id=content_part_id,
        )
        
        annotator = LatexContentAnnotator.__new__(LatexContentAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_latex' for r in results)
        latex_types = {r.value for r in results if r.key == 'latex_type'}
        assert 'display' in latex_types
    
    def test_detects_inline_math(self, content_part_id):
        """Should detect inline $ math."""
        data = make_content_part_data(
            text_content="The value $x = 5$ is the solution.",
            content_part_id=content_part_id,
        )
        
        annotator = LatexContentAnnotator.__new__(LatexContentAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_latex' for r in results)
        latex_types = {r.value for r in results if r.key == 'latex_type'}
        assert 'inline' in latex_types
    
    def test_detects_latex_commands(self, content_part_id):
        """Should detect LaTeX commands."""
        data = make_content_part_data(
            text_content="Use \\frac{a}{b} for fractions.",
            content_part_id=content_part_id,
        )
        
        annotator = LatexContentAnnotator.__new__(LatexContentAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_latex' for r in results)
        latex_types = {r.value for r in results if r.key == 'latex_type'}
        assert 'commands' in latex_types
    
    def test_multiple_latex_types(self, content_part_id):
        """Should detect multiple LaTeX types."""
        data = make_content_part_data(
            text_content="Inline $x$ and display $$\\sum_{i=1}^n i$$",
            content_part_id=content_part_id,
        )
        
        annotator = LatexContentAnnotator.__new__(LatexContentAnnotator)
        results = annotator.annotate(data)
        
        latex_types = {r.value for r in results if r.key == 'latex_type'}
        assert len(latex_types) >= 2
    
    def test_no_latex(self, content_part_id):
        """Should not detect in plain text."""
        data = make_content_part_data(
            text_content="The price is $100 or maybe $200.",
            content_part_id=content_part_id,
        )
        
        annotator = LatexContentAnnotator.__new__(LatexContentAnnotator)
        results = annotator.annotate(data)
        
        # Single $ with numbers shouldn't match inline math pattern
        # (inline pattern requires non-$ chars inside)
        # But this test may be tricky - adjust based on actual behavior
        pass  # May or may not detect - depends on pattern specifics


# ============================================================
# WikiLinkContentAnnotator Tests
# ============================================================

class TestWikiLinkContentAnnotator:
    """Test wiki link detection at content-part level."""
    
    def test_detects_wiki_links(self, content_part_id):
        """Should detect [[wiki links]]."""
        data = make_content_part_data(
            text_content="The [[cat]] is a [[mammal]].",
            content_part_id=content_part_id,
        )
        
        annotator = WikiLinkContentAnnotator.__new__(WikiLinkContentAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_wiki_links' for r in results)
        
        count_result = next(r for r in results if r.key == 'wiki_link_count')
        assert count_result.value == 2
    
    def test_counts_many_wiki_links(self, content_part_id):
        """Should count multiple wiki links."""
        data = make_content_part_data(
            text_content="[[One]] [[Two]] [[Three]] [[Four]] [[Five]]",
            content_part_id=content_part_id,
        )
        
        annotator = WikiLinkContentAnnotator.__new__(WikiLinkContentAnnotator)
        results = annotator.annotate(data)
        
        count_result = next(r for r in results if r.key == 'wiki_link_count')
        assert count_result.value == 5
    
    def test_no_wiki_links(self, content_part_id):
        """Should not detect in plain text."""
        data = make_content_part_data(
            text_content="Regular text with [single brackets] and no wiki links.",
            content_part_id=content_part_id,
        )
        
        annotator = WikiLinkContentAnnotator.__new__(WikiLinkContentAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0


# ============================================================
# ContentPartAnnotator Base Class Tests
# ============================================================

class TestContentPartAnnotatorBase:
    """Test base class attributes and behavior."""
    
    def test_entity_type(self):
        """All content-part annotators should use CONTENT_PART entity type."""
        for annotator_cls in CONTENT_PART_ANNOTATORS:
            assert annotator_cls.ENTITY_TYPE == EntityType.CONTENT_PART
    
    def test_annotators_have_annotation_key(self):
        """All annotators should have ANNOTATION_KEY defined."""
        for annotator_cls in CONTENT_PART_ANNOTATORS:
            assert annotator_cls.ANNOTATION_KEY, f"{annotator_cls.__name__} missing ANNOTATION_KEY"
    
    def test_annotators_have_priority(self):
        """All annotators should have PRIORITY defined."""
        for annotator_cls in CONTENT_PART_ANNOTATORS:
            assert hasattr(annotator_cls, 'PRIORITY')
            assert isinstance(annotator_cls.PRIORITY, int)


# ============================================================
# Registry Tests
# ============================================================

class TestContentPartAnnotatorRegistry:
    """Test the content-part annotator registry."""
    
    def test_all_annotators_in_registry(self):
        """All annotators should be in CONTENT_PART_ANNOTATORS."""
        assert CodeBlockAnnotator in CONTENT_PART_ANNOTATORS
        assert ScriptHeaderAnnotator in CONTENT_PART_ANNOTATORS
        assert LatexContentAnnotator in CONTENT_PART_ANNOTATORS
        assert WikiLinkContentAnnotator in CONTENT_PART_ANNOTATORS
    
    def test_registry_count(self):
        """Registry should have expected number of annotators."""
        assert len(CONTENT_PART_ANNOTATORS) == 4



---
File: tests/unit/test_extractor_utils.py
---
# tests/unit/test_extractor_utils.py
"""Unit tests for extractor utility functions."""

import pytest
from datetime import datetime, timezone

from llm_archive.extractors.base import parse_timestamp, normalize_role, safe_get, compute_content_hash


class TestParseTimestamp:
    """Tests for timestamp parsing."""
    
    def test_parse_epoch_int(self):
        """Test parsing integer epoch timestamp."""
        result = parse_timestamp(1700000000)
        assert result is not None
        assert result.tzinfo is not None
        assert result.year == 2023
    
    def test_parse_epoch_float(self):
        """Test parsing float epoch timestamp."""
        result = parse_timestamp(1700000000.123)
        assert result is not None
        assert result.tzinfo is not None
    
    def test_parse_iso_string(self):
        """Test parsing ISO 8601 string."""
        result = parse_timestamp("2024-01-15T10:00:00Z")
        assert result is not None
        assert result.tzinfo is not None
        assert result.year == 2024
        assert result.month == 1
        assert result.day == 15
    
    def test_parse_iso_string_with_offset(self):
        """Test parsing ISO 8601 with timezone offset."""
        result = parse_timestamp("2024-01-15T10:00:00+05:00")
        assert result is not None
        assert result.tzinfo is not None
    
    def test_parse_none(self):
        """Test parsing None returns None."""
        result = parse_timestamp(None)
        assert result is None
    
    def test_parse_invalid_string(self):
        """Test parsing invalid string returns None."""
        result = parse_timestamp("not a timestamp")
        assert result is None
    
    def test_parse_negative_epoch(self):
        """Test parsing negative epoch (before 1970)."""
        result = parse_timestamp(-1000000)
        assert result is not None
        assert result.year < 1970


class TestNormalizeRole:
    """Tests for role normalization."""
    
    def test_normalize_user(self):
        """Test 'user' stays 'user'."""
        assert normalize_role("user", "chatgpt") == "user"
    
    def test_normalize_assistant(self):
        """Test 'assistant' stays 'assistant'."""
        assert normalize_role("assistant", "chatgpt") == "assistant"
    
    def test_normalize_human_to_user(self):
        """Test 'human' becomes 'user' (Claude format)."""
        assert normalize_role("human", "claude") == "user"
    
    def test_normalize_human_uppercase(self):
        """Test uppercase 'HUMAN' becomes 'user'."""
        assert normalize_role("HUMAN", "claude") == "user"
    
    def test_normalize_system(self):
        """Test 'system' stays 'system'."""
        assert normalize_role("system", "chatgpt") == "system"
    
    def test_normalize_none(self):
        """Test None becomes 'unknown'."""
        assert normalize_role(None, "chatgpt") == "unknown"


class TestSafeGet:
    """Tests for safe dictionary traversal."""
    
    def test_simple_get(self):
        """Test simple key access."""
        data = {"key": "value"}
        assert safe_get(data, "key") == "value"
    
    def test_nested_get(self):
        """Test nested key access."""
        data = {"level1": {"level2": {"level3": "value"}}}
        assert safe_get(data, "level1", "level2", "level3") == "value"
    
    def test_missing_key(self):
        """Test missing key returns default."""
        data = {"key": "value"}
        assert safe_get(data, "missing") is None
        assert safe_get(data, "missing", default="default") == "default"
    
    def test_missing_nested_key(self):
        """Test missing nested key returns default."""
        data = {"level1": {"level2": "value"}}
        assert safe_get(data, "level1", "level2", "level3") is None
    
    def test_non_dict_intermediate(self):
        """Test non-dict intermediate value returns default."""
        data = {"level1": "not a dict"}
        assert safe_get(data, "level1", "level2") is None
    
    def test_none_intermediate(self):
        """Test None intermediate value returns default."""
        data = {"level1": None}
        assert safe_get(data, "level1", "level2") is None


class TestTimestampEdgeCases:
    """Edge case tests for timestamp parsing."""
    
    def test_zero_epoch(self):
        """Test epoch 0 (1970-01-01)."""
        result = parse_timestamp(0)
        assert result is not None
        assert result.year == 1970
    
    def test_very_large_epoch(self):
        """Test very large epoch value."""
        # Year 2100
        result = parse_timestamp(4102444800)
        assert result is not None
        assert result.year == 2100
    
    def test_iso_without_timezone(self):
        """Test ISO string without timezone gets UTC."""
        result = parse_timestamp("2024-01-15T10:00:00")
        assert result is not None
        assert result.tzinfo is not None


class TestComputeContentHash:
    """Tests for content hash computation."""
    
    def test_hash_dict(self):
        """Test hashing a dictionary."""
        data = {'text': 'Hello world', 'role': 'user'}
        result = compute_content_hash(data)
        
        assert result is not None
        assert len(result) == 64  # SHA-256 hex string
    
    def test_hash_string(self):
        """Test hashing a plain string."""
        result = compute_content_hash('Hello world')
        
        assert result is not None
        assert len(result) == 64
    
    def test_hash_is_deterministic(self):
        """Test that same content produces same hash."""
        data = {'message': 'test', 'value': 123}
        
        hash1 = compute_content_hash(data)
        hash2 = compute_content_hash(data)
        
        assert hash1 == hash2
    
    def test_hash_is_order_independent(self):
        """Test that key order doesn't affect hash."""
        data1 = {'a': 1, 'b': 2}
        data2 = {'b': 2, 'a': 1}
        
        hash1 = compute_content_hash(data1)
        hash2 = compute_content_hash(data2)
        
        assert hash1 == hash2
    
    def test_different_content_different_hash(self):
        """Test that different content produces different hash."""
        data1 = {'text': 'Hello'}
        data2 = {'text': 'World'}
        
        hash1 = compute_content_hash(data1)
        hash2 = compute_content_hash(data2)
        
        assert hash1 != hash2
    
    def test_hash_nested_dict(self):
        """Test hashing nested dictionary."""
        data = {
            'content': {
                'parts': ['Hello', {'type': 'code', 'text': 'print(1)'}]
            },
            'metadata': {'author': 'user'}
        }
        
        result = compute_content_hash(data)
        assert len(result) == 64
    
    def test_hash_list(self):
        """Test hashing a list."""
        data = [{'text': 'message 1'}, {'text': 'message 2'}]
        
        result = compute_content_hash(data)
        assert len(result) == 64



---
File: tests/unit/test_models.py
---
# tests/unit/test_models.py
"""Unit tests for SQLAlchemy models - no database required."""

import pytest
from uuid import uuid4
from datetime import datetime, timezone

from llm_archive.models import (
    Dialogue,
    Message,
    ContentPart,
)


class TestDialogueModel:
    """Tests for Dialogue model instantiation."""
    
    def test_create_dialogue_instance(self):
        """Test creating a Dialogue instance."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='conv-001',
            title='Test Conversation',
            source_json={'test': True},
        )
        
        assert dialogue.source == 'chatgpt'
        assert dialogue.source_id == 'conv-001'
        assert dialogue.title == 'Test Conversation'
        assert dialogue.source_json == {'test': True}
    
    def test_dialogue_with_timestamps(self):
        """Test Dialogue with timestamp fields."""
        now = datetime.now(timezone.utc)
        dialogue = Dialogue(
            source='claude',
            source_id='conv-002',
            created_at=now,
            updated_at=now,
            source_json={},
        )
        
        assert dialogue.created_at == now
        assert dialogue.updated_at == now
    
    def test_dialogue_minimal_fields(self):
        """Test Dialogue with only required fields."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='conv-003',
            source_json={},
        )
        
        assert dialogue.source == 'chatgpt'
        assert dialogue.source_id == 'conv-003'
        assert dialogue.title is None
        assert dialogue.created_at is None


class TestMessageModel:
    """Tests for Message model instantiation."""
    
    def test_create_message_instance(self):
        """Test creating a Message instance."""
        dialogue_id = uuid4()
        message = Message(
            dialogue_id=dialogue_id,
            source_id='msg-001',
            role='user',
            source_json={'content': 'Hello'},
        )
        
        assert message.dialogue_id == dialogue_id
        assert message.source_id == 'msg-001'
        assert message.role == 'user'
        assert message.source_json == {'content': 'Hello'}
    
    def test_message_with_parent(self):
        """Test Message with parent reference."""
        dialogue_id = uuid4()
        parent_id = uuid4()
        
        message = Message(
            dialogue_id=dialogue_id,
            source_id='msg-002',
            role='assistant',
            parent_id=parent_id,
            source_json={},
        )
        
        assert message.parent_id == parent_id
    
    def test_message_with_author(self):
        """Test Message with author fields."""
        message = Message(
            dialogue_id=uuid4(),
            source_id='msg-003',
            role='user',
            author_id='user-123',
            author_name='John Doe',
            source_json={},
        )
        
        assert message.author_id == 'user-123'
        assert message.author_name == 'John Doe'
    
    def test_message_with_content_hash(self):
        """Test Message with content hash for change detection."""
        message = Message(
            dialogue_id=uuid4(),
            source_id='msg-004',
            role='user',
            content_hash='a' * 64,  # SHA-256 hash
            source_json={},
        )
        
        assert message.content_hash == 'a' * 64
    
    def test_message_with_deleted_at(self):
        """Test Message with soft delete timestamp."""
        now = datetime.now(timezone.utc)
        message = Message(
            dialogue_id=uuid4(),
            source_id='msg-005',
            role='user',
            deleted_at=now,
            source_json={},
        )
        
        assert message.deleted_at == now
    
    def test_message_not_deleted_by_default(self):
        """Test that deleted_at is None by default."""
        message = Message(
            dialogue_id=uuid4(),
            source_id='msg-006',
            role='user',
            source_json={},
        )
        
        assert message.deleted_at is None


class TestContentPartModel:
    """Tests for ContentPart model instantiation."""
    
    def test_create_text_content_part(self):
        """Test creating a text ContentPart."""
        message_id = uuid4()
        part = ContentPart(
            message_id=message_id,
            sequence=0,
            part_type='text',
            text_content='Hello, world!',
            source_json={'type': 'text'},
        )
        
        assert part.message_id == message_id
        assert part.sequence == 0
        assert part.part_type == 'text'
        assert part.text_content == 'Hello, world!'
    
    def test_create_code_content_part(self):
        """Test creating a code ContentPart with language."""
        part = ContentPart(
            message_id=uuid4(),
            sequence=1,
            part_type='code',
            text_content='print("hello")',
            language='python',
            source_json={'type': 'code', 'language': 'python'},
        )
        
        assert part.part_type == 'code'
        assert part.language == 'python'
        assert part.text_content == 'print("hello")'
    
    def test_create_image_content_part(self):
        """Test creating an image ContentPart with media type and URL."""
        part = ContentPart(
            message_id=uuid4(),
            sequence=0,
            part_type='image',
            media_type='image/png',
            url='https://example.com/image.png',
            source_json={'type': 'image'},
        )
        
        assert part.part_type == 'image'
        assert part.media_type == 'image/png'
        assert part.url == 'https://example.com/image.png'
    
    def test_create_tool_use_content_part(self):
        """Test creating a tool_use ContentPart."""
        part = ContentPart(
            message_id=uuid4(),
            sequence=0,
            part_type='tool_use',
            tool_name='web_search',
            tool_use_id='tool-123',
            tool_input={'query': 'test search'},
            source_json={'type': 'tool_use'},
        )
        
        assert part.part_type == 'tool_use'
        assert part.tool_name == 'web_search'
        assert part.tool_use_id == 'tool-123'
        assert part.tool_input == {'query': 'test search'}
    
    def test_create_tool_result_content_part(self):
        """Test creating a tool_result ContentPart."""
        part = ContentPart(
            message_id=uuid4(),
            sequence=1,
            part_type='tool_result',
            tool_use_id='tool-123',
            text_content='Search results: ...',
            is_error=False,
            source_json={'type': 'tool_result'},
        )
        
        assert part.part_type == 'tool_result'
        assert part.tool_use_id == 'tool-123'
        assert part.is_error is False


class TestModelTableNames:
    """Tests for model table name configuration."""
    
    def test_dialogue_table_name(self):
        """Test Dialogue uses raw schema."""
        assert Dialogue.__tablename__ == 'dialogues'
        assert Dialogue.__table__.schema == 'raw'
    
    def test_message_table_name(self):
        """Test Message uses raw schema."""
        assert Message.__tablename__ == 'messages'
        assert Message.__table__.schema == 'raw'
    
    def test_content_part_table_name(self):
        """Test ContentPart uses raw schema."""
        assert ContentPart.__tablename__ == 'content_parts'
        assert ContentPart.__table__.schema == 'raw'
    



---
File: tests/unit/test_prompt_response.py
---
# tests/unit/test_prompt_response.py
"""Unit tests for prompt-response builders and annotators."""

import pytest
from datetime import datetime, timezone
from uuid import uuid4

from llm_archive.annotators.prompt_response import (
    PromptResponseData,
    PromptResponseAnnotator,
    WikiCandidateAnnotator,
    NaiveTitleAnnotator,
    HasCodeAnnotator,
    HasLatexAnnotator,
)
from llm_archive.annotations.core import ValueType, EntityType, AnnotationResult


# ============================================================
# Test Fixtures
# ============================================================

@pytest.fixture
def pr_id():
    """Generate a prompt-response ID."""
    return uuid4()


def make_pr_data(
    prompt_text: str = "Test prompt",
    response_text: str = "Test response",
    pr_id: uuid4 = None,
    response_role: str = 'assistant',
    prompt_role: str = 'user',
) -> PromptResponseData:
    """Create PromptResponseData for testing."""
    return PromptResponseData(
        prompt_response_id=pr_id or uuid4(),
        dialogue_id=uuid4(),
        prompt_message_id=uuid4(),
        response_message_id=uuid4(),
        prompt_text=prompt_text,
        response_text=response_text,
        prompt_word_count=len(prompt_text.split()) if prompt_text else 0,
        response_word_count=len(response_text.split()) if response_text else 0,
        prompt_role=prompt_role,
        response_role=response_role,
        created_at=datetime.now(timezone.utc),
    )


# ============================================================
# WikiCandidateAnnotator Tests
# ============================================================

class TestWikiCandidateAnnotator:
    """Test wiki article detection."""
    
    def test_detects_wiki_links(self, pr_id):
        """Should detect responses with wiki links."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="# Cats\n\nCats are [[mammals]] that are [[domesticated]].",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        # Should have both exchange_type and wiki_link_count annotations
        assert len(results) == 2
        
        exchange_type_result = next(r for r in results if r.key == 'exchange_type')
        assert exchange_type_result.value == 'wiki_article'
        assert exchange_type_result.value_type == ValueType.STRING
        assert exchange_type_result.reason == 'wiki_links_detected'
        
        count_result = next(r for r in results if r.key == 'wiki_link_count')
        assert count_result.value == 2
        assert count_result.value_type == ValueType.NUMERIC
    
    def test_high_confidence_multiple_links(self, pr_id):
        """Should have higher confidence with 3+ wiki links."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="[[Cats]] are [[mammals]]. They eat [[mice]] and [[birds]].",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        exchange_type_result = next(r for r in results if r.key == 'exchange_type')
        assert exchange_type_result.confidence == 0.95
    
    def test_lower_confidence_single_link(self, pr_id):
        """Should have lower confidence with just 1-2 links."""
        data = make_pr_data(
            prompt_text="Tell me about cats",
            response_text="Cats are [[mammals]].",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        exchange_type_result = next(r for r in results if r.key == 'exchange_type')
        assert exchange_type_result.confidence == 0.8
    
    def test_no_wiki_links(self, pr_id):
        """Should not detect if no wiki links."""
        data = make_pr_data(
            prompt_text="Tell me about cats",
            response_text="Cats are mammals. They are cute.",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_skips_non_assistant(self, pr_id):
        """Should skip non-assistant responses."""
        data = make_pr_data(
            prompt_text="Write [[wiki]] style",
            response_text="Here's [[content]]",
            pr_id=pr_id,
            response_role='user',  # Not assistant
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_counts_links_correctly(self, pr_id):
        """Should count wiki links correctly."""
        data = make_pr_data(
            response_text="[[One]] [[Two]] [[Three]] [[Four]] [[Five]]",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        count_result = next(r for r in results if r.key == 'wiki_link_count')
        assert count_result.value == 5
    
    def test_handles_empty_brackets(self, pr_id):
        """Should count empty brackets as potential links."""
        data = make_pr_data(
            response_text="Empty [[]] brackets and [[valid]] link",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        count_result = next(r for r in results if r.key == 'wiki_link_count')
        assert count_result.value == 2


# ============================================================
# NaiveTitleAnnotator Tests
# ============================================================

class TestNaiveTitleAnnotator:
    """Test naive title extraction."""
    
    def test_extracts_markdown_h1(self, pr_id):
        """Should extract # Title."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="# The Domestic Cat\n\n[[Cats]] are mammals...",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 1
        assert results[0].value == 'The Domestic Cat'
        assert results[0].key == 'proposed_title'
        assert results[0].value_type == ValueType.STRING
        assert results[0].reason == 'markdown_header'
    
    def test_extracts_markdown_h2(self, pr_id):
        """Should extract ## Title."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="## Feline History\n\nThe history of cats...",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 1
        assert results[0].value == 'Feline History'
        assert results[0].reason == 'markdown_header'
    
    def test_extracts_markdown_h3(self, pr_id):
        """Should extract ### Title."""
        data = make_pr_data(
            response_text="### Deep Section\n\nContent here...",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 1
        assert results[0].value == 'Deep Section'
    
    def test_extracts_bold_title(self, pr_id):
        """Should extract **Title**."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="**The Domestic Cat**\n\n[[Cats]] are mammals...",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 1
        assert results[0].value == 'The Domestic Cat'
        assert results[0].reason == 'bold_header'
    
    def test_extracts_bold_with_subtitle(self, pr_id):
        """Should extract **Title** - Subtitle pattern."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="**Felis catus** - The Domestic Cat\n\nContent...",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 1
        assert results[0].value == 'Felis catus'
        assert results[0].reason == 'bold_header_with_suffix'
    
    def test_no_title_preamble(self, pr_id):
        """Should return nothing if first line is preamble."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="Sure, here's an article about cats:\n\n# The Domestic Cat\n\n...",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        # This is expected - naive extractor misses the title
        # because first line is a preamble
        assert len(results) == 0
    
    def test_no_title_plain_text(self, pr_id):
        """Should return nothing if no clear title format."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="Cats have been domesticated for thousands of years...",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_skips_non_assistant(self, pr_id):
        """Should skip non-assistant responses."""
        data = make_pr_data(
            prompt_text="# My Title",
            response_text="# Another Title",
            pr_id=pr_id,
            response_role='user',
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_empty_response(self, pr_id):
        """Should handle empty response."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_none_response(self, pr_id):
        """Should handle None response."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="placeholder",
            pr_id=pr_id,
        )
        data.response_text = None  # Override
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_whitespace_only_first_line(self, pr_id):
        """Should skip whitespace-only first lines."""
        data = make_pr_data(
            response_text="   \n# Real Title\n\nContent",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        # .strip() is called on the document, so the title should be extracted.
        assert len(results) == 1
    
    def test_strips_title_whitespace(self, pr_id):
        """Should strip whitespace from extracted title."""
        data = make_pr_data(
            response_text="#   Spaced Title   \n\nContent",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 1
        assert results[0].value == 'Spaced Title'


# ============================================================
# Annotation Filter Tests (class attributes)
# ============================================================

class TestAnnotatorFilters:
    """Test annotation filter attributes."""
    
    def test_wiki_candidate_has_no_requirements(self):
        """WikiCandidateAnnotator should have no prerequisites."""
        assert WikiCandidateAnnotator.REQUIRES_FLAGS == []
        assert WikiCandidateAnnotator.REQUIRES_STRINGS == []
        assert WikiCandidateAnnotator.SKIP_IF_FLAGS == []
        assert WikiCandidateAnnotator.SKIP_IF_STRINGS == []
    
    def test_naive_title_requires_wiki(self):
        """NaiveTitleAnnotator should require wiki_article."""
        assert ('exchange_type', 'wiki_article') in NaiveTitleAnnotator.REQUIRES_STRINGS
    
    def test_annotator_metadata(self):
        """Check annotator class metadata."""
        assert WikiCandidateAnnotator.ENTITY_TYPE == EntityType.PROMPT_RESPONSE
        assert WikiCandidateAnnotator.ANNOTATION_KEY == 'exchange_type'
        assert WikiCandidateAnnotator.VALUE_TYPE == ValueType.STRING
        
        assert NaiveTitleAnnotator.ENTITY_TYPE == EntityType.PROMPT_RESPONSE
        assert NaiveTitleAnnotator.ANNOTATION_KEY == 'proposed_title'
        assert NaiveTitleAnnotator.VALUE_TYPE == ValueType.STRING
        
        # Wiki detection should run before title extraction
        assert WikiCandidateAnnotator.PRIORITY > NaiveTitleAnnotator.PRIORITY
    
    def test_custom_annotator_with_filters(self):
        """Test defining custom annotator with filters."""
        
        class PreambleDetector(PromptResponseAnnotator):
            ANNOTATION_KEY = 'has_preamble'
            VALUE_TYPE = ValueType.FLAG
            REQUIRES_STRINGS = [('exchange_type', 'wiki_article')]
            SKIP_IF_FLAGS = ['preamble_checked']
            
            def annotate(self, data):
                return []
        
        assert PreambleDetector.REQUIRES_STRINGS == [('exchange_type', 'wiki_article')]
        assert PreambleDetector.SKIP_IF_FLAGS == ['preamble_checked']


# ============================================================
# AnnotationResult Tests
# ============================================================

class TestAnnotationResult:
    """Test AnnotationResult dataclass behavior."""
    
    def test_result_with_reason(self, pr_id):
        """Results should include reason when provided."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="# Cats\n\n[[Cats]] are mammals.",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        exchange_type_result = next(r for r in results if r.key == 'exchange_type')
        assert exchange_type_result.reason == 'wiki_links_detected'
    
    def test_key_is_required(self, pr_id):
        """Key should always be set on results."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="# Title\n\n[[link]]",
            pr_id=pr_id,
        )
        
        wiki_annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        wiki_results = wiki_annotator.annotate(data)
        
        title_annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        title_results = title_annotator.annotate(data)
        
        for result in wiki_results + title_results:
            assert result.key is not None
    
    def test_value_type_is_set(self, pr_id):
        """Results should have explicit value_type."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="# Title\n\n[[link1]] [[link2]] [[link3]]",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        # Should have string and numeric results
        string_results = [r for r in results if r.value_type == ValueType.STRING]
        numeric_results = [r for r in results if r.value_type == ValueType.NUMERIC]
        
        assert len(string_results) >= 1
        assert len(numeric_results) >= 1


# ============================================================
# PromptResponseData Tests
# ============================================================

class TestPromptResponseData:
    """Test PromptResponseData dataclass."""
    
    def test_all_fields_accessible(self):
        """All fields should be accessible."""
        data = make_pr_data(
            prompt_text="Hello",
            response_text="World",
        )
        
        assert data.prompt_text == "Hello"
        assert data.response_text == "World"
        assert data.prompt_role == 'user'
        assert data.response_role == 'assistant'
        assert isinstance(data.prompt_response_id, type(uuid4()))
        assert isinstance(data.dialogue_id, type(uuid4()))
    
    def test_word_counts_calculated(self):
        """Word counts should be calculated from text."""
        data = make_pr_data(
            prompt_text="one two three",
            response_text="four five six seven",
        )
        
        assert data.prompt_word_count == 3
        assert data.response_word_count == 4
    
    def test_handles_none_text(self):
        """Should handle None text gracefully."""
        data = PromptResponseData(
            prompt_response_id=uuid4(),
            dialogue_id=uuid4(),
            prompt_message_id=uuid4(),
            response_message_id=uuid4(),
            prompt_text=None,
            response_text=None,
            prompt_word_count=0,
            response_word_count=0,
            prompt_role='user',
            response_role='assistant',
            created_at=datetime.now(timezone.utc),
        )
        
        assert data.prompt_text is None
        assert data.response_text is None


# ============================================================
# HasCodeAnnotator Tests
# ============================================================

class TestHasCodeAnnotator:
    """Test code detection annotator."""
    
    def test_detects_code_blocks(self, pr_id):
        """Should detect ``` code blocks."""
        data = make_pr_data(
            response_text="Here's some code:\n```python\nprint('hello')\n```",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        flag_results = [r for r in results if r.key == 'has_code']
        assert len(flag_results) == 1
        assert flag_results[0].value_type == ValueType.FLAG
        assert flag_results[0].confidence >= 0.9
        
        evidence_results = [r for r in results if r.key == 'code_evidence']
        evidence_values = {r.value for r in evidence_results}
        assert 'code_block' in evidence_values
    
    def test_detects_shebang(self, pr_id):
        """Should detect shebang lines."""
        data = make_pr_data(
            response_text="#!/usr/bin/env python\nprint('hello')",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_code' for r in results)
        evidence_values = {r.value for r in results if r.key == 'code_evidence'}
        assert 'shebang' in evidence_values
    
    def test_detects_c_include(self, pr_id):
        """Should detect C/C++ includes."""
        data = make_pr_data(
            response_text='#include <stdio.h>\nint main() { return 0; }',
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_code' for r in results)
        evidence_values = {r.value for r in results if r.key == 'code_evidence'}
        assert 'c_include' in evidence_values
    
    def test_detects_python_function(self, pr_id):
        """Should detect Python function definitions."""
        data = make_pr_data(
            response_text="def hello_world():\n    print('hello')",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_code' for r in results)
        evidence_values = {r.value for r in results if r.key == 'code_evidence'}
        assert 'python_function' in evidence_values
    
    def test_detects_js_function(self, pr_id):
        """Should detect JavaScript function definitions."""
        data = make_pr_data(
            response_text="function hello() {\n  console.log('hello');\n}",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_code' for r in results)
        evidence_values = {r.value for r in results if r.key == 'code_evidence'}
        assert 'js_function' in evidence_values
    
    def test_detects_arrow_function(self, pr_id):
        """Should detect arrow functions."""
        data = make_pr_data(
            response_text="const hello = (name) => console.log(`Hello ${name}`);",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_code' for r in results)
        evidence_values = {r.value for r in results if r.key == 'code_evidence'}
        assert 'arrow_function' in evidence_values
    
    def test_detects_python_import(self, pr_id):
        """Should detect Python imports."""
        data = make_pr_data(
            response_text="import pandas as pd\nfrom datetime import datetime",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_code' for r in results)
        evidence_values = {r.value for r in results if r.key == 'code_evidence'}
        assert 'python_import' in evidence_values
    
    def test_no_code_in_plain_text(self, pr_id):
        """Should not detect code in plain text."""
        data = make_pr_data(
            response_text="Cats are wonderful pets. They like to sleep and play.",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_skips_non_assistant(self, pr_id):
        """Should skip non-assistant responses."""
        data = make_pr_data(
            response_text="```python\nprint('hello')\n```",
            pr_id=pr_id,
            response_role='user',
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_multiple_evidence_types(self, pr_id):
        """Should detect multiple evidence types."""
        data = make_pr_data(
            response_text="```python\nimport os\ndef main():\n    pass\n```",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        evidence_results = [r for r in results if r.key == 'code_evidence']
        assert len(evidence_results) >= 2  # code_block + python_function + python_import


# ============================================================
# HasLatexAnnotator Tests
# ============================================================

class TestHasLatexAnnotator:
    """Test LaTeX detection annotator."""
    
    def test_detects_display_math(self, pr_id):
        """Should detect display math $$...$$."""
        data = make_pr_data(
            response_text="The equation is: $$E = mc^2$$",
            pr_id=pr_id,
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_latex' for r in results)
        latex_types = {r.value for r in results if r.key == 'latex_type'}
        assert 'display' in latex_types
    
    def test_detects_bracket_display_math(self, pr_id):
        """Should detect \\[...\\] display math."""
        data = make_pr_data(
            response_text="The integral is: \\[\\int_0^\\infty e^{-x} dx = 1\\]",
            pr_id=pr_id,
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_latex' for r in results)
    
    def test_detects_latex_commands(self, pr_id):
        """Should detect LaTeX commands."""
        data = make_pr_data(
            response_text="Use \\frac{a}{b} for fractions and \\sqrt{x} for roots.",
            pr_id=pr_id,
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_latex' for r in results)
        latex_types = {r.value for r in results if r.key == 'latex_type'}
        assert 'commands' in latex_types
    
    def test_detects_greek_letters(self, pr_id):
        """Should detect Greek letter commands."""
        data = make_pr_data(
            response_text="The angle \\theta is measured from \\alpha to \\omega.",
            pr_id=pr_id,
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_latex' for r in results)
    
    def test_no_latex_in_plain_text(self, pr_id):
        """Should not detect LaTeX in plain text."""
        data = make_pr_data(
            response_text="The value of pi is approximately 3.14159.",
            pr_id=pr_id,
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_skips_non_assistant(self, pr_id):
        """Should skip non-assistant responses."""
        data = make_pr_data(
            response_text="$$E = mc^2$$",
            pr_id=pr_id,
            response_role='user',
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_high_confidence_for_display_math(self, pr_id):
        """Should have high confidence for display math."""
        data = make_pr_data(
            response_text="$$\\sum_{i=1}^n i = \\frac{n(n+1)}{2}$$",
            pr_id=pr_id,
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        flag_result = next(r for r in results if r.key == 'has_latex')
        assert flag_result.confidence >= 0.9
    
    def test_lower_confidence_for_commands_only(self, pr_id):
        """Should have lower confidence for commands without display math."""
        data = make_pr_data(
            response_text="Use \\alpha and \\beta for parameters.",
            pr_id=pr_id,
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        flag_result = next(r for r in results if r.key == 'has_latex')
        assert flag_result.confidence < 0.9


# ============================================================
# Annotator Registry Tests
# ============================================================

class TestAnnotatorRegistry:
    """Test the annotator registry and runner."""
    
    def test_all_annotators_in_registry(self):
        """All annotators should be in PROMPT_RESPONSE_ANNOTATORS."""
        from llm_archive.annotators.prompt_response import PROMPT_RESPONSE_ANNOTATORS
        
        assert WikiCandidateAnnotator in PROMPT_RESPONSE_ANNOTATORS
        assert NaiveTitleAnnotator in PROMPT_RESPONSE_ANNOTATORS
        assert HasCodeAnnotator in PROMPT_RESPONSE_ANNOTATORS
        assert HasLatexAnnotator in PROMPT_RESPONSE_ANNOTATORS
    
    def test_annotators_have_unique_keys(self):
        """Each annotator should have a unique ANNOTATION_KEY."""
        from llm_archive.annotators.prompt_response import PROMPT_RESPONSE_ANNOTATORS
        
        keys = [cls.ANNOTATION_KEY for cls in PROMPT_RESPONSE_ANNOTATORS]
        assert len(keys) == len(set(keys)), "Duplicate ANNOTATION_KEYs found"
    
    def test_priority_ordering(self):
        """Annotators should have distinct priorities for deterministic ordering."""
        from llm_archive.annotators.prompt_response import PROMPT_RESPONSE_ANNOTATORS
        
        priorities = [cls.PRIORITY for cls in PROMPT_RESPONSE_ANNOTATORS]
        # Note: Priorities don't have to be unique, but it helps with debugging
        assert len(priorities) == len(PROMPT_RESPONSE_ANNOTATORS)


