{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2a8a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genson import SchemaBuilder\n",
    "\n",
    "seed_schema = {\n",
    "    'type': 'array',\n",
    "    'items': {\n",
    "        'type': 'object',\n",
    "        'properties': {\n",
    "            'mapping': {\n",
    "                'type': 'object',\n",
    "                'patternProperties': {\n",
    "                    # UUID pattern - GenSON will fill in the actual schema\n",
    "                    r'^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$': None,\n",
    "                    # Also handle the client-created-root pattern\n",
    "                    r'^client-created-': None\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "builder = SchemaBuilder()\n",
    "builder.add_schema(seed_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1372e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dmarx/proj/chat2obs/src\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "import json\n",
    "PATH=str((Path().cwd().parent /'src').absolute())\n",
    "print(PATH)\n",
    "if PATH not in sys.path:\n",
    "    sys.path.append(PATH)\n",
    "import chat2obs\n",
    "from chat2obs.process_chatgpt import load_conversations, convs_to_articles\n",
    "\n",
    "root = \"../data/ingestion/chatgpt/a40ff5f79c1b3edd3c366f0f628fb79170bae83ecf3a1758b5b258c71f843f53-2025-06-05-03-28-15-df2ed357a4e64443bf464446686c9692/\"\n",
    "fpath = Path(root) / \"conversations.json\"\n",
    "convs = json.load(fpath.open())\n",
    "\n",
    "builder.add_object(convs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411b8899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$schema': 'http://json-schema.org/schema#',\n",
       " 'type': 'array',\n",
       " 'items': {'type': 'object',\n",
       "  'properties': {'mapping': {'type': 'object',\n",
       "    'patternProperties': {'^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$': {'type': 'object',\n",
       "      'properties': {'id': {'type': 'string'},\n",
       "       'message': {'anyOf': [{'type': 'null'},\n",
       "         {'type': 'object',\n",
       "          'properties': {'id': {'type': 'string'},\n",
       "           'author': {'type': 'object',\n",
       "            'properties': {'role': {'type': 'string'},\n",
       "             'name': {'type': ['null', 'string']},\n",
       "             'metadata': {'type': 'object',\n",
       "              'properties': {'real_author': {'type': 'string'}}}},\n",
       "            'required': ['metadata', 'name', 'role']},\n",
       "           'create_time': {'type': ['null', 'number']},\n",
       "           'update_time': {'type': ['null', 'number']},\n",
       "           'content': {'type': 'object',\n",
       "            'properties': {'content_type': {'type': 'string'},\n",
       "             'parts': {'type': 'array',\n",
       "              'items': {'anyOf': [{'type': 'string'},\n",
       "                {'type': 'object',\n",
       "                 'properties': {'content_type': {'type': 'string'},\n",
       "                  'asset_pointer': {'type': 'string'},\n",
       "                  'size_bytes': {'type': 'integer'},\n",
       "                  'width': {'type': 'integer'},\n",
       "                  'height': {'type': 'integer'},\n",
       "                  'fovea': {'type': ['integer', 'null']},\n",
       "                  'metadata': {'anyOf': [{'type': 'null'},\n",
       "                    {'type': 'object',\n",
       "                     'properties': {'dalle': {'anyOf': [{'type': 'null'},\n",
       "                        {'type': 'object',\n",
       "                         'properties': {'gen_id': {'type': 'string'},\n",
       "                          'prompt': {'type': 'string'},\n",
       "                          'seed': {'type': ['integer', 'null']},\n",
       "                          'parent_gen_id': {'type': ['null', 'string']},\n",
       "                          'edit_op': {'type': ['null', 'string']},\n",
       "                          'serialization_title': {'type': 'string'}},\n",
       "                         'required': ['edit_op',\n",
       "                          'gen_id',\n",
       "                          'parent_gen_id',\n",
       "                          'prompt',\n",
       "                          'seed',\n",
       "                          'serialization_title']}]},\n",
       "                      'gizmo': {'type': 'null'},\n",
       "                      'generation': {'anyOf': [{'type': 'null'},\n",
       "                        {'type': 'object',\n",
       "                         'properties': {'gen_id': {'type': 'string'},\n",
       "                          'gen_size': {'type': 'string'},\n",
       "                          'seed': {'type': 'null'},\n",
       "                          'parent_gen_id': {'type': 'null'},\n",
       "                          'height': {'type': 'integer'},\n",
       "                          'width': {'type': 'integer'},\n",
       "                          'transparent_background': {'type': 'boolean'},\n",
       "                          'serialization_title': {'type': 'string'}},\n",
       "                         'required': ['gen_id',\n",
       "                          'gen_size',\n",
       "                          'height',\n",
       "                          'parent_gen_id',\n",
       "                          'seed',\n",
       "                          'serialization_title',\n",
       "                          'transparent_background',\n",
       "                          'width']}]},\n",
       "                      'container_pixel_height': {'type': ['integer', 'null']},\n",
       "                      'container_pixel_width': {'type': ['integer', 'null']},\n",
       "                      'emu_omit_glimpse_image': {'type': 'null'},\n",
       "                      'emu_patches_override': {'type': 'null'},\n",
       "                      'sanitized': {'type': 'boolean'},\n",
       "                      'asset_pointer_link': {'type': 'null'},\n",
       "                      'watermarked_asset_pointer': {'type': 'null'},\n",
       "                      'start_timestamp': {'type': 'null'},\n",
       "                      'end_timestamp': {'type': 'null'},\n",
       "                      'pretokenized_vq': {'type': 'null'},\n",
       "                      'interruptions': {'type': 'null'},\n",
       "                      'original_audio_source': {'type': 'null'},\n",
       "                      'transcription': {'type': 'null'},\n",
       "                      'word_transcription': {'type': 'null'},\n",
       "                      'start': {'type': 'number'},\n",
       "                      'end': {'type': 'number'}}}]},\n",
       "                  'expiry_datetime': {'type': 'null'},\n",
       "                  'frames_asset_pointers': {'type': 'array'},\n",
       "                  'video_container_asset_pointer': {'type': 'null'},\n",
       "                  'audio_asset_pointer': {'type': 'object',\n",
       "                   'properties': {'expiry_datetime': {'type': 'null'},\n",
       "                    'content_type': {'type': 'string'},\n",
       "                    'asset_pointer': {'type': 'string'},\n",
       "                    'size_bytes': {'type': 'integer'},\n",
       "                    'format': {'type': 'string'},\n",
       "                    'metadata': {'type': 'object',\n",
       "                     'properties': {'start_timestamp': {'type': 'null'},\n",
       "                      'end_timestamp': {'type': 'null'},\n",
       "                      'pretokenized_vq': {'type': 'null'},\n",
       "                      'interruptions': {'type': 'null'},\n",
       "                      'original_audio_source': {'type': 'null'},\n",
       "                      'transcription': {'type': 'null'},\n",
       "                      'word_transcription': {'type': 'null'},\n",
       "                      'start': {'type': 'number'},\n",
       "                      'end': {'type': 'number'}},\n",
       "                     'required': ['end',\n",
       "                      'end_timestamp',\n",
       "                      'interruptions',\n",
       "                      'original_audio_source',\n",
       "                      'pretokenized_vq',\n",
       "                      'start',\n",
       "                      'start_timestamp',\n",
       "                      'transcription',\n",
       "                      'word_transcription']}},\n",
       "                   'required': ['asset_pointer',\n",
       "                    'content_type',\n",
       "                    'expiry_datetime',\n",
       "                    'format',\n",
       "                    'metadata',\n",
       "                    'size_bytes']},\n",
       "                  'audio_start_timestamp': {'type': 'number'},\n",
       "                  'text': {'type': 'string'},\n",
       "                  'direction': {'type': 'string'},\n",
       "                  'decoding_id': {'type': 'null'},\n",
       "                  'format': {'type': 'string'}},\n",
       "                 'required': ['content_type']}]}},\n",
       "             'language': {'type': 'string'},\n",
       "             'response_format_name': {'type': 'null'},\n",
       "             'text': {'type': 'string'},\n",
       "             'thoughts': {'type': 'array',\n",
       "              'items': {'type': 'object',\n",
       "               'properties': {'summary': {'type': 'string'},\n",
       "                'content': {'type': 'string'}},\n",
       "               'required': ['content', 'summary']}},\n",
       "             'source_analysis_msg_id': {'type': 'string'},\n",
       "             'content': {'type': 'string'},\n",
       "             'result': {'type': 'string'},\n",
       "             'summary': {'type': ['null', 'string']},\n",
       "             'assets': {'type': 'array'},\n",
       "             'tether_id': {'type': 'null'},\n",
       "             'url': {'type': 'string'},\n",
       "             'domain': {'type': 'string'},\n",
       "             'title': {'type': 'string'},\n",
       "             'snippet': {'type': 'string'},\n",
       "             'pub_date': {'type': 'null'},\n",
       "             'crawl_date': {'type': 'null'},\n",
       "             'pub_timestamp': {'type': 'number'},\n",
       "             'ref_id': {'type': 'string'},\n",
       "             'name': {'type': 'string'}},\n",
       "            'required': ['content_type']},\n",
       "           'status': {'type': 'string'},\n",
       "           'end_turn': {'type': ['boolean', 'null']},\n",
       "           'weight': {'type': 'number'},\n",
       "           'metadata': {'type': 'object',\n",
       "            'properties': {'is_visually_hidden_from_conversation': {'type': 'boolean'},\n",
       "             'selected_sources': {'type': 'array',\n",
       "              'items': {'type': 'string'}},\n",
       "             'selected_github_repos': {'type': 'array'},\n",
       "             'serialization_metadata': {'type': 'object',\n",
       "              'properties': {'custom_symbol_offsets': {'type': 'array',\n",
       "                'items': {'type': 'object',\n",
       "                 'properties': {'symbol': {'type': 'string'},\n",
       "                  'startIndex': {'type': 'integer'},\n",
       "                  'endIndex': {'type': 'integer'}},\n",
       "                 'required': ['endIndex', 'startIndex', 'symbol']}}},\n",
       "              'required': ['custom_symbol_offsets']},\n",
       "             'request_id': {'type': ['null', 'string']},\n",
       "             'message_source': {'type': 'null'},\n",
       "             'timestamp_': {'type': 'string'},\n",
       "             'message_type': {'type': ['null', 'string']},\n",
       "             'model_slug': {'type': 'string'},\n",
       "             'default_model_slug': {'type': 'string'},\n",
       "             'parent_id': {'type': 'string'},\n",
       "             'is_complete': {'type': 'boolean'},\n",
       "             'finish_details': {'type': 'object',\n",
       "              'properties': {'type': {'type': 'string'},\n",
       "               'stop_tokens': {'type': 'array', 'items': {'type': 'integer'}},\n",
       "               'stop': {'type': 'string'}},\n",
       "              'required': ['type']},\n",
       "             'sonic_classification_result': {'type': 'object',\n",
       "              'properties': {'latency_ms': {'type': ['null', 'number']},\n",
       "               'search_prob': {'type': ['null', 'number']},\n",
       "               'force_search_threshold': {'type': ['null', 'number']},\n",
       "               'classifier_config_name': {'type': 'string'}},\n",
       "              'required': ['latency_ms', 'search_prob']},\n",
       "             'citations': {'type': 'array',\n",
       "              'items': {'type': 'object',\n",
       "               'properties': {'start_ix': {'type': 'integer'},\n",
       "                'end_ix': {'type': 'integer'},\n",
       "                'invalid_reason': {'type': 'string'},\n",
       "                'citation_format_type': {'type': 'string'},\n",
       "                'metadata': {'type': 'object',\n",
       "                 'properties': {'type': {'type': 'string'},\n",
       "                  'title': {'type': 'string'},\n",
       "                  'url': {'type': 'string'},\n",
       "                  'text': {'type': 'string'},\n",
       "                  'pub_date': {'type': ['null', 'string']},\n",
       "                  'extra': {'anyOf': [{'type': 'null'},\n",
       "                    {'type': 'object',\n",
       "                     'properties': {'cited_message_idx': {'type': 'integer'},\n",
       "                      'search_result_idx': {'type': ['integer', 'null']},\n",
       "                      'evidence_text': {'type': 'string'},\n",
       "                      'cloud_doc_url': {'type': 'null'}},\n",
       "                     'required': ['cited_message_idx', 'evidence_text']}]},\n",
       "                  'og_tags': {'type': 'null'}},\n",
       "                 'required': ['extra',\n",
       "                  'pub_date',\n",
       "                  'text',\n",
       "                  'title',\n",
       "                  'type',\n",
       "                  'url']}},\n",
       "               'required': ['end_ix', 'start_ix']}},\n",
       "             'content_references': {'type': 'array',\n",
       "              'items': {'type': 'object',\n",
       "               'properties': {'matched_text': {'type': 'string'},\n",
       "                'start_idx': {'type': 'integer'},\n",
       "                'end_idx': {'type': 'integer'},\n",
       "                'refs': {'type': 'array',\n",
       "                 'items': {'anyOf': [{'type': 'string'},\n",
       "                   {'type': 'object',\n",
       "                    'properties': {'turn_index': {'type': 'integer'},\n",
       "                     'ref_type': {'type': 'string'},\n",
       "                     'ref_index': {'type': 'integer'}},\n",
       "                    'required': ['ref_index', 'ref_type', 'turn_index']}]}},\n",
       "                'alt': {'type': ['null', 'string']},\n",
       "                'prompt_text': {'type': ['null', 'string']},\n",
       "                'type': {'type': 'string'},\n",
       "                'invalid': {'type': 'boolean'},\n",
       "                'safe_urls': {'type': 'array', 'items': {'type': 'string'}},\n",
       "                'attributable_index': {'type': 'string'},\n",
       "                'attributions': {'type': 'null'},\n",
       "                'attributions_debug': {'type': 'null'},\n",
       "                'items': {'type': 'array',\n",
       "                 'items': {'type': 'object',\n",
       "                  'properties': {'title': {'type': 'string'},\n",
       "                   'url': {'type': 'string'},\n",
       "                   'pub_date': {'type': ['null', 'number']},\n",
       "                   'snippet': {'type': ['null', 'string']},\n",
       "                   'attribution_segments': {'anyOf': [{'type': 'null'},\n",
       "                     {'type': 'array', 'items': {'type': 'string'}}]},\n",
       "                   'supporting_websites': {'type': 'array',\n",
       "                    'items': {'type': 'object',\n",
       "                     'properties': {'title': {'type': 'string'},\n",
       "                      'url': {'type': 'string'},\n",
       "                      'pub_date': {'type': ['null', 'number']},\n",
       "                      'snippet': {'type': 'string'},\n",
       "                      'attribution': {'type': 'string'}},\n",
       "                     'required': ['attribution',\n",
       "                      'pub_date',\n",
       "                      'snippet',\n",
       "                      'title',\n",
       "                      'url']}},\n",
       "                   'refs': {'type': 'array',\n",
       "                    'items': {'type': 'object',\n",
       "                     'properties': {'turn_index': {'type': 'integer'},\n",
       "                      'ref_type': {'type': 'string'},\n",
       "                      'ref_index': {'type': 'integer'}},\n",
       "                     'required': ['ref_index', 'ref_type', 'turn_index']}},\n",
       "                   'hue': {'type': 'null'},\n",
       "                   'attributions': {'type': 'null'},\n",
       "                   'attribution': {'type': 'string'}},\n",
       "                  'required': ['pub_date', 'snippet', 'title', 'url']}},\n",
       "                'status': {'type': 'string'},\n",
       "                'error': {'type': 'null'},\n",
       "                'style': {'type': ['null', 'string']},\n",
       "                'sources': {'type': 'array',\n",
       "                 'items': {'type': 'object',\n",
       "                  'properties': {'title': {'type': 'string'},\n",
       "                   'url': {'type': 'string'},\n",
       "                   'attribution': {'type': 'string'}},\n",
       "                  'required': ['attribution', 'title', 'url']}},\n",
       "                'has_images': {'type': 'boolean'},\n",
       "                'images': {'type': 'array',\n",
       "                 'items': {'type': 'object',\n",
       "                  'properties': {'url': {'type': 'string'},\n",
       "                   'content_url': {'type': 'string'},\n",
       "                   'thumbnail_url': {'type': 'string'},\n",
       "                   'title': {'type': 'string'},\n",
       "                   'content_size': {'type': 'object',\n",
       "                    'properties': {'width': {'type': 'integer'},\n",
       "                     'height': {'type': 'integer'}},\n",
       "                    'required': ['height', 'width']},\n",
       "                   'thumbnail_size': {'type': 'object',\n",
       "                    'properties': {'width': {'type': 'integer'},\n",
       "                     'height': {'type': 'integer'}},\n",
       "                    'required': ['height', 'width']},\n",
       "                   'thumbnail_crop_info': {'type': 'null'},\n",
       "                   'attribution': {'type': 'string'}},\n",
       "                  'required': ['attribution',\n",
       "                   'content_size',\n",
       "                   'content_url',\n",
       "                   'thumbnail_crop_info',\n",
       "                   'thumbnail_size',\n",
       "                   'thumbnail_url',\n",
       "                   'title',\n",
       "                   'url']}},\n",
       "                'title': {'type': 'string'},\n",
       "                'url': {'type': 'string'},\n",
       "                'pub_date': {'type': ['null', 'number']},\n",
       "                'snippet': {'type': 'string'},\n",
       "                'attribution': {'type': 'string'},\n",
       "                'icon_type': {'type': 'null'}},\n",
       "               'required': ['end_idx', 'matched_text', 'start_idx', 'type']}},\n",
       "             'command': {'type': 'string'},\n",
       "             'status': {'type': 'string'},\n",
       "             'search_source': {'type': 'string'},\n",
       "             'client_reported_search_source': {'type': ['null', 'string']},\n",
       "             'debug_sonic_thread_id': {'type': 'string'},\n",
       "             'search_result_groups': {'type': 'array',\n",
       "              'items': {'type': 'object',\n",
       "               'properties': {'type': {'type': 'string'},\n",
       "                'domain': {'type': 'string'},\n",
       "                'entries': {'type': 'array',\n",
       "                 'items': {'type': 'object',\n",
       "                  'properties': {'type': {'type': 'string'},\n",
       "                   'url': {'type': 'string'},\n",
       "                   'title': {'type': 'string'},\n",
       "                   'snippet': {'type': 'string'},\n",
       "                   'ref_id': {'anyOf': [{'type': 'null'},\n",
       "                     {'type': 'object',\n",
       "                      'properties': {'turn_index': {'type': 'integer'},\n",
       "                       'ref_type': {'type': 'string'},\n",
       "                       'ref_index': {'type': 'integer'}},\n",
       "                      'required': ['ref_index', 'ref_type', 'turn_index']}]},\n",
       "                   'content_type': {'type': 'null'},\n",
       "                   'pub_date': {'type': ['null', 'number']},\n",
       "                   'attributions': {'type': 'null'},\n",
       "                   'attribution': {'type': 'string'},\n",
       "                   'attributions_debug': {'type': 'null'}},\n",
       "                  'required': ['pub_date',\n",
       "                   'ref_id',\n",
       "                   'snippet',\n",
       "                   'title',\n",
       "                   'type',\n",
       "                   'url']}}},\n",
       "               'required': ['domain', 'entries', 'type']}},\n",
       "             'safe_urls': {'type': 'array', 'items': {'type': 'string'}},\n",
       "             'message_locale': {'type': 'string'},\n",
       "             'image_results': {'type': 'array',\n",
       "              'items': {'type': 'object',\n",
       "               'properties': {'url': {'type': 'string'},\n",
       "                'content_url': {'type': 'string'},\n",
       "                'thumbnail_url': {'type': 'string'},\n",
       "                'title': {'type': 'string'},\n",
       "                'content_size': {'type': 'object',\n",
       "                 'properties': {'width': {'type': 'integer'},\n",
       "                  'height': {'type': 'integer'}},\n",
       "                 'required': ['height', 'width']},\n",
       "                'thumbnail_size': {'type': 'object',\n",
       "                 'properties': {'width': {'type': 'integer'},\n",
       "                  'height': {'type': 'integer'}},\n",
       "                 'required': ['height', 'width']},\n",
       "                'thumbnail_crop_info': {'type': 'null'},\n",
       "                'attribution': {'type': 'string'}},\n",
       "               'required': ['attribution',\n",
       "                'content_size',\n",
       "                'content_url',\n",
       "                'thumbnail_crop_info',\n",
       "                'thumbnail_size',\n",
       "                'thumbnail_url',\n",
       "                'title',\n",
       "                'url']}},\n",
       "             'rebase_developer_message': {'type': 'boolean'},\n",
       "             'reasoning_status': {'type': 'string'},\n",
       "             'search_queries': {'type': 'array',\n",
       "              'items': {'type': 'object',\n",
       "               'properties': {'type': {'type': 'string'},\n",
       "                'q': {'type': 'string'}},\n",
       "               'required': ['q', 'type']}},\n",
       "             'search_display_string': {'type': 'string'},\n",
       "             'searched_display_string': {'type': 'string'},\n",
       "             'finished_duration_sec': {'type': 'integer'},\n",
       "             'canvas': {'type': 'object',\n",
       "              'properties': {'textdoc_id': {'type': 'string'},\n",
       "               'textdoc_type': {'type': 'string'},\n",
       "               'version': {'type': 'integer'},\n",
       "               'title': {'type': 'string'},\n",
       "               'create_source': {'type': 'string'},\n",
       "               'from_version': {'type': 'integer'},\n",
       "               'textdoc_content_length': {'type': 'integer'},\n",
       "               'user_message_type': {'type': 'string'},\n",
       "               'selection_metadata': {'type': 'object',\n",
       "                'properties': {'selection_type': {'type': 'string'},\n",
       "                 'selection_position_range': {'type': 'object',\n",
       "                  'properties': {'start': {'type': 'integer'},\n",
       "                   'end': {'type': 'integer'}},\n",
       "                  'required': ['end', 'start']}},\n",
       "                'required': ['selection_position_range', 'selection_type']},\n",
       "               'comment_ids': {'type': 'array', 'items': {'type': 'string'}},\n",
       "               'has_user_edit': {'type': 'boolean'},\n",
       "               'is_failure': {'type': 'boolean'}}},\n",
       "             'targeted_reply': {'type': 'string'},\n",
       "             'targeted_reply_label': {'type': 'string'},\n",
       "             'attachments': {'type': 'array',\n",
       "              'items': {'type': 'object',\n",
       "               'properties': {'id': {'type': 'string'},\n",
       "                'size': {'type': 'integer'},\n",
       "                'name': {'type': 'string'},\n",
       "                'mime_type': {'type': 'string'},\n",
       "                'width': {'type': 'integer'},\n",
       "                'height': {'type': 'integer'},\n",
       "                'mimeType': {'type': 'string'}},\n",
       "               'required': ['id', 'name']}},\n",
       "             'caterpillar_selected_sources': {'type': 'array',\n",
       "              'items': {'type': 'string'}},\n",
       "             'gizmo_id': {'type': ['null', 'string']},\n",
       "             'rebase_system_message': {'type': 'boolean'},\n",
       "             'category_suggestions': {'type': 'array',\n",
       "              'items': {'type': 'object',\n",
       "               'properties': {'title': {'type': 'string'},\n",
       "                'category_id': {'type': 'string'},\n",
       "                'suggestions': {'anyOf': [{'type': 'null'},\n",
       "                  {'type': 'array',\n",
       "                   'items': {'type': 'object',\n",
       "                    'properties': {'title': {'type': 'null'},\n",
       "                     'description': {'type': 'string'},\n",
       "                     'prompt': {'type': 'string'},\n",
       "                     'system_hint': {'type': 'string'},\n",
       "                     'category_id': {'type': 'string'},\n",
       "                     'cta_label': {'type': 'null'},\n",
       "                     'image_url': {'type': 'null'},\n",
       "                     'model_override': {'type': 'null'},\n",
       "                     'id': {'type': 'string'}},\n",
       "                    'required': ['category_id',\n",
       "                     'cta_label',\n",
       "                     'description',\n",
       "                     'id',\n",
       "                     'prompt',\n",
       "                     'system_hint',\n",
       "                     'title']}}]},\n",
       "                'style': {'type': 'string'}},\n",
       "               'required': ['category_id', 'suggestions', 'title']}},\n",
       "             'finished_text': {'type': 'string'},\n",
       "             'initial_text': {'type': 'string'},\n",
       "             '_cite_metadata': {'type': 'object',\n",
       "              'properties': {'citation_format': {'type': 'object',\n",
       "                'properties': {'name': {'type': 'string'},\n",
       "                 'regex': {'type': 'string'}},\n",
       "                'required': ['name']},\n",
       "               'metadata_list': {'type': 'array',\n",
       "                'items': {'type': 'object',\n",
       "                 'properties': {'type': {'type': 'string'},\n",
       "                  'title': {'type': 'string'},\n",
       "                  'url': {'type': 'string'},\n",
       "                  'text': {'type': 'string'},\n",
       "                  'pub_date': {'type': ['null', 'string']},\n",
       "                  'extra': {'type': 'null'},\n",
       "                  'og_tags': {'type': 'null'},\n",
       "                  'name': {'type': 'string'},\n",
       "                  'id': {'type': 'string'},\n",
       "                  'source': {'type': 'string'}},\n",
       "                 'required': ['extra', 'text', 'type']}},\n",
       "               'original_query': {'type': 'null'}},\n",
       "              'required': ['citation_format',\n",
       "               'metadata_list',\n",
       "               'original_query']},\n",
       "             'args': {'type': 'array',\n",
       "              'items': {'anyOf': [{'type': ['integer', 'string']},\n",
       "                {'type': 'array', 'items': {'type': 'integer'}}]}},\n",
       "             'system_hints': {'type': 'array', 'items': {'type': 'string'}},\n",
       "             'cloud_doc_urls': {'type': 'array', 'items': {'type': 'null'}},\n",
       "             'search_engine': {'type': 'string'},\n",
       "             'aggregate_result': {'type': 'object',\n",
       "              'properties': {'status': {'type': 'string'},\n",
       "               'run_id': {'type': 'string'},\n",
       "               'start_time': {'type': 'number'},\n",
       "               'update_time': {'type': 'number'},\n",
       "               'code': {'type': 'string'},\n",
       "               'end_time': {'type': ['null', 'number']},\n",
       "               'final_expression_output': {'type': ['null', 'string']},\n",
       "               'in_kernel_exception': {'anyOf': [{'type': 'null'},\n",
       "                 {'type': 'object',\n",
       "                  'properties': {'name': {'type': 'string'},\n",
       "                   'traceback': {'type': 'array', 'items': {'type': 'string'}},\n",
       "                   'args': {'type': 'array', 'items': {'type': 'string'}},\n",
       "                   'notes': {'type': 'array'}},\n",
       "                  'required': ['args', 'name', 'notes', 'traceback']}]},\n",
       "               'system_exception': {'type': 'null'},\n",
       "               'messages': {'type': 'array',\n",
       "                'items': {'type': 'object',\n",
       "                 'properties': {'message_type': {'type': 'string'},\n",
       "                  'time': {'type': 'number'},\n",
       "                  'sender': {'type': 'string'},\n",
       "                  'image_payload': {'type': 'null'},\n",
       "                  'image_url': {'type': 'string'},\n",
       "                  'width': {'type': 'integer'},\n",
       "                  'height': {'type': 'integer'},\n",
       "                  'stream_name': {'type': 'string'},\n",
       "                  'text': {'type': 'string'},\n",
       "                  'timeout_triggered': {'type': 'number'}},\n",
       "                 'required': ['message_type', 'sender', 'time']}},\n",
       "               'jupyter_messages': {'type': 'array',\n",
       "                'items': {'type': 'object',\n",
       "                 'properties': {'msg_type': {'type': 'string'},\n",
       "                  'parent_header': {'type': 'object',\n",
       "                   'properties': {'msg_id': {'type': 'string'},\n",
       "                    'version': {'type': 'string'}},\n",
       "                   'required': ['msg_id', 'version']},\n",
       "                  'content': {'type': 'object',\n",
       "                   'properties': {'execution_state': {'type': 'string'},\n",
       "                    'data': {'type': 'object',\n",
       "                     'properties': {'text/plain': {'type': 'string'},\n",
       "                      'text/html': {'type': 'string'},\n",
       "                      'image/vnd.openai.fileservice2.png': {'type': 'string'},\n",
       "                      'image/vnd.openai.fileservice.png': {'type': 'string'}},\n",
       "                     'required': ['text/plain']},\n",
       "                    'name': {'type': 'string'},\n",
       "                    'text': {'type': 'string'},\n",
       "                    'traceback': {'type': 'array',\n",
       "                     'items': {'type': 'string'}},\n",
       "                    'ename': {'type': 'string'},\n",
       "                    'evalue': {'type': 'string'}}},\n",
       "                  'timeout': {'type': 'number'}},\n",
       "                 'required': ['msg_type']}},\n",
       "               'timeout_triggered': {'type': ['null', 'number']}},\n",
       "              'required': ['code',\n",
       "               'end_time',\n",
       "               'final_expression_output',\n",
       "               'in_kernel_exception',\n",
       "               'jupyter_messages',\n",
       "               'messages',\n",
       "               'run_id',\n",
       "               'start_time',\n",
       "               'status',\n",
       "               'system_exception',\n",
       "               'timeout_triggered',\n",
       "               'update_time']},\n",
       "             'paragen_variants_info': {'type': 'object',\n",
       "              'properties': {'type': {'type': 'string'},\n",
       "               'num_variants_in_stream': {'type': 'integer'},\n",
       "               'display_treatment': {'type': 'string'},\n",
       "               'conversation_id': {'type': 'string'}},\n",
       "              'required': ['conversation_id',\n",
       "               'display_treatment',\n",
       "               'num_variants_in_stream',\n",
       "               'type']},\n",
       "             'paragen_variant_choice': {'type': 'string'},\n",
       "             'voice_mode_message': {'type': 'boolean'},\n",
       "             'kwargs': {'type': 'object',\n",
       "              'properties': {'message_id': {'type': 'string'},\n",
       "               'pending_message_id': {'type': ['null', 'string']},\n",
       "               'sync_write': {'type': 'boolean'}},\n",
       "              'required': ['message_id']},\n",
       "             'augmented_paragen_prompt_label': {'type': ['null', 'string']},\n",
       "             'exclusive_key': {'type': 'string'},\n",
       "             'model_switcher_deny': {'type': 'array',\n",
       "              'items': {'type': 'object',\n",
       "               'properties': {'slug': {'type': 'string'},\n",
       "                'context': {'type': 'string'},\n",
       "                'reason': {'type': 'string'},\n",
       "                'description': {'type': 'string'}},\n",
       "               'required': ['context', 'description', 'reason', 'slug']}},\n",
       "             'pad': {'type': 'string'},\n",
       "             'real_time_audio_has_video': {'type': 'boolean'},\n",
       "             'ada_visualizations': {'type': 'array',\n",
       "              'items': {'type': 'object',\n",
       "               'properties': {'type': {'type': 'string'},\n",
       "                'file_id': {'type': 'string'},\n",
       "                'title': {'type': 'string'},\n",
       "                'chart_type': {'type': 'string'},\n",
       "                'fallback_to_image': {'type': 'boolean'}},\n",
       "               'required': ['chart_type',\n",
       "                'fallback_to_image',\n",
       "                'file_id',\n",
       "                'title',\n",
       "                'type']}},\n",
       "             'requested_model_slug': {'type': 'string'},\n",
       "             'dalle': {'type': 'object',\n",
       "              'properties': {'from_client': {'type': 'object',\n",
       "                'properties': {'operation': {'type': 'object',\n",
       "                  'properties': {'type': {'type': 'string'},\n",
       "                   'original_gen_id': {'type': 'string'},\n",
       "                   'original_file_id': {'type': 'string'}},\n",
       "                  'required': ['original_file_id',\n",
       "                   'original_gen_id',\n",
       "                   'type']}},\n",
       "                'required': ['operation']}},\n",
       "              'required': ['from_client']},\n",
       "             'filter_out_for_training': {'type': 'boolean'},\n",
       "             'jit_plugin_data': {'type': 'object',\n",
       "              'properties': {'from_server': {'type': 'object',\n",
       "                'properties': {'type': {'type': 'string'},\n",
       "                 'body': {'type': 'object',\n",
       "                  'properties': {'domain': {'type': 'string'},\n",
       "                   'is_consequential': {'type': 'boolean'},\n",
       "                   'privacy_policy': {'type': 'string'},\n",
       "                   'method': {'type': 'string'},\n",
       "                   'path': {'type': 'string'},\n",
       "                   'operation': {'type': 'string'},\n",
       "                   'params': {'type': 'object',\n",
       "                    'properties': {'country_name': {'type': 'string'},\n",
       "                     'state_name': {'type': 'string'},\n",
       "                     'city_name': {'type': 'string'},\n",
       "                     'filters': {'type': 'string'},\n",
       "                     'num_trails': {'type': 'integer'},\n",
       "                     'raw_query': {'type': 'string'},\n",
       "                     'location_helper': {'type': 'string'}},\n",
       "                    'required': ['city_name',\n",
       "                     'country_name',\n",
       "                     'filters',\n",
       "                     'location_helper',\n",
       "                     'num_trails',\n",
       "                     'raw_query',\n",
       "                     'state_name']},\n",
       "                   'actions': {'type': 'array',\n",
       "                    'items': {'type': 'object',\n",
       "                     'properties': {'name': {'type': 'string'},\n",
       "                      'type': {'type': 'string'},\n",
       "                      'allow': {'type': 'object',\n",
       "                       'properties': {'target_message_id': {'type': 'string'}},\n",
       "                       'required': ['target_message_id']},\n",
       "                      'always_allow': {'type': 'object',\n",
       "                       'properties': {'target_message_id': {'type': 'string'},\n",
       "                        'operation_hash': {'type': 'string'}},\n",
       "                       'required': ['operation_hash', 'target_message_id']},\n",
       "                      'deny': {'type': 'object',\n",
       "                       'properties': {'target_message_id': {'type': 'string'}},\n",
       "                       'required': ['target_message_id']}},\n",
       "                     'required': ['type']}}},\n",
       "                  'required': ['actions',\n",
       "                   'domain',\n",
       "                   'is_consequential',\n",
       "                   'method',\n",
       "                   'operation',\n",
       "                   'params',\n",
       "                   'path',\n",
       "                   'privacy_policy']}},\n",
       "                'required': ['body', 'type']},\n",
       "               'from_client': {'type': 'object',\n",
       "                'properties': {'user_action': {'type': 'object',\n",
       "                  'properties': {'data': {'type': 'object',\n",
       "                    'properties': {'type': {'type': 'string'}},\n",
       "                    'required': ['type']},\n",
       "                   'target_message_id': {'type': 'string'}},\n",
       "                  'required': ['data', 'target_message_id']}},\n",
       "                'required': ['user_action']}}},\n",
       "             'invoked_plugin': {'type': 'object',\n",
       "              'properties': {'type': {'type': 'string'},\n",
       "               'namespace': {'type': 'string'},\n",
       "               'plugin_id': {'type': 'string'},\n",
       "               'http_response_status': {'type': 'integer'}},\n",
       "              'required': ['http_response_status',\n",
       "               'namespace',\n",
       "               'plugin_id',\n",
       "               'type']}}},\n",
       "           'recipient': {'type': 'string'},\n",
       "           'channel': {'type': ['null', 'string']}},\n",
       "          'required': ['author',\n",
       "           'channel',\n",
       "           'content',\n",
       "           'create_time',\n",
       "           'end_turn',\n",
       "           'id',\n",
       "           'metadata',\n",
       "           'recipient',\n",
       "           'status',\n",
       "           'update_time',\n",
       "           'weight']}]},\n",
       "       'parent': {'type': ['null', 'string']},\n",
       "       'children': {'type': 'array', 'items': {'type': 'string'}}},\n",
       "      'required': ['children', 'id', 'message', 'parent']},\n",
       "     '^client-created-': {'type': 'object',\n",
       "      'properties': {'id': {'type': 'string'},\n",
       "       'message': {'type': 'null'},\n",
       "       'parent': {'type': 'null'},\n",
       "       'children': {'type': 'array', 'items': {'type': 'string'}}},\n",
       "      'required': ['children', 'id', 'message', 'parent']}}},\n",
       "   'title': {'type': 'string'},\n",
       "   'create_time': {'type': 'number'},\n",
       "   'update_time': {'type': 'number'},\n",
       "   'moderation_results': {'type': 'array'},\n",
       "   'current_node': {'type': 'string'},\n",
       "   'plugin_ids': {'anyOf': [{'type': 'null'},\n",
       "     {'type': 'array', 'items': {'type': 'string'}}]},\n",
       "   'conversation_id': {'type': 'string'},\n",
       "   'conversation_template_id': {'type': ['null', 'string']},\n",
       "   'gizmo_id': {'type': ['null', 'string']},\n",
       "   'gizmo_type': {'type': ['null', 'string']},\n",
       "   'is_archived': {'type': 'boolean'},\n",
       "   'is_starred': {'type': 'null'},\n",
       "   'safe_urls': {'type': 'array', 'items': {'type': 'string'}},\n",
       "   'blocked_urls': {'type': 'array'},\n",
       "   'default_model_slug': {'type': ['null', 'string']},\n",
       "   'conversation_origin': {'type': 'null'},\n",
       "   'voice': {'type': ['null', 'string']},\n",
       "   'async_status': {'type': ['integer', 'null']},\n",
       "   'disabled_tool_ids': {'type': 'array'},\n",
       "   'is_do_not_remember': {'type': ['boolean', 'null']},\n",
       "   'memory_scope': {'type': 'string'},\n",
       "   'id': {'type': 'string'}},\n",
       "  'required': ['async_status',\n",
       "   'blocked_urls',\n",
       "   'conversation_id',\n",
       "   'conversation_origin',\n",
       "   'conversation_template_id',\n",
       "   'create_time',\n",
       "   'current_node',\n",
       "   'default_model_slug',\n",
       "   'disabled_tool_ids',\n",
       "   'gizmo_id',\n",
       "   'gizmo_type',\n",
       "   'id',\n",
       "   'is_archived',\n",
       "   'is_do_not_remember',\n",
       "   'is_starred',\n",
       "   'mapping',\n",
       "   'memory_scope',\n",
       "   'moderation_results',\n",
       "   'plugin_ids',\n",
       "   'safe_urls',\n",
       "   'title',\n",
       "   'update_time',\n",
       "   'voice']}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.to_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d543842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$schema': 'http://json-schema.org/schema#',\n",
       " 'type': 'object',\n",
       " 'properties': {'mapping': {'type': 'object',\n",
       "   'patternProperties': {'^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$': {'type': 'object',\n",
       "     'properties': {'id': {'type': 'string'},\n",
       "      'message': {'type': 'object',\n",
       "       'properties': {'id': {'type': 'string'},\n",
       "        'author': {'type': 'object',\n",
       "         'properties': {'role': {'type': 'string'},\n",
       "          'name': {'type': ['null', 'string']},\n",
       "          'metadata': {'type': 'object',\n",
       "           'properties': {'real_author': {'type': 'string'}}}},\n",
       "         'required': ['metadata', 'name', 'role']},\n",
       "        'create_time': {'type': ['null', 'number']},\n",
       "        'update_time': {'type': 'null'},\n",
       "        'content': {'type': 'object',\n",
       "         'properties': {'content_type': {'type': 'string'},\n",
       "          'parts': {'type': 'array', 'items': {'type': 'string'}},\n",
       "          'thoughts': {'type': 'array',\n",
       "           'items': {'type': 'object',\n",
       "            'properties': {'summary': {'type': 'string'},\n",
       "             'content': {'type': 'string'}},\n",
       "            'required': ['content', 'summary']}},\n",
       "          'source_analysis_msg_id': {'type': 'string'},\n",
       "          'language': {'type': 'string'},\n",
       "          'response_format_name': {'type': 'null'},\n",
       "          'text': {'type': 'string'},\n",
       "          'content': {'type': 'string'}},\n",
       "         'required': ['content_type']},\n",
       "        'status': {'type': 'string'},\n",
       "        'end_turn': {'type': ['boolean', 'null']},\n",
       "        'weight': {'type': 'number'},\n",
       "        'metadata': {'type': 'object',\n",
       "         'properties': {'is_visually_hidden_from_conversation': {'type': 'boolean'},\n",
       "          'rebase_developer_message': {'type': 'boolean'},\n",
       "          'selected_github_repos': {'type': 'array'},\n",
       "          'serialization_metadata': {'type': 'object',\n",
       "           'properties': {'custom_symbol_offsets': {'type': 'array'}},\n",
       "           'required': ['custom_symbol_offsets']},\n",
       "          'request_id': {'type': 'string'},\n",
       "          'message_source': {'type': 'null'},\n",
       "          'timestamp_': {'type': 'string'},\n",
       "          'message_type': {'type': 'null'},\n",
       "          'model_slug': {'type': 'string'},\n",
       "          'default_model_slug': {'type': 'string'},\n",
       "          'parent_id': {'type': 'string'},\n",
       "          'reasoning_status': {'type': 'string'},\n",
       "          'citations': {'type': 'array'},\n",
       "          'content_references': {'type': 'array',\n",
       "           'items': {'type': 'object',\n",
       "            'properties': {'matched_text': {'type': 'string'},\n",
       "             'start_idx': {'type': 'integer'},\n",
       "             'end_idx': {'type': 'integer'},\n",
       "             'safe_urls': {'type': 'array', 'items': {'type': 'string'}},\n",
       "             'refs': {'type': 'array',\n",
       "              'items': {'anyOf': [{'type': 'string'},\n",
       "                {'type': 'object',\n",
       "                 'properties': {'turn_index': {'type': 'integer'},\n",
       "                  'ref_type': {'type': 'string'},\n",
       "                  'ref_index': {'type': 'integer'}},\n",
       "                 'required': ['ref_index', 'ref_type', 'turn_index']}]}},\n",
       "             'alt': {'type': ['null', 'string']},\n",
       "             'prompt_text': {'type': ['null', 'string']},\n",
       "             'type': {'type': 'string'},\n",
       "             'items': {'type': 'array',\n",
       "              'items': {'type': 'object',\n",
       "               'properties': {'title': {'type': 'string'},\n",
       "                'url': {'type': 'string'},\n",
       "                'pub_date': {'type': ['null', 'number']},\n",
       "                'snippet': {'type': 'string'},\n",
       "                'attribution_segments': {'type': 'null'},\n",
       "                'supporting_websites': {'type': 'array'},\n",
       "                'refs': {'type': 'array',\n",
       "                 'items': {'type': 'object',\n",
       "                  'properties': {'turn_index': {'type': 'integer'},\n",
       "                   'ref_type': {'type': 'string'},\n",
       "                   'ref_index': {'type': 'integer'}},\n",
       "                  'required': ['ref_index', 'ref_type', 'turn_index']}},\n",
       "                'hue': {'type': 'null'},\n",
       "                'attributions': {'type': 'null'},\n",
       "                'attribution': {'type': 'string'}},\n",
       "               'required': ['attribution',\n",
       "                'attribution_segments',\n",
       "                'attributions',\n",
       "                'hue',\n",
       "                'pub_date',\n",
       "                'refs',\n",
       "                'snippet',\n",
       "                'supporting_websites',\n",
       "                'title',\n",
       "                'url']}},\n",
       "             'status': {'type': 'string'},\n",
       "             'error': {'type': 'null'},\n",
       "             'style': {'type': 'null'},\n",
       "             'sources': {'type': 'array',\n",
       "              'items': {'type': 'object',\n",
       "               'properties': {'title': {'type': 'string'},\n",
       "                'url': {'type': 'string'},\n",
       "                'attribution': {'type': 'string'}},\n",
       "               'required': ['attribution', 'title', 'url']}},\n",
       "             'has_images': {'type': 'boolean'},\n",
       "             'invalid': {'type': 'boolean'},\n",
       "             'images': {'type': 'array',\n",
       "              'items': {'type': 'object',\n",
       "               'properties': {'url': {'type': 'string'},\n",
       "                'content_url': {'type': 'string'},\n",
       "                'thumbnail_url': {'type': 'string'},\n",
       "                'title': {'type': 'string'},\n",
       "                'content_size': {'type': 'object',\n",
       "                 'properties': {'width': {'type': 'integer'},\n",
       "                  'height': {'type': 'integer'}},\n",
       "                 'required': ['height', 'width']},\n",
       "                'thumbnail_size': {'type': 'object',\n",
       "                 'properties': {'width': {'type': 'integer'},\n",
       "                  'height': {'type': 'integer'}},\n",
       "                 'required': ['height', 'width']},\n",
       "                'thumbnail_crop_info': {'type': 'null'},\n",
       "                'attribution': {'type': 'string'}},\n",
       "               'required': ['attribution',\n",
       "                'content_size',\n",
       "                'content_url',\n",
       "                'thumbnail_crop_info',\n",
       "                'thumbnail_size',\n",
       "                'thumbnail_url',\n",
       "                'title',\n",
       "                'url']}}},\n",
       "            'required': ['end_idx', 'matched_text', 'start_idx', 'type']}},\n",
       "          'finish_details': {'type': 'object',\n",
       "           'properties': {'type': {'type': 'string'},\n",
       "            'stop_tokens': {'type': 'array', 'items': {'type': 'integer'}}},\n",
       "           'required': ['stop_tokens', 'type']},\n",
       "          'is_complete': {'type': 'boolean'},\n",
       "          'search_queries': {'type': 'array',\n",
       "           'items': {'type': 'object',\n",
       "            'properties': {'type': {'type': 'string'},\n",
       "             'q': {'type': 'string'}},\n",
       "            'required': ['q', 'type']}},\n",
       "          'search_display_string': {'type': 'string'},\n",
       "          'searched_display_string': {'type': 'string'},\n",
       "          'search_result_groups': {'type': 'array',\n",
       "           'items': {'type': 'object',\n",
       "            'properties': {'type': {'type': 'string'},\n",
       "             'domain': {'type': 'string'},\n",
       "             'entries': {'type': 'array',\n",
       "              'items': {'type': 'object',\n",
       "               'properties': {'type': {'type': 'string'},\n",
       "                'url': {'type': 'string'},\n",
       "                'title': {'type': 'string'},\n",
       "                'snippet': {'type': 'string'},\n",
       "                'ref_id': {'anyOf': [{'type': 'null'},\n",
       "                  {'type': 'object',\n",
       "                   'properties': {'turn_index': {'type': 'integer'},\n",
       "                    'ref_type': {'type': 'string'},\n",
       "                    'ref_index': {'type': 'integer'}},\n",
       "                   'required': ['ref_index', 'ref_type', 'turn_index']}]},\n",
       "                'pub_date': {'type': ['null', 'number']},\n",
       "                'attribution': {'type': 'string'}},\n",
       "               'required': ['attribution',\n",
       "                'pub_date',\n",
       "                'ref_id',\n",
       "                'snippet',\n",
       "                'title',\n",
       "                'type',\n",
       "                'url']}}},\n",
       "            'required': ['domain', 'entries', 'type']}},\n",
       "          'debug_sonic_thread_id': {'type': 'string'},\n",
       "          'finished_duration_sec': {'type': 'integer'},\n",
       "          'safe_urls': {'type': 'array', 'items': {'type': 'string'}},\n",
       "          'canvas': {'type': 'object',\n",
       "           'properties': {'textdoc_id': {'type': 'string'},\n",
       "            'textdoc_type': {'type': 'string'},\n",
       "            'version': {'type': 'integer'},\n",
       "            'title': {'type': 'string'},\n",
       "            'create_source': {'type': 'string'},\n",
       "            'from_version': {'type': 'integer'}},\n",
       "           'required': ['textdoc_id', 'textdoc_type', 'version']},\n",
       "          'command': {'type': 'string'}}},\n",
       "        'recipient': {'type': 'string'},\n",
       "        'channel': {'type': ['null', 'string']}},\n",
       "       'required': ['author',\n",
       "        'channel',\n",
       "        'content',\n",
       "        'create_time',\n",
       "        'end_turn',\n",
       "        'id',\n",
       "        'metadata',\n",
       "        'recipient',\n",
       "        'status',\n",
       "        'update_time',\n",
       "        'weight']},\n",
       "      'parent': {'type': 'string'},\n",
       "      'children': {'type': 'array', 'items': {'type': 'string'}}},\n",
       "     'required': ['children', 'id', 'message', 'parent']},\n",
       "    '^client-created-': {'type': 'object',\n",
       "     'properties': {'id': {'type': 'string'},\n",
       "      'message': {'type': 'null'},\n",
       "      'parent': {'type': 'null'},\n",
       "      'children': {'type': 'array', 'items': {'type': 'string'}}},\n",
       "     'required': ['children', 'id', 'message', 'parent']}}},\n",
       "  'title': {'type': 'string'},\n",
       "  'create_time': {'type': 'number'},\n",
       "  'update_time': {'type': 'number'},\n",
       "  'moderation_results': {'type': 'array'},\n",
       "  'current_node': {'type': 'string'},\n",
       "  'plugin_ids': {'type': 'null'},\n",
       "  'conversation_id': {'type': 'string'},\n",
       "  'conversation_template_id': {'type': 'null'},\n",
       "  'gizmo_id': {'type': 'null'},\n",
       "  'gizmo_type': {'type': 'null'},\n",
       "  'is_archived': {'type': 'boolean'},\n",
       "  'is_starred': {'type': 'null'},\n",
       "  'safe_urls': {'type': 'array', 'items': {'type': 'string'}},\n",
       "  'blocked_urls': {'type': 'array'},\n",
       "  'default_model_slug': {'type': 'string'},\n",
       "  'conversation_origin': {'type': 'null'},\n",
       "  'voice': {'type': 'null'},\n",
       "  'async_status': {'type': 'null'},\n",
       "  'disabled_tool_ids': {'type': 'array'},\n",
       "  'is_do_not_remember': {'type': 'boolean'},\n",
       "  'memory_scope': {'type': 'string'},\n",
       "  'id': {'type': 'string'}},\n",
       " 'required': ['async_status',\n",
       "  'blocked_urls',\n",
       "  'conversation_id',\n",
       "  'conversation_origin',\n",
       "  'conversation_template_id',\n",
       "  'create_time',\n",
       "  'current_node',\n",
       "  'default_model_slug',\n",
       "  'disabled_tool_ids',\n",
       "  'gizmo_id',\n",
       "  'gizmo_type',\n",
       "  'id',\n",
       "  'is_archived',\n",
       "  'is_do_not_remember',\n",
       "  'is_starred',\n",
       "  'mapping',\n",
       "  'memory_scope',\n",
       "  'moderation_results',\n",
       "  'plugin_ids',\n",
       "  'safe_urls',\n",
       "  'title',\n",
       "  'update_time',\n",
       "  'voice']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_seed_schema = {\n",
    "        'type': 'object',\n",
    "        'properties': {\n",
    "            'mapping': {\n",
    "                'type': 'object',\n",
    "                'patternProperties': {\n",
    "                    # UUID pattern - GenSON will fill in the actual schema\n",
    "                    r'^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$': None,\n",
    "                    # Also handle the client-created-root pattern\n",
    "                    r'^client-created-': None\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "c = convs[2]\n",
    "\n",
    "conv_schema = SchemaBuilder()\n",
    "conv_schema.add_schema(conv_seed_schema)\n",
    "conv_schema.add_object(c)\n",
    "conv_schema.to_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be1f5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load your conversations (assuming you have them in memory)\\n# conversations = your_conversation_data\\n\\n# Create and configure tagger\\ntagger = create_default_tagger()\\n\\n# Tag all conversations\\ntagged_results = tagger.tag_conversations(conversations)\\n\\n# Print summary\\ntagger.print_summary(tagged_results)\\n\\n# Filter for different types of conversations\\ncoding_conversations = tagger.filter_by_tags(\\n    tagged_results, \\n    include_tags={\\'coding_assistance\\'}\\n)\\n\\ncoding_start_conversations = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags={\\'coding_assistance_start\\'}\\n)\\n\\ncontext_heavy_conversations = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags={\\'context_heavy_start\\'}\\n)\\n\\n# Exclude conversations that start with large content or attachments\\nlightweight_conversations = tagger.filter_by_tags(\\n    tagged_results,\\n    exclude_tags={\\'starts_large_content\\', \\'starts_with_attachments\\'}\\n)\\n\\n# Debug a specific conversation to see why it was tagged\\ntagger.debug_conversation(conversations[0])\\n\\n# Add custom rules for your specific needs\\ntagger.add_base_rule(\\n    \\'starts_with_question\\',\\n    lambda conv: get_first_user_message(conv) and \\n                 get_first_user_message(conv).get(\\'content\\', {}).get(\\'text\\', \\'\\').strip().endswith(\\'?\\'),\\n    \\'Conversation starts with a question\\'\\n)\\n\\n# Example: Find conversations that start with large content but aren\\'t coding\\nlarge_non_coding = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags={\\'starts_large_content\\'},\\n    exclude_tags={\\'coding_assistance\\', \\'coding_assistance_start\\'}\\n)\\n\\nprint(f\"Found {len(large_non_coding)} conversations with large initial content that aren\\'t coding-related\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation_tagger.py\n",
    "\"\"\"\n",
    "Flexible conversation tagging system for LLM conversation analysis.\n",
    "Supports base tags (from content) and supplemental tags (from other tags).\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Any, List, Callable, Set\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class ConversationTagger:\n",
    "    \"\"\"\n",
    "    Flexible system for tagging conversations with multiple criteria.\n",
    "    Supports base rules (applied directly to conversations) and \n",
    "    supplemental rules (applied based on existing tags).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_rules: Dict[str, Callable] = {}\n",
    "        self.supplemental_rules: Dict[str, Callable] = {}\n",
    "        self.rule_descriptions: Dict[str, str] = {}\n",
    "    \n",
    "    def add_base_rule(self, tag_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"\n",
    "        Add a base tagging rule that analyzes conversation content.\n",
    "        \n",
    "        Args:\n",
    "            tag_name: Name of the tag to apply\n",
    "            rule_function: Function that takes (conversation) -> bool\n",
    "            description: Human-readable description of the rule\n",
    "        \"\"\"\n",
    "        self.base_rules[tag_name] = rule_function\n",
    "        self.rule_descriptions[tag_name] = description\n",
    "    \n",
    "    def add_supplemental_rule(self, tag_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"\n",
    "        Add a supplemental rule that depends on existing tags.\n",
    "        \n",
    "        Args:\n",
    "            tag_name: Name of the tag to apply  \n",
    "            rule_function: Function that takes (conversation, current_tags) -> bool\n",
    "            description: Human-readable description of the rule\n",
    "        \"\"\"\n",
    "        self.supplemental_rules[tag_name] = rule_function\n",
    "        self.rule_descriptions[tag_name] = description\n",
    "    \n",
    "    def tag_conversation(self, conversation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Apply all tagging rules to a conversation.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with 'tags', 'debug_info', and original conversation data\n",
    "        \"\"\"\n",
    "        tags = set()\n",
    "        debug_info = defaultdict(list)\n",
    "        \n",
    "        # Apply base rules\n",
    "        for tag_name, rule_func in self.base_rules.items():\n",
    "            try:\n",
    "                if rule_func(conversation):\n",
    "                    tags.add(tag_name)\n",
    "                    debug_info['applied_rules'].append(f\"BASE: {tag_name}\")\n",
    "                else:\n",
    "                    debug_info['skipped_rules'].append(f\"BASE: {tag_name}\")\n",
    "            except Exception as e:\n",
    "                debug_info['errors'].append(f\"BASE: {tag_name} - {str(e)}\")\n",
    "        \n",
    "        # Apply supplemental rules (may need multiple passes)\n",
    "        max_iterations = 5\n",
    "        for iteration in range(max_iterations):\n",
    "            initial_tag_count = len(tags)\n",
    "            \n",
    "            for tag_name, rule_func in self.supplemental_rules.items():\n",
    "                if tag_name in tags:  # Already applied\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    if rule_func(conversation, tags):\n",
    "                        tags.add(tag_name)\n",
    "                        debug_info['applied_rules'].append(f\"SUPP: {tag_name} (iter {iteration})\")\n",
    "                    else:\n",
    "                        debug_info['skipped_rules'].append(f\"SUPP: {tag_name} (iter {iteration})\")\n",
    "                except Exception as e:\n",
    "                    debug_info['errors'].append(f\"SUPP: {tag_name} - {str(e)}\")\n",
    "            \n",
    "            # If no new tags were added, we're done\n",
    "            if len(tags) == initial_tag_count:\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'conversation_id': conversation.get('conversation_id', conversation.get('id', 'unknown')),\n",
    "            'title': conversation.get('title', 'Untitled'),\n",
    "            'tags': list(tags),\n",
    "            'debug_info': dict(debug_info),\n",
    "            'conversation': conversation\n",
    "        }\n",
    "    \n",
    "    def tag_conversations(self, conversations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Tag multiple conversations and return results.\"\"\"\n",
    "        return [self.tag_conversation(conv) for conv in conversations]\n",
    "    \n",
    "    def filter_by_tags(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                      include_tags: Set[str] = None, \n",
    "                      exclude_tags: Set[str] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Filter conversations by tags.\n",
    "        \n",
    "        Args:\n",
    "            tagged_conversations: List of tagged conversation results\n",
    "            include_tags: Only return conversations with ALL these tags\n",
    "            exclude_tags: Exclude conversations with ANY of these tags\n",
    "        \"\"\"\n",
    "        filtered = []\n",
    "        \n",
    "        for tagged_conv in tagged_conversations:\n",
    "            conv_tags = set(tagged_conv['tags'])\n",
    "            \n",
    "            # Check exclusions first\n",
    "            if exclude_tags and any(tag in conv_tags for tag in exclude_tags):\n",
    "                continue\n",
    "                \n",
    "            # Check inclusions\n",
    "            if include_tags and not include_tags.issubset(conv_tags):\n",
    "                continue\n",
    "                \n",
    "            filtered.append(tagged_conv)\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def print_summary(self, tagged_conversations: List[Dict[str, Any]]):\n",
    "        \"\"\"Print a summary of tagging results.\"\"\"\n",
    "        total = len(tagged_conversations)\n",
    "        tag_counts = defaultdict(int)\n",
    "        \n",
    "        for tagged_conv in tagged_conversations:\n",
    "            for tag in tagged_conv['tags']:\n",
    "                tag_counts[tag] += 1\n",
    "        \n",
    "        print(f\"Tagged {total} conversations\")\n",
    "        print(f\"Tag distribution:\")\n",
    "        for tag, count in sorted(tag_counts.items()):\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"  {tag}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    def debug_conversation(self, conversation: Dict[str, Any]):\n",
    "        \"\"\"Debug a single conversation - show which rules fire.\"\"\"\n",
    "        result = self.tag_conversation(conversation)\n",
    "        \n",
    "        print(f\"Conversation: {result['title'][:50]}...\")\n",
    "        print(f\"Tags applied: {result['tags']}\")\n",
    "        print(f\"\\nRule details:\")\n",
    "        \n",
    "        for rule in result['debug_info'].get('applied_rules', []):\n",
    "            print(f\"   {rule}\")\n",
    "            \n",
    "        for rule in result['debug_info'].get('skipped_rules', []):\n",
    "            print(f\"   {rule}\")\n",
    "            \n",
    "        for error in result['debug_info'].get('errors', []):\n",
    "            print(f\"    ERROR: {error}\")\n",
    "\n",
    "\n",
    "# Pre-defined rule functions for common patterns\n",
    "def has_large_content(conversation: Dict[str, Any], min_length: int = 2000) -> bool:\n",
    "    \"\"\"Check if conversation has unusually large content (suggests context sharing).\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        if node_id.startswith('client-created'):\n",
    "            continue\n",
    "            \n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        if len(text) > min_length:\n",
    "            return True\n",
    "            \n",
    "        parts = content.get('parts', [])\n",
    "        for part in parts:\n",
    "            if isinstance(part, str) and len(part) > min_length:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_code_patterns(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for clear code patterns in conversation.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        if node_id.startswith('client-created'):\n",
    "            continue\n",
    "            \n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "        \n",
    "        # Check content\n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        \n",
    "        # Strong code indicators\n",
    "        code_indicators = [\n",
    "            '```',  # Code blocks\n",
    "            'def ', 'function ', 'class ',  # Definitions\n",
    "            'import ', 'from ', 'require(',  # Imports\n",
    "            '#!/bin/', '#include',  # Script headers\n",
    "        ]\n",
    "        \n",
    "        if any(indicator in all_text for indicator in code_indicators):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_github_repos(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if GitHub repositories were selected for context.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        if node_id.startswith('client-created'):\n",
    "            continue\n",
    "            \n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        repos = metadata.get('selected_github_repos', [])\n",
    "        if repos:  # Non-empty list\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_canvas_operations(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for canvas/document operations.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        if node_id.startswith('client-created'):\n",
    "            continue\n",
    "            \n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if metadata.get('canvas'):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_search_operations(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for web search operations.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        if node_id.startswith('client-created'):\n",
    "            continue\n",
    "            \n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if (metadata.get('search_queries') or \n",
    "            metadata.get('search_result_groups') or\n",
    "            metadata.get('content_references')):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_reasoning_thoughts(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for reasoning/thinking patterns.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        if node_id.startswith('client-created'):\n",
    "            continue\n",
    "            \n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        content = message.get('content', {})\n",
    "        if content.get('thoughts'):  # Reasoning thoughts\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def get_first_user_message(conversation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Find the first user message in the conversation.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    user_messages = []\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        author = message.get('author', {})\n",
    "        if author.get('role') == 'user':\n",
    "            create_time = message.get('create_time')\n",
    "            user_messages.append((create_time or 0, message, node))\n",
    "    \n",
    "    if not user_messages:\n",
    "        return None\n",
    "    \n",
    "    # Sort by create_time and return the first user message\n",
    "    user_messages.sort(key=lambda x: x[0])\n",
    "    return user_messages[0][1]  # Return the message object\n",
    "\n",
    "\n",
    "def first_user_has_large_content(conversation: Dict[str, Any], min_length: int = 2000) -> bool:\n",
    "    \"\"\"Check if the first user message has large content.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    content = first_message.get('content', {})\n",
    "    text = content.get('text', '')\n",
    "    if len(text) > min_length:\n",
    "        return True\n",
    "        \n",
    "    parts = content.get('parts', [])\n",
    "    for part in parts:\n",
    "        if isinstance(part, str) and len(part) > min_length:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def first_user_has_code_patterns(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message contains code patterns.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    content = first_message.get('content', {})\n",
    "    text = content.get('text', '')\n",
    "    parts = content.get('parts', [])\n",
    "    all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "    \n",
    "    # Strong code indicators\n",
    "    code_indicators = [\n",
    "        '```',  # Code blocks\n",
    "        'def ', 'function ', 'class ',  # Definitions\n",
    "        'import ', 'from ', 'require(',  # Imports\n",
    "        '#!/bin/', '#include',  # Script headers\n",
    "    ]\n",
    "    \n",
    "    return any(indicator in all_text for indicator in code_indicators)\n",
    "\n",
    "\n",
    "def first_user_has_attachments(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message has attachments.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    metadata = first_message.get('metadata', {})\n",
    "    attachments = metadata.get('attachments', [])\n",
    "    return len(attachments) > 0\n",
    "\n",
    "\n",
    "# Example usage setup\n",
    "def create_default_tagger() -> ConversationTagger:\n",
    "    \"\"\"Create a tagger with common rules pre-configured.\"\"\"\n",
    "    tagger = ConversationTagger()\n",
    "    \n",
    "    # Base content analysis rules (anywhere in conversation)\n",
    "    tagger.add_base_rule(\n",
    "        'large_content',\n",
    "        lambda conv: has_large_content(conv, 2000),\n",
    "        'Content longer than 2000 characters (anywhere in conversation)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'code_patterns', \n",
    "        has_code_patterns,\n",
    "        'Contains clear code patterns (```, def, function, etc.)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'github_context',\n",
    "        has_github_repos,\n",
    "        'GitHub repositories selected for context'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'canvas_operations',\n",
    "        has_canvas_operations,\n",
    "        'Uses canvas/document features'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'web_search',\n",
    "        has_search_operations,\n",
    "        'Includes web search functionality'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'reasoning',\n",
    "        has_reasoning_thoughts,\n",
    "        'Contains reasoning/thinking content'\n",
    "    )\n",
    "    \n",
    "    # First user message specific rules\n",
    "    tagger.add_base_rule(\n",
    "        'starts_large_content',\n",
    "        lambda conv: first_user_has_large_content(conv, 2000),\n",
    "        'First user message has large content (>2000 chars)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_code_patterns',\n",
    "        first_user_has_code_patterns,\n",
    "        'First user message contains code patterns'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_with_attachments',\n",
    "        first_user_has_attachments,\n",
    "        'First user message has attachments'\n",
    "    )\n",
    "    \n",
    "    # Supplemental rules based on tag combinations\n",
    "    tagger.add_supplemental_rule(\n",
    "        'coding_assistance',\n",
    "        lambda conv, tags: ('code_patterns' in tags or 'github_context' in tags),\n",
    "        'Likely coding assistance (has code patterns or GitHub context)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'coding_assistance_start',\n",
    "        lambda conv, tags: ('starts_code_patterns' in tags or \n",
    "                           'starts_large_content' in tags or \n",
    "                           'starts_with_attachments' in tags),\n",
    "        'Likely coding assistance based on how conversation starts'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'research_session',\n",
    "        lambda conv, tags: ('web_search' in tags and 'large_content' in tags),\n",
    "        'Research session (web search + substantial content)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'complex_analysis',\n",
    "        lambda conv, tags: ('reasoning' in tags and len(tags) >= 3),\n",
    "        'Complex analysis (reasoning + multiple other features)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'context_heavy_start',\n",
    "        lambda conv, tags: ('starts_large_content' in tags or 'starts_with_attachments' in tags),\n",
    "        'Conversation starts with substantial context (large content or files)'\n",
    "    )\n",
    "    \n",
    "    return tagger\n",
    "\n",
    "\n",
    "# Jupyter notebook usage example:\n",
    "\"\"\"\n",
    "# Load your conversations (assuming you have them in memory)\n",
    "# conversations = your_conversation_data\n",
    "\n",
    "# Create and configure tagger\n",
    "tagger = create_default_tagger()\n",
    "\n",
    "# Tag all conversations\n",
    "tagged_results = tagger.tag_conversations(conversations)\n",
    "\n",
    "# Print summary\n",
    "tagger.print_summary(tagged_results)\n",
    "\n",
    "# Filter for different types of conversations\n",
    "coding_conversations = tagger.filter_by_tags(\n",
    "    tagged_results, \n",
    "    include_tags={'coding_assistance'}\n",
    ")\n",
    "\n",
    "coding_start_conversations = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags={'coding_assistance_start'}\n",
    ")\n",
    "\n",
    "context_heavy_conversations = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags={'context_heavy_start'}\n",
    ")\n",
    "\n",
    "# Exclude conversations that start with large content or attachments\n",
    "lightweight_conversations = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    exclude_tags={'starts_large_content', 'starts_with_attachments'}\n",
    ")\n",
    "\n",
    "# Debug a specific conversation to see why it was tagged\n",
    "tagger.debug_conversation(conversations[0])\n",
    "\n",
    "# Add custom rules for your specific needs\n",
    "tagger.add_base_rule(\n",
    "    'starts_with_question',\n",
    "    lambda conv: get_first_user_message(conv) and \n",
    "                 get_first_user_message(conv).get('content', {}).get('text', '').strip().endswith('?'),\n",
    "    'Conversation starts with a question'\n",
    ")\n",
    "\n",
    "# Example: Find conversations that start with large content but aren't coding\n",
    "large_non_coding = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags={'starts_large_content'},\n",
    "    exclude_tags={'coding_assistance', 'coding_assistance_start'}\n",
    ")\n",
    "\n",
    "print(f\"Found {len(large_non_coding)} conversations with large initial content that aren't coding-related\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838cc9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged 1673 conversations\n",
      "Tag distribution:\n",
      "  canvas_operations: 3 (0.2%)\n",
      "  code_patterns: 1502 (89.8%)\n",
      "  coding_assistance: 1502 (89.8%)\n",
      "  coding_assistance_start: 260 (15.5%)\n",
      "  complex_analysis: 8 (0.5%)\n",
      "  context_heavy_start: 127 (7.6%)\n",
      "  large_content: 1362 (81.4%)\n",
      "  reasoning: 12 (0.7%)\n",
      "  research_session: 53 (3.2%)\n",
      "  starts_code_patterns: 240 (14.3%)\n",
      "  starts_large_content: 119 (7.1%)\n",
      "  starts_with_attachments: 8 (0.5%)\n",
      "  web_search: 66 (3.9%)\n"
     ]
    }
   ],
   "source": [
    "tagger = create_default_tagger()\n",
    "\n",
    "# Tag all conversations\n",
    "tagged_results = tagger.tag_conversations(convs)\n",
    "\n",
    "# Print summary\n",
    "tagger.print_summary(tagged_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e688fc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Create enhanced tagger with all previously discussed rules\\ntagger = create_default_tagger()\\ntagged_results = tagger.tag_conversations(conversations)\\n\\n# Enhanced summary shows both simple and structured tag statistics\\ntagger.print_summary(tagged_results)\\n\\n# ===== BASIC FILTERING (Boolean tags) =====\\n\\n# Find coding assistance conversations\\ncoding_conversations = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags=[\\'coding_assistance\\']\\n)\\n\\n# Exclude coding and enhanced features for clean analysis\\nbasic_conversations = tagger.filter_by_tags(\\n    tagged_results,\\n    exclude_tags=[\\'coding_assistance\\', \\'enhanced_conversation\\', \\'context_heavy_start\\']\\n)\\n\\n# Find research sessions\\nresearch_conversations = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags=[\\'research_session\\']\\n)\\n\\n# ===== STRUCTURED TAG FILTERING =====\\n\\n# Find short conversations (1-3 messages)\\nshort_conversations = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags=[{\\'name\\': \\'conversation_length\\', \\'category\\': \\'short\\'}]\\n)\\n\\n# Find conversations with 5-15 user messages\\nmedium_length_conversations = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags=[{\\'name\\': \\'conversation_length\\', \\'count\\': {\\'gte\\': 5, \\'lte\\': 15}}]\\n)\\n\\n# Find conversations with long prompts (mean > 1000 chars)\\nlong_prompt_conversations = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags=[{\\'name\\': \\'prompt_stats\\', \\'mean\\': {\\'gt\\': 1000}}]\\n)\\n\\n# Find conversations with consistent prompt lengths\\nconsistent_conversations = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags=[{\\'name\\': \\'prompt_stats\\', \\'consistency\\': \\'consistent\\'}]\\n)\\n\\n# ===== GIZMO/PLUGIN FILTERING =====\\n\\n# Find conversations using DALL-E specifically\\ndalle_conversations = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags=[{\\'name\\': \\'gizmo\\', \\'name\\': \\'dalle\\'}]\\n)\\n\\n# Find any gizmo usage\\nany_gizmo_conversations = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags=[\\'gizmo\\']\\n)\\n\\n# Find conversations using browser plugin\\nbrowser_conversations = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags=[{\\'name\\': \\'plugin\\', \\'name\\': \\'browser\\'}]\\n)\\n\\n# ===== COMPLEX PATTERN FILTERING =====\\n\\n# Find context dumps (short conversations with large initial content)\\ncontext_dumps = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags=[\\'context_dump\\']\\n)\\n\\n# Find interactive sessions (long, consistent conversations)\\ninteractive_sessions = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags=[\\'interactive_session\\']\\n)\\n\\n# Find evolving discussions (long conversations with variable prompts)\\nevolving_discussions = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags=[\\'evolving_discussion\\']\\n)\\n\\n# Find conversations that start with code context\\ncode_context_starts = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags=[\\'starts_code_patterns\\', \\'starts_code_attachments\\']\\n)\\n\\n# ===== STATISTICAL ANALYSIS =====\\n\\n# Extract conversation lengths for analysis\\nconversation_lengths = tagger.get_tag_values(tagged_results, \\'conversation_length\\', \\'count\\')\\navg_length = sum(conversation_lengths) / len(conversation_lengths)\\nprint(f\"Average conversation length: {avg_length:.1f} messages\")\\n\\n# Extract prompt statistics\\nmean_prompt_lengths = tagger.get_tag_values(tagged_results, \\'prompt_stats\\', \\'mean\\')\\noverall_avg_prompt = sum(mean_prompt_lengths) / len(mean_prompt_lengths)\\nprint(f\"Average prompt length across all conversations: {overall_avg_prompt:.1f} characters\")\\n\\n# Analyze prompt variance\\nprompt_variances = tagger.get_tag_values(tagged_results, \\'prompt_stats\\', \\'variance\\')\\nhigh_variance_count = sum(1 for v in prompt_variances if v > 10000)\\nprint(f\"Conversations with highly variable prompts: {high_variance_count}\")\\n\\n# ===== DEBUGGING SPECIFIC CASES =====\\n\\n# Debug a conversation to see all applied tags\\nprint(\"\\\\n=== Debug Example ===\")\\ntagger.debug_conversation(conversations[0])\\n\\n# Find and analyze the false positive from earlier\\nproblem_conversations = tagger.filter_by_tags(\\n    tagged_results,\\n    include_tags=[\\'canvas_operations\\'],\\n    exclude_tags=[\\'coding_assistance\\']\\n)\\nprint(f\"\\\\nFound {len(problem_conversations)} canvas conversations that aren\\'t coding-related\")\\n\\n# ===== COMPREHENSIVE FILTERING FOR DOWNSTREAM PROCESSING =====\\n\\n# Example: Clean conversations for analysis (exclude everything complex)\\nclean_for_analysis = tagger.filter_by_tags(\\n    tagged_results,\\n    exclude_tags=[\\n        \\'coding_assistance\\',           # No coding help\\n        \\'coding_assistance_start\\',     # No coding context\\n        \\'enhanced_conversation\\',       # No gizmos/plugins\\n        \\'context_heavy_start\\',         # No large initial context\\n        \\'context_dump\\',               # No context dumps\\n        \\'research_session\\'            # No research sessions\\n    ],\\n    include_tags=[\\n        {\\'name\\': \\'conversation_length\\', \\'count\\': {\\'gte\\': 2}},  # At least 2 messages\\n        {\\'name\\': \\'prompt_stats\\', \\'mean\\': {\\'lt\\': 1000}}         # Reasonable prompt length\\n    ]\\n)\\n\\nprint(f\"\\\\nFiltered to {len(clean_for_analysis)} clean conversations for downstream processing\")\\n\\n# ===== CUSTOM RULES =====\\n\\n# Add domain-specific rules\\ntagger.add_base_rule(\\n    \\'starts_with_question\\',\\n    lambda conv: (get_first_user_message(conv) and \\n                 get_first_user_message(conv).get(\\'content\\', {}).get(\\'text\\', \\'\\').strip().endswith(\\'?\\')),\\n    \\'Conversation starts with a question\\'\\n)\\n\\ntagger.add_supplemental_rule(\\n    \\'qa_session\\',\\n    lambda conv, tags: (\\n        any(tag.name == \\'starts_with_question\\' for tag in tags) and\\n        any(tag.name == \\'conversation_length\\' and \\n            tag.attributes.get(\\'category\\') in [\\'short\\', \\'medium\\'] for tag in tags)\\n    ),\\n    \\'Question-and-answer session\\'\\n)\\n\\n# Re-tag with new rules\\nupdated_results = tagger.tag_conversations(conversations)\\n\\n# ===== DISTRIBUTION ANALYSIS =====\\n\\nprint(\"\\\\n=== Tag Distribution Analysis ===\")\\ntag_counts = {}\\nfor result in tagged_results:\\n    for tag in result[\\'tags\\']:\\n        if tag.name not in tag_counts:\\n            tag_counts[tag.name] = 0\\n        tag_counts[tag.name] += 1\\n\\nfor tag_name, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True):\\n    percentage = (count / len(tagged_results)) * 100\\n    print(f\"{tag_name}: {count} ({percentage:.1f}%)\")\\n\\n# ===== GIZMO/PLUGIN USAGE ANALYSIS =====\\n\\nprint(\"\\\\n=== Gizmo/Plugin Usage ===\")\\ngizmo_usage = {}\\nplugin_usage = {}\\n\\nfor result in tagged_results:\\n    for tag in result[\\'tags\\']:\\n        if tag.name == \\'gizmo\\':\\n            gizmo_name = tag.attributes.get(\\'name\\', \\'unknown\\')\\n            gizmo_usage[gizmo_name] = gizmo_usage.get(gizmo_name, 0) + 1\\n        elif tag.name == \\'plugin\\':\\n            plugin_name = tag.attributes.get(\\'name\\', \\'unknown\\')\\n            plugin_usage[plugin_name] = plugin_usage.get(plugin_name, 0) + 1\\n\\nprint(\"Gizmos:\")\\nfor gizmo, count in sorted(gizmo_usage.items(), key=lambda x: x[1], reverse=True):\\n    print(f\"  {gizmo}: {count} conversations\")\\n\\nprint(\"Plugins:\")\\nfor plugin, count in sorted(plugin_usage.items(), key=lambda x: x[1], reverse=True):\\n    print(f\"  {plugin}: {count} conversations\")\\n\\n# ===== EXPORT FILTERED DATASETS =====\\n\\n# Create different filtered datasets for various purposes\\ndatasets = {\\n    \\'coding_assistance\\': tagger.filter_by_tags(tagged_results, include_tags=[\\'coding_assistance\\']),\\n    \\'research_sessions\\': tagger.filter_by_tags(tagged_results, include_tags=[\\'research_session\\']),\\n    \\'basic_qa\\': tagger.filter_by_tags(tagged_results, include_tags=[\\'qa_session\\']),\\n    \\'enhanced_features\\': tagger.filter_by_tags(tagged_results, include_tags=[\\'enhanced_conversation\\']),\\n    \\'clean_conversations\\': clean_for_analysis,\\n}\\n\\nfor dataset_name, conversations in datasets.items():\\n    print(f\"{dataset_name}: {len(conversations)} conversations\")\\n    # conversations now contains just the conversation objects you can process further\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation_tagger.py\n",
    "\"\"\"\n",
    "Enhanced conversation tagging system with structured tags supporting key-value attributes.\n",
    "Tags can be simple strings or objects with metadata for better analysis and filtering.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Any, List, Callable, Set, Union\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "class Tag:\n",
    "    \"\"\"Represents a tag with optional key-value attributes.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, **attributes):\n",
    "        self.name = name\n",
    "        self.attributes = attributes\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.attributes:\n",
    "            attrs_str = \", \".join(f\"{k}={v}\" for k, v in self.attributes.items())\n",
    "            return f\"{self.name}({attrs_str})\"\n",
    "        return self.name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Tag('{self.name}', {self.attributes})\"\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, str):\n",
    "            return self.name == other\n",
    "        elif isinstance(other, Tag):\n",
    "            return self.name == other.name and self.attributes == other.attributes\n",
    "        return False\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.name, tuple(sorted(self.attributes.items()))))\n",
    "    \n",
    "    def matches(self, name: str, **criteria) -> bool:\n",
    "        \"\"\"Check if tag matches name and optional attribute criteria.\"\"\"\n",
    "        if self.name != name:\n",
    "            return False\n",
    "        \n",
    "        for key, value in criteria.items():\n",
    "            if key not in self.attributes:\n",
    "                return False\n",
    "            \n",
    "            attr_value = self.attributes[key]\n",
    "            \n",
    "            # Support comparison operators\n",
    "            if isinstance(value, dict):\n",
    "                for op, target in value.items():\n",
    "                    if op == 'gt' and not (attr_value > target):\n",
    "                        return False\n",
    "                    elif op == 'gte' and not (attr_value >= target):\n",
    "                        return False\n",
    "                    elif op == 'lt' and not (attr_value < target):\n",
    "                        return False\n",
    "                    elif op == 'lte' and not (attr_value <= target):\n",
    "                        return False\n",
    "                    elif op == 'eq' and not (attr_value == target):\n",
    "                        return False\n",
    "                    elif op == 'ne' and not (attr_value != target):\n",
    "                        return False\n",
    "                    elif op == 'in' and not (attr_value in target):\n",
    "                        return False\n",
    "            else:\n",
    "                # Direct equality\n",
    "                if attr_value != value:\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "\n",
    "class ConversationTagger:\n",
    "    \"\"\"\n",
    "    Enhanced tagging system supporting structured tags with attributes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_rules: Dict[str, Callable] = {}\n",
    "        self.multi_tag_rules: Dict[str, Callable] = {}\n",
    "        self.supplemental_rules: Dict[str, Callable] = {}\n",
    "        self.rule_descriptions: Dict[str, str] = {}\n",
    "    \n",
    "    def add_base_rule(self, tag_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"Add a base tagging rule that returns bool or Tag object.\"\"\"\n",
    "        self.base_rules[tag_name] = rule_function\n",
    "        self.rule_descriptions[tag_name] = description\n",
    "    \n",
    "    def add_multi_tag_rule(self, rule_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"Add a rule that returns multiple tags (strings or Tag objects).\"\"\"\n",
    "        self.multi_tag_rules[rule_name] = rule_function\n",
    "        self.rule_descriptions[rule_name] = description\n",
    "    \n",
    "    def add_supplemental_rule(self, tag_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"Add a supplemental rule that depends on existing tags.\"\"\"\n",
    "        self.supplemental_rules[tag_name] = rule_function\n",
    "        self.rule_descriptions[tag_name] = description\n",
    "    \n",
    "    def _normalize_tag(self, tag: Union[str, Tag]) -> Tag:\n",
    "        \"\"\"Convert string tags to Tag objects.\"\"\"\n",
    "        if isinstance(tag, str):\n",
    "            return Tag(tag)\n",
    "        return tag\n",
    "    \n",
    "    def tag_conversation(self, conversation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Apply all tagging rules to a conversation.\"\"\"\n",
    "        tags = set()\n",
    "        debug_info = defaultdict(list)\n",
    "        \n",
    "        # Apply base rules\n",
    "        for tag_name, rule_func in self.base_rules.items():\n",
    "            try:\n",
    "                result = rule_func(conversation)\n",
    "                if result:\n",
    "                    if isinstance(result, bool):\n",
    "                        tag = Tag(tag_name)\n",
    "                    else:\n",
    "                        tag = self._normalize_tag(result)\n",
    "                    tags.add(tag)\n",
    "                    debug_info['applied_rules'].append(f\"BASE: {tag}\")\n",
    "                else:\n",
    "                    debug_info['skipped_rules'].append(f\"BASE: {tag_name}\")\n",
    "            except Exception as e:\n",
    "                debug_info['errors'].append(f\"BASE: {tag_name} - {str(e)}\")\n",
    "        \n",
    "        # Apply multi-tag rules\n",
    "        for rule_name, rule_func in self.multi_tag_rules.items():\n",
    "            try:\n",
    "                new_tags = rule_func(conversation)\n",
    "                if new_tags:\n",
    "                    normalized_tags = [self._normalize_tag(tag) for tag in new_tags]\n",
    "                    tags.update(normalized_tags)\n",
    "                    debug_info['applied_rules'].append(f\"MULTI: {rule_name} -> {[str(t) for t in normalized_tags]}\")\n",
    "                else:\n",
    "                    debug_info['skipped_rules'].append(f\"MULTI: {rule_name}\")\n",
    "            except Exception as e:\n",
    "                debug_info['errors'].append(f\"MULTI: {rule_name} - {str(e)}\")\n",
    "        \n",
    "        # Apply supplemental rules\n",
    "        max_iterations = 5\n",
    "        for iteration in range(max_iterations):\n",
    "            initial_tag_count = len(tags)\n",
    "            \n",
    "            for tag_name, rule_func in self.supplemental_rules.items():\n",
    "                # Check if tag already exists\n",
    "                if any(tag.name == tag_name for tag in tags):\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    result = rule_func(conversation, tags)\n",
    "                    if result:\n",
    "                        if isinstance(result, bool):\n",
    "                            tag = Tag(tag_name)\n",
    "                        else:\n",
    "                            tag = self._normalize_tag(result)\n",
    "                        tags.add(tag)\n",
    "                        debug_info['applied_rules'].append(f\"SUPP: {tag} (iter {iteration})\")\n",
    "                    else:\n",
    "                        debug_info['skipped_rules'].append(f\"SUPP: {tag_name} (iter {iteration})\")\n",
    "                except Exception as e:\n",
    "                    debug_info['errors'].append(f\"SUPP: {tag_name} - {str(e)}\")\n",
    "            \n",
    "            if len(tags) == initial_tag_count:\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'conversation_id': conversation.get('conversation_id', conversation.get('id', 'unknown')),\n",
    "            'title': conversation.get('title', 'Untitled'),\n",
    "            'tags': list(tags),\n",
    "            'debug_info': dict(debug_info),\n",
    "            'conversation': conversation\n",
    "        }\n",
    "    \n",
    "    def tag_conversations(self, conversations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Tag multiple conversations.\"\"\"\n",
    "        return [self.tag_conversation(conv) for conv in conversations]\n",
    "    \n",
    "    def filter_by_tags(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                      include_tags: List[Union[str, Dict]] = None,\n",
    "                      exclude_tags: List[Union[str, Dict]] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Filter conversations by tags with attribute support.\n",
    "        \n",
    "        Args:\n",
    "            include_tags: List of tag names or dicts with criteria\n",
    "                Examples: ['web_search', {'name': 'gizmo', 'type': 'dalle'}]\n",
    "            exclude_tags: Similar format for exclusions\n",
    "        \"\"\"\n",
    "        filtered = []\n",
    "        \n",
    "        for tagged_conv in tagged_conversations:\n",
    "            tags = tagged_conv['tags']\n",
    "            \n",
    "            # Check exclusions first\n",
    "            if exclude_tags:\n",
    "                should_exclude = False\n",
    "                for exclude_criterion in exclude_tags:\n",
    "                    if self._matches_criterion(tags, exclude_criterion):\n",
    "                        should_exclude = True\n",
    "                        break\n",
    "                if should_exclude:\n",
    "                    continue\n",
    "            \n",
    "            # Check inclusions\n",
    "            if include_tags:\n",
    "                should_include = True\n",
    "                for include_criterion in include_tags:\n",
    "                    if not self._matches_criterion(tags, include_criterion):\n",
    "                        should_include = False\n",
    "                        break\n",
    "                if not should_include:\n",
    "                    continue\n",
    "            \n",
    "            filtered.append(tagged_conv)\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def _matches_criterion(self, tags: List[Tag], criterion: Union[str, Dict]) -> bool:\n",
    "        \"\"\"Check if any tag matches the given criterion.\"\"\"\n",
    "        if isinstance(criterion, str):\n",
    "            return any(tag.name == criterion for tag in tags)\n",
    "        \n",
    "        elif isinstance(criterion, dict):\n",
    "            name = criterion.get('name')\n",
    "            if not name:\n",
    "                return False\n",
    "            \n",
    "            criteria = {k: v for k, v in criterion.items() if k != 'name'}\n",
    "            return any(tag.matches(name, **criteria) for tag in tags)\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_tag_values(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                      tag_name: str, attribute: str) -> List[Any]:\n",
    "        \"\"\"Extract attribute values from tags across conversations.\"\"\"\n",
    "        values = []\n",
    "        for tagged_conv in tagged_conversations:\n",
    "            for tag in tagged_conv['tags']:\n",
    "                if tag.name == tag_name and attribute in tag.attributes:\n",
    "                    values.append(tag.attributes[attribute])\n",
    "        return values\n",
    "    \n",
    "    def print_summary(self, tagged_conversations: List[Dict[str, Any]]):\n",
    "        \"\"\"Print enhanced summary with attribute statistics.\"\"\"\n",
    "        total = len(tagged_conversations)\n",
    "        tag_counts = defaultdict(int)\n",
    "        tag_attributes = defaultdict(list)\n",
    "        \n",
    "        for tagged_conv in tagged_conversations:\n",
    "            for tag in tagged_conv['tags']:\n",
    "                tag_counts[tag.name] += 1\n",
    "                if tag.attributes:\n",
    "                    tag_attributes[tag.name].extend(tag.attributes.items())\n",
    "        \n",
    "        print(f\"Tagged {total} conversations\")\n",
    "        print(f\"Tag distribution:\")\n",
    "        for tag_name, count in sorted(tag_counts.items()):\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"  {tag_name}: {count} ({percentage:.1f}%)\")\n",
    "            \n",
    "            # Show attribute statistics\n",
    "            if tag_name in tag_attributes:\n",
    "                attrs = defaultdict(list)\n",
    "                for key, value in tag_attributes[tag_name]:\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        attrs[key].append(value)\n",
    "                \n",
    "                for attr_name, values in attrs.items():\n",
    "                    if values:\n",
    "                        avg = sum(values) / len(values)\n",
    "                        min_val = min(values)\n",
    "                        max_val = max(values)\n",
    "                        print(f\"    {attr_name}: avg={avg:.1f}, range=[{min_val}, {max_val}]\")\n",
    "    \n",
    "    def debug_conversation(self, conversation: Dict[str, Any]):\n",
    "        \"\"\"Debug a single conversation with enhanced tag details.\"\"\"\n",
    "        result = self.tag_conversation(conversation)\n",
    "        \n",
    "        print(f\"Conversation: {result['title'][:50]}...\")\n",
    "        print(f\"Tags applied:\")\n",
    "        for tag in result['tags']:\n",
    "            print(f\"  {tag}\")\n",
    "        \n",
    "        print(f\"\\nRule details:\")\n",
    "        for rule in result['debug_info'].get('applied_rules', []):\n",
    "            print(f\"   {rule}\")\n",
    "        \n",
    "        for error in result['debug_info'].get('errors', []):\n",
    "            print(f\"    ERROR: {error}\")\n",
    "\n",
    "\n",
    "# Enhanced rule functions for all previously discussed tagging rules\n",
    "\n",
    "def get_all_user_messages(conversation: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get all user messages in chronological order.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    user_messages = []\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        author = message.get('author', {})\n",
    "        if author.get('role') == 'user':\n",
    "            create_time = message.get('create_time') or 0\n",
    "            user_messages.append((create_time, message))\n",
    "    \n",
    "    user_messages.sort(key=lambda x: x[0])\n",
    "    return [msg for _, msg in user_messages]\n",
    "\n",
    "\n",
    "def get_first_user_message(conversation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Find the first user message in the conversation.\"\"\"\n",
    "    user_messages = get_all_user_messages(conversation)\n",
    "    return user_messages[0] if user_messages else None\n",
    "\n",
    "\n",
    "def create_conversation_length_tag(conversation: Dict[str, Any]) -> Tag:\n",
    "    \"\"\"Create structured tag for conversation length.\"\"\"\n",
    "    user_count = len(get_all_user_messages(conversation))\n",
    "    \n",
    "    # Determine category\n",
    "    if user_count == 1:\n",
    "        category = 'single'\n",
    "    elif user_count <= 3:\n",
    "        category = 'short'\n",
    "    elif user_count <= 10:\n",
    "        category = 'medium'\n",
    "    elif user_count <= 25:\n",
    "        category = 'long'\n",
    "    else:\n",
    "        category = 'very_long'\n",
    "    \n",
    "    return Tag('conversation_length', count=user_count, category=category)\n",
    "\n",
    "\n",
    "def create_prompt_stats_tag(conversation: Dict[str, Any]) -> Tag:\n",
    "    \"\"\"Create structured tag for prompt statistics.\"\"\"\n",
    "    user_messages = get_all_user_messages(conversation)\n",
    "    \n",
    "    if not user_messages:\n",
    "        return Tag('prompt_stats', count=0, mean=0, median=0, variance=0, \n",
    "                  length_category='none', consistency='none')\n",
    "    \n",
    "    # Calculate message lengths\n",
    "    lengths = []\n",
    "    for message in user_messages:\n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        lengths.append(len(all_text))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_length = sum(lengths) / len(lengths)\n",
    "    sorted_lengths = sorted(lengths)\n",
    "    n = len(sorted_lengths)\n",
    "    median_length = (sorted_lengths[n//2] if n % 2 == 1 \n",
    "                    else (sorted_lengths[n//2-1] + sorted_lengths[n//2]) / 2)\n",
    "    variance = sum((x - mean_length) ** 2 for x in lengths) / len(lengths) if len(lengths) > 1 else 0\n",
    "    \n",
    "    # Determine categories\n",
    "    if mean_length < 50:\n",
    "        length_category = 'very_short'\n",
    "    elif mean_length < 200:\n",
    "        length_category = 'short'\n",
    "    elif mean_length < 1000:\n",
    "        length_category = 'medium'\n",
    "    elif mean_length < 3000:\n",
    "        length_category = 'long'\n",
    "    else:\n",
    "        length_category = 'very_long'\n",
    "    \n",
    "    if variance < 1000:\n",
    "        consistency = 'consistent'\n",
    "    elif variance < 10000:\n",
    "        consistency = 'mixed'\n",
    "    else:\n",
    "        consistency = 'variable'\n",
    "    \n",
    "    return Tag('prompt_stats', \n",
    "               count=len(lengths),\n",
    "               mean=round(mean_length, 1),\n",
    "               median=round(median_length, 1),\n",
    "               variance=round(variance, 1),\n",
    "               length_category=length_category,\n",
    "               consistency=consistency)\n",
    "\n",
    "\n",
    "def create_gizmo_plugin_tags(conversation: Dict[str, Any]) -> List[Tag]:\n",
    "    \"\"\"Create structured tags for gizmos and plugins.\"\"\"\n",
    "    tags = []\n",
    "    gizmos = set()\n",
    "    plugins = set()\n",
    "    \n",
    "    # Check conversation-level\n",
    "    if conversation.get('gizmo_id'):\n",
    "        gizmos.add(conversation['gizmo_id'])\n",
    "    \n",
    "    plugin_ids = conversation.get('plugin_ids', [])\n",
    "    if plugin_ids:\n",
    "        plugins.update(plugin_ids)\n",
    "    \n",
    "    # Check message-level\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        \n",
    "        # Invoked plugins\n",
    "        invoked_plugin = metadata.get('invoked_plugin', {})\n",
    "        if invoked_plugin:\n",
    "            if invoked_plugin.get('plugin_id'):\n",
    "                plugins.add(invoked_plugin['plugin_id'])\n",
    "            if invoked_plugin.get('namespace'):\n",
    "                plugins.add(invoked_plugin['namespace'])\n",
    "        \n",
    "        # Gizmo usage\n",
    "        if metadata.get('gizmo_id'):\n",
    "            gizmos.add(metadata['gizmo_id'])\n",
    "    \n",
    "    # Create tags\n",
    "    for gizmo in gizmos:\n",
    "        tags.append(Tag('gizmo', name=gizmo))\n",
    "    \n",
    "    for plugin in plugins:\n",
    "        tags.append(Tag('plugin', name=plugin))\n",
    "    \n",
    "    return tags\n",
    "\n",
    "\n",
    "# Boolean rule functions for basic content analysis\n",
    "def has_large_content(conversation: Dict[str, Any], min_length: int = 2000) -> bool:\n",
    "    \"\"\"Check if conversation has unusually large content anywhere.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        if len(text) > min_length:\n",
    "            return True\n",
    "            \n",
    "        parts = content.get('parts', [])\n",
    "        for part in parts:\n",
    "            if isinstance(part, str) and len(part) > min_length:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_code_patterns(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for clear code patterns anywhere in conversation.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "        \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        \n",
    "        # Strong code indicators\n",
    "        code_indicators = [\n",
    "            '```',  # Code blocks\n",
    "            'def ', 'function ', 'class ',  # Function/class definitions\n",
    "            'import ', 'from ', 'require(',  # Import statements\n",
    "            '#!/bin/', '#include', 'using namespace',  # Script headers\n",
    "        ]\n",
    "        \n",
    "        if any(indicator in all_text for indicator in code_indicators):\n",
    "            return True\n",
    "            \n",
    "        # Also check for high density of coding keywords\n",
    "        coding_keywords = ['function', 'class', 'import', 'def ', 'const ', 'let ', 'var ', 'return', 'if ', 'for ', 'while ']\n",
    "        keyword_count = sum(1 for keyword in coding_keywords if keyword in all_text.lower())\n",
    "        if len(all_text) > 1000 and keyword_count >= 3:  # Multiple coding keywords in large text suggest actual code\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_github_repos(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if GitHub repositories were selected for context.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        repos = metadata.get('selected_github_repos', [])\n",
    "        if repos:  # Non-empty list\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_canvas_operations(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for canvas/document operations.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if metadata.get('canvas'):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_web_search(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for web search operations.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if (metadata.get('search_queries') or \n",
    "            metadata.get('search_result_groups') or\n",
    "            metadata.get('content_references')):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_reasoning_thoughts(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for reasoning/thinking patterns.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        content = message.get('content', {})\n",
    "        if content.get('thoughts'):  # Reasoning thoughts\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_code_execution(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for code execution artifacts.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if (metadata.get('aggregate_result') or \n",
    "            metadata.get('jupyter_messages')):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "# First user message specific rules\n",
    "def first_user_has_large_content(conversation: Dict[str, Any], min_length: int = 2000) -> bool:\n",
    "    \"\"\"Check if the first user message has large content.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    content = first_message.get('content', {})\n",
    "    text = content.get('text', '')\n",
    "    if len(text) > min_length:\n",
    "        return True\n",
    "        \n",
    "    parts = content.get('parts', [])\n",
    "    for part in parts:\n",
    "        if isinstance(part, str) and len(part) > min_length:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def first_user_has_code_patterns(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message contains code patterns.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    content = first_message.get('content', {})\n",
    "    text = content.get('text', '')\n",
    "    parts = content.get('parts', [])\n",
    "    all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "    \n",
    "    # Strong code indicators\n",
    "    code_indicators = [\n",
    "        '```',  # Code blocks\n",
    "        'def ', 'function ', 'class ',  # Definitions\n",
    "        'import ', 'from ', 'require(',  # Imports\n",
    "        '#!/bin/', '#include',  # Script headers\n",
    "    ]\n",
    "    \n",
    "    return any(indicator in all_text for indicator in code_indicators)\n",
    "\n",
    "\n",
    "def first_user_has_attachments(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message has attachments.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    metadata = first_message.get('metadata', {})\n",
    "    attachments = metadata.get('attachments', [])\n",
    "    return len(attachments) > 0\n",
    "\n",
    "\n",
    "def first_user_has_code_attachments(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message has code-related attachments.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    metadata = first_message.get('metadata', {})\n",
    "    attachments = metadata.get('attachments', [])\n",
    "    \n",
    "    for attachment in attachments:\n",
    "        mime_type = attachment.get('mime_type', '').lower()\n",
    "        name = attachment.get('name', '').lower()\n",
    "        \n",
    "        # Check for code file extensions\n",
    "        code_extensions = ['.py', '.js', '.java', '.cpp', '.c', '.go', '.rs', '.ts', '.jsx', '.tsx', '.sql', '.sh', '.rb', '.php']\n",
    "        if any(ext in name for ext in code_extensions):\n",
    "            return True\n",
    "            \n",
    "        # Check for code-related MIME types\n",
    "        code_mimes = ['text/x-python', 'text/x-java', 'application/javascript', 'text/x-script']\n",
    "        if any(mime in mime_type for mime in code_mimes):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def create_default_tagger() -> ConversationTagger:\n",
    "    \"\"\"Create a tagger with all previously discussed rules in the enhanced structured framework.\"\"\"\n",
    "    tagger = ConversationTagger()\n",
    "    \n",
    "    # ===== BASIC CONTENT ANALYSIS RULES (Boolean) =====\n",
    "    tagger.add_base_rule(\n",
    "        'large_content', \n",
    "        lambda conv: has_large_content(conv, 2000),\n",
    "        'Content longer than 2000 characters anywhere in conversation'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'code_patterns', \n",
    "        has_code_patterns,\n",
    "        'Contains clear code patterns (```, def, function, etc.)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'github_context',\n",
    "        has_github_repos,\n",
    "        'GitHub repositories selected for context'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'canvas_operations',\n",
    "        has_canvas_operations,\n",
    "        'Uses canvas/document features'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'web_search',\n",
    "        has_web_search,\n",
    "        'Includes web search functionality'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'reasoning',\n",
    "        has_reasoning_thoughts,\n",
    "        'Contains reasoning/thinking content'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'code_execution',\n",
    "        has_code_execution,\n",
    "        'Contains code execution (Jupyter, aggregate results)'\n",
    "    )\n",
    "    \n",
    "    # ===== FIRST USER MESSAGE RULES (Boolean) =====\n",
    "    tagger.add_base_rule(\n",
    "        'starts_large_content',\n",
    "        lambda conv: first_user_has_large_content(conv, 2000),\n",
    "        'First user message has large content (>2000 chars)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_code_patterns',\n",
    "        first_user_has_code_patterns,\n",
    "        'First user message contains code patterns'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_with_attachments',\n",
    "        first_user_has_attachments,\n",
    "        'First user message has any attachments'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_code_attachments',\n",
    "        first_user_has_code_attachments,\n",
    "        'First user message has code-related attachments'\n",
    "    )\n",
    "    \n",
    "    # ===== STRUCTURED TAG RULES =====\n",
    "    tagger.add_base_rule(\n",
    "        'conversation_length',\n",
    "        create_conversation_length_tag,\n",
    "        'Conversation length with count and category'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'prompt_stats',\n",
    "        create_prompt_stats_tag,\n",
    "        'User message statistics (length, variance, etc.)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_multi_tag_rule(\n",
    "        'gizmo_plugin_usage',\n",
    "        create_gizmo_plugin_tags,\n",
    "        'Specific gizmos and plugins used in conversation'\n",
    "    )\n",
    "    \n",
    "    # ===== SUPPLEMENTAL RULES (Based on existing tags) =====\n",
    "    \n",
    "    # Coding assistance detection\n",
    "    tagger.add_supplemental_rule(\n",
    "        'coding_assistance',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['code_patterns', 'github_context', 'code_execution'] for tag in tags)\n",
    "        ),\n",
    "        'Likely coding assistance (code patterns, GitHub, or execution)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'coding_assistance_start',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['starts_code_patterns', 'starts_large_content', \n",
    "                           'starts_with_attachments', 'starts_code_attachments'] for tag in tags)\n",
    "        ),\n",
    "        'Likely coding assistance based on how conversation starts'\n",
    "    )\n",
    "    \n",
    "    # Research and analysis patterns  \n",
    "    tagger.add_supplemental_rule(\n",
    "        'research_session',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'web_search' for tag in tags) and \n",
    "            any(tag.name == 'large_content' for tag in tags)\n",
    "        ),\n",
    "        'Research session (web search + substantial content)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'complex_analysis',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'reasoning' for tag in tags) and \n",
    "            len([tag for tag in tags if tag.name in ['web_search', 'large_content', 'canvas_operations']]) >= 2\n",
    "        ),\n",
    "        'Complex analysis (reasoning + multiple advanced features)'\n",
    "    )\n",
    "    \n",
    "    # Context and interaction patterns\n",
    "    tagger.add_supplemental_rule(\n",
    "        'context_heavy_start',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['starts_large_content', 'starts_with_attachments'] for tag in tags)\n",
    "        ),\n",
    "        'Conversation starts with substantial context'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'enhanced_conversation',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['gizmo', 'plugin'] for tag in tags)\n",
    "        ),\n",
    "        'Uses enhanced features (gizmos or plugins)'\n",
    "    )\n",
    "    \n",
    "    # Length-based classifications using structured tags\n",
    "    tagger.add_supplemental_rule(\n",
    "        'brief_interaction',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('category') in ['single', 'short'] for tag in tags)\n",
    "        ),\n",
    "        'Brief interaction (1-3 user messages)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'extended_conversation',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('category') in ['long', 'very_long'] for tag in tags)\n",
    "        ),\n",
    "        'Extended conversation (11+ user messages)'\n",
    "    )\n",
    "    \n",
    "    # Prompt pattern classifications using structured tags\n",
    "    tagger.add_supplemental_rule(\n",
    "        'long_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('length_category') in ['long', 'very_long'] for tag in tags)\n",
    "        ),\n",
    "        'Conversation has consistently long prompts'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'short_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('length_category') in ['very_short', 'short'] for tag in tags)\n",
    "        ),\n",
    "        'Conversation has consistently short prompts'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'consistent_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'consistent' for tag in tags)\n",
    "        ),\n",
    "        'User prompts are consistent in length'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'variable_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'variable' for tag in tags)\n",
    "        ),\n",
    "        'User prompts vary significantly in length'\n",
    "    )\n",
    "    \n",
    "    # Combined patterns for specific use cases\n",
    "    tagger.add_supplemental_rule(\n",
    "        'context_dump',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('category') in ['single', 'short'] for tag in tags) and\n",
    "            any(tag.name in ['starts_large_content', 'starts_with_attachments'] for tag in tags)\n",
    "        ),\n",
    "        'Short conversation starting with large context (likely context dump)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'interactive_session',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('count', 0) >= 5 for tag in tags) and\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'consistent' for tag in tags)\n",
    "        ),\n",
    "        'Extended back-and-forth conversation with consistent prompt style'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'evolving_discussion',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('count', 0) >= 5 for tag in tags) and\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'variable' for tag in tags)\n",
    "        ),\n",
    "        'Extended conversation where prompt style evolves'\n",
    "    )\n",
    "    \n",
    "    return tagger\n",
    "\n",
    "\n",
    "# Enhanced usage examples with all ported rules\n",
    "\"\"\"\n",
    "# Create enhanced tagger with all previously discussed rules\n",
    "tagger = create_default_tagger()\n",
    "tagged_results = tagger.tag_conversations(conversations)\n",
    "\n",
    "# Enhanced summary shows both simple and structured tag statistics\n",
    "tagger.print_summary(tagged_results)\n",
    "\n",
    "# ===== BASIC FILTERING (Boolean tags) =====\n",
    "\n",
    "# Find coding assistance conversations\n",
    "coding_conversations = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags=['coding_assistance']\n",
    ")\n",
    "\n",
    "# Exclude coding and enhanced features for clean analysis\n",
    "basic_conversations = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    exclude_tags=['coding_assistance', 'enhanced_conversation', 'context_heavy_start']\n",
    ")\n",
    "\n",
    "# Find research sessions\n",
    "research_conversations = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags=['research_session']\n",
    ")\n",
    "\n",
    "# ===== STRUCTURED TAG FILTERING =====\n",
    "\n",
    "# Find short conversations (1-3 messages)\n",
    "short_conversations = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags=[{'name': 'conversation_length', 'category': 'short'}]\n",
    ")\n",
    "\n",
    "# Find conversations with 5-15 user messages\n",
    "medium_length_conversations = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags=[{'name': 'conversation_length', 'count': {'gte': 5, 'lte': 15}}]\n",
    ")\n",
    "\n",
    "# Find conversations with long prompts (mean > 1000 chars)\n",
    "long_prompt_conversations = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags=[{'name': 'prompt_stats', 'mean': {'gt': 1000}}]\n",
    ")\n",
    "\n",
    "# Find conversations with consistent prompt lengths\n",
    "consistent_conversations = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags=[{'name': 'prompt_stats', 'consistency': 'consistent'}]\n",
    ")\n",
    "\n",
    "# ===== GIZMO/PLUGIN FILTERING =====\n",
    "\n",
    "# Find conversations using DALL-E specifically\n",
    "dalle_conversations = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags=[{'name': 'gizmo', 'name': 'dalle'}]\n",
    ")\n",
    "\n",
    "# Find any gizmo usage\n",
    "any_gizmo_conversations = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags=['gizmo']\n",
    ")\n",
    "\n",
    "# Find conversations using browser plugin\n",
    "browser_conversations = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags=[{'name': 'plugin', 'name': 'browser'}]\n",
    ")\n",
    "\n",
    "# ===== COMPLEX PATTERN FILTERING =====\n",
    "\n",
    "# Find context dumps (short conversations with large initial content)\n",
    "context_dumps = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags=['context_dump']\n",
    ")\n",
    "\n",
    "# Find interactive sessions (long, consistent conversations)\n",
    "interactive_sessions = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags=['interactive_session']\n",
    ")\n",
    "\n",
    "# Find evolving discussions (long conversations with variable prompts)\n",
    "evolving_discussions = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags=['evolving_discussion']\n",
    ")\n",
    "\n",
    "# Find conversations that start with code context\n",
    "code_context_starts = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags=['starts_code_patterns', 'starts_code_attachments']\n",
    ")\n",
    "\n",
    "# ===== STATISTICAL ANALYSIS =====\n",
    "\n",
    "# Extract conversation lengths for analysis\n",
    "conversation_lengths = tagger.get_tag_values(tagged_results, 'conversation_length', 'count')\n",
    "avg_length = sum(conversation_lengths) / len(conversation_lengths)\n",
    "print(f\"Average conversation length: {avg_length:.1f} messages\")\n",
    "\n",
    "# Extract prompt statistics\n",
    "mean_prompt_lengths = tagger.get_tag_values(tagged_results, 'prompt_stats', 'mean')\n",
    "overall_avg_prompt = sum(mean_prompt_lengths) / len(mean_prompt_lengths)\n",
    "print(f\"Average prompt length across all conversations: {overall_avg_prompt:.1f} characters\")\n",
    "\n",
    "# Analyze prompt variance\n",
    "prompt_variances = tagger.get_tag_values(tagged_results, 'prompt_stats', 'variance')\n",
    "high_variance_count = sum(1 for v in prompt_variances if v > 10000)\n",
    "print(f\"Conversations with highly variable prompts: {high_variance_count}\")\n",
    "\n",
    "# ===== DEBUGGING SPECIFIC CASES =====\n",
    "\n",
    "# Debug a conversation to see all applied tags\n",
    "print(\"\\\\n=== Debug Example ===\")\n",
    "tagger.debug_conversation(conversations[0])\n",
    "\n",
    "# Find and analyze the false positive from earlier\n",
    "problem_conversations = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags=['canvas_operations'],\n",
    "    exclude_tags=['coding_assistance']\n",
    ")\n",
    "print(f\"\\\\nFound {len(problem_conversations)} canvas conversations that aren't coding-related\")\n",
    "\n",
    "# ===== COMPREHENSIVE FILTERING FOR DOWNSTREAM PROCESSING =====\n",
    "\n",
    "# Example: Clean conversations for analysis (exclude everything complex)\n",
    "clean_for_analysis = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    exclude_tags=[\n",
    "        'coding_assistance',           # No coding help\n",
    "        'coding_assistance_start',     # No coding context\n",
    "        'enhanced_conversation',       # No gizmos/plugins\n",
    "        'context_heavy_start',         # No large initial context\n",
    "        'context_dump',               # No context dumps\n",
    "        'research_session'            # No research sessions\n",
    "    ],\n",
    "    include_tags=[\n",
    "        {'name': 'conversation_length', 'count': {'gte': 2}},  # At least 2 messages\n",
    "        {'name': 'prompt_stats', 'mean': {'lt': 1000}}         # Reasonable prompt length\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\\\nFiltered to {len(clean_for_analysis)} clean conversations for downstream processing\")\n",
    "\n",
    "# ===== CUSTOM RULES =====\n",
    "\n",
    "# Add domain-specific rules\n",
    "tagger.add_base_rule(\n",
    "    'starts_with_question',\n",
    "    lambda conv: (get_first_user_message(conv) and \n",
    "                 get_first_user_message(conv).get('content', {}).get('text', '').strip().endswith('?')),\n",
    "    'Conversation starts with a question'\n",
    ")\n",
    "\n",
    "tagger.add_supplemental_rule(\n",
    "    'qa_session',\n",
    "    lambda conv, tags: (\n",
    "        any(tag.name == 'starts_with_question' for tag in tags) and\n",
    "        any(tag.name == 'conversation_length' and \n",
    "            tag.attributes.get('category') in ['short', 'medium'] for tag in tags)\n",
    "    ),\n",
    "    'Question-and-answer session'\n",
    ")\n",
    "\n",
    "# Re-tag with new rules\n",
    "updated_results = tagger.tag_conversations(conversations)\n",
    "\n",
    "# ===== DISTRIBUTION ANALYSIS =====\n",
    "\n",
    "print(\"\\\\n=== Tag Distribution Analysis ===\")\n",
    "tag_counts = {}\n",
    "for result in tagged_results:\n",
    "    for tag in result['tags']:\n",
    "        if tag.name not in tag_counts:\n",
    "            tag_counts[tag.name] = 0\n",
    "        tag_counts[tag.name] += 1\n",
    "\n",
    "for tag_name, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / len(tagged_results)) * 100\n",
    "    print(f\"{tag_name}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# ===== GIZMO/PLUGIN USAGE ANALYSIS =====\n",
    "\n",
    "print(\"\\\\n=== Gizmo/Plugin Usage ===\")\n",
    "gizmo_usage = {}\n",
    "plugin_usage = {}\n",
    "\n",
    "for result in tagged_results:\n",
    "    for tag in result['tags']:\n",
    "        if tag.name == 'gizmo':\n",
    "            gizmo_name = tag.attributes.get('name', 'unknown')\n",
    "            gizmo_usage[gizmo_name] = gizmo_usage.get(gizmo_name, 0) + 1\n",
    "        elif tag.name == 'plugin':\n",
    "            plugin_name = tag.attributes.get('name', 'unknown')\n",
    "            plugin_usage[plugin_name] = plugin_usage.get(plugin_name, 0) + 1\n",
    "\n",
    "print(\"Gizmos:\")\n",
    "for gizmo, count in sorted(gizmo_usage.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {gizmo}: {count} conversations\")\n",
    "\n",
    "print(\"Plugins:\")\n",
    "for plugin, count in sorted(plugin_usage.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {plugin}: {count} conversations\")\n",
    "\n",
    "# ===== EXPORT FILTERED DATASETS =====\n",
    "\n",
    "# Create different filtered datasets for various purposes\n",
    "datasets = {\n",
    "    'coding_assistance': tagger.filter_by_tags(tagged_results, include_tags=['coding_assistance']),\n",
    "    'research_sessions': tagger.filter_by_tags(tagged_results, include_tags=['research_session']),\n",
    "    'basic_qa': tagger.filter_by_tags(tagged_results, include_tags=['qa_session']),\n",
    "    'enhanced_features': tagger.filter_by_tags(tagged_results, include_tags=['enhanced_conversation']),\n",
    "    'clean_conversations': clean_for_analysis,\n",
    "}\n",
    "\n",
    "for dataset_name, conversations in datasets.items():\n",
    "    print(f\"{dataset_name}: {len(conversations)} conversations\")\n",
    "    # conversations now contains just the conversation objects you can process further\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acc33dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged 1673 conversations\n",
      "Tag distribution:\n",
      "  brief_interaction: 906 (54.2%)\n",
      "  canvas_operations: 3 (0.2%)\n",
      "  code_execution: 26 (1.6%)\n",
      "  code_patterns: 1546 (92.4%)\n",
      "  coding_assistance: 1551 (92.7%)\n",
      "  coding_assistance_start: 260 (15.5%)\n",
      "  complex_analysis: 5 (0.3%)\n",
      "  consistent_prompts: 896 (53.6%)\n",
      "  context_dump: 64 (3.8%)\n",
      "  context_heavy_start: 127 (7.6%)\n",
      "  conversation_length: 1673 (100.0%)\n",
      "    count: avg=7.6, range=[1, 160]\n",
      "  evolving_discussion: 247 (14.8%)\n",
      "  extended_conversation: 304 (18.2%)\n",
      "  interactive_session: 184 (11.0%)\n",
      "  large_content: 1362 (81.4%)\n",
      "  long_prompts: 151 (9.0%)\n",
      "  prompt_stats: 1673 (100.0%)\n",
      "    count: avg=7.6, range=[1, 160]\n",
      "    mean: avg=1142.4, range=[1.0, 184662.5]\n",
      "    median: avg=814.2, range=[1.0, 184662.5]\n",
      "    variance: avg=51563065.2, range=[0, 34065162056.2]\n",
      "  reasoning: 12 (0.7%)\n",
      "  research_session: 53 (3.2%)\n",
      "  short_prompts: 1261 (75.4%)\n",
      "  starts_code_patterns: 240 (14.3%)\n",
      "  starts_large_content: 119 (7.1%)\n",
      "  starts_with_attachments: 8 (0.5%)\n",
      "  variable_prompts: 405 (24.2%)\n",
      "  web_search: 66 (3.9%)\n"
     ]
    }
   ],
   "source": [
    "tagger = create_default_tagger()\n",
    "tagged_results = tagger.tag_conversations(convs)\n",
    "tagger.print_summary(tagged_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75e6b2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Create enhanced tagger with all rules\\ntagger = create_default_tagger()\\ntagged_results = tagger.tag_conversations(conversations)\\n\\n# ===== COMPREHENSIVE SUMMARY WITH DEBUGGING =====\\n\\n# Basic summary (shows all tags with counts and percentages)\\ntagger.print_summary(tagged_results)\\n\\n# Detailed breakdown (shows gizmo/plugin specifics, distributions)\\ntagger.print_detailed_breakdown(tagged_results)\\n\\n# Debug tag creation for first few conversations\\ntagger.debug_tag_creation(conversations, max_conversations=3)\\n\\n# Validate the tag system is working correctly\\ntagger.validate_tag_system(tagged_results)\\n\\n# ===== INVESTIGATE MISSING GIZMO/PLUGIN TAGS =====\\n\\n# If no gizmo/plugin tags show up, let\\'s debug why\\nprint(\"\\\\n=== INVESTIGATING GIZMO/PLUGIN DETECTION ===\")\\n\\n# Check raw data for gizmo/plugin fields in first few conversations\\nfor i, conv in enumerate(conversations[:5]):\\n    print(f\"\\\\nConversation {i+1}: {conv.get(\\'title\\', \\'Untitled\\')[:50]}...\")\\n\\n    # Check conversation-level gizmo/plugin fields\\n    print(f\"  gizmo_id: {conv.get(\\'gizmo_id\\')}\")\\n    print(f\"  plugin_ids: {conv.get(\\'plugin_ids\\')}\")\\n\\n    # Check message-level fields\\n    mapping = conv.get(\\'mapping\\', {})\\n    gizmo_messages = []\\n    plugin_messages = []\\n\\n    for node_id, node in mapping.items():\\n        message = node.get(\\'message\\')\\n        if not message:\\n            continue\\n\\n        metadata = message.get(\\'metadata\\', {})\\n        if metadata.get(\\'gizmo_id\\'):\\n            gizmo_messages.append(metadata[\\'gizmo_id\\'])\\n        if metadata.get(\\'invoked_plugin\\'):\\n            plugin_messages.append(metadata[\\'invoked_plugin\\'])\\n\\n    if gizmo_messages:\\n        print(f\"  message-level gizmos: {gizmo_messages}\")\\n    if plugin_messages:\\n        print(f\"  message-level plugins: {plugin_messages}\")\\n\\n    if not any([conv.get(\\'gizmo_id\\'), conv.get(\\'plugin_ids\\'), gizmo_messages, plugin_messages]):\\n        print(f\"   No gizmo/plugin usage detected in this conversation\")\\n\\n# Test the gizmo/plugin detection function directly\\nprint(\"\\\\n=== TESTING GIZMO/PLUGIN DETECTION FUNCTION ===\")\\nfor i, conv in enumerate(conversations[:3]):\\n    print(f\"\\\\nConversation {i+1}:\")\\n    gizmo_plugin_tags = create_gizmo_plugin_tags(conv)\\n    if gizmo_plugin_tags:\\n        print(f\"  Generated tags: {[str(tag) for tag in gizmo_plugin_tags]}\")\\n    else:\\n        print(f\"  No gizmo/plugin tags generated\")\\n\\n# ===== ADD CUSTOM DEBUGGING RULES =====\\n\\n# Add a rule to detect any field that might contain gizmo/plugin info\\ndef debug_gizmo_plugin_fields(conversation: Dict[str, Any]) -> List[str]:\\n    \"\"\"Debug function to find any gizmo/plugin-related fields.\"\"\"\\n    found_fields = []\\n\\n    # Check top-level fields\\n    for key, value in conversation.items():\\n        if \\'gizmo\\' in key.lower() or \\'plugin\\' in key.lower():\\n            found_fields.append(f\"top_level.{key}={value}\")\\n\\n    # Check message metadata\\n    mapping = conversation.get(\\'mapping\\', {})\\n    for node_id, node in mapping.items():\\n        message = node.get(\\'message\\')\\n        if not message:\\n            continue\\n\\n        metadata = message.get(\\'metadata\\', {})\\n        for key, value in metadata.items():\\n            if \\'gizmo\\' in key.lower() or \\'plugin\\' in key.lower():\\n                found_fields.append(f\"metadata.{key}={value}\")\\n\\n    return found_fields\\n\\n# Test field detection\\nprint(\"\\\\n=== SEARCHING FOR ANY GIZMO/PLUGIN FIELDS ===\")\\nfor i, conv in enumerate(conversations[:10]):\\n    fields = debug_gizmo_plugin_fields(conv)\\n    if fields:\\n        print(f\"Conversation {i+1}: {fields}\")\\n\\n# ===== ENHANCED SUMMARY USAGE =====\\n\\n# Quick summary without details\\ntagger.print_summary(tagged_results, show_details=False)\\n\\n# Full detailed analysis\\nprint(\"\\\\n\" + \"=\"*60)\\nprint(\"FULL DETAILED ANALYSIS\")\\nprint(\"=\"*60)\\n\\ntagger.print_summary(tagged_results, show_details=True)\\ntagger.print_detailed_breakdown(tagged_results)\\ntagger.validate_tag_system(tagged_results)\\n\\n# ===== CONTINUOUS MONITORING =====\\n\\n# Function to monitor tag distribution changes when you add new rules\\ndef compare_tag_distributions(old_results, new_results):\\n    \"\"\"Compare tag distributions between two runs.\"\"\"\\n    old_counts = defaultdict(int)\\n    new_counts = defaultdict(int)\\n\\n    for result in old_results:\\n        for tag in result[\\'tags\\']:\\n            old_counts[tag.name] += 1\\n\\n    for result in new_results:\\n        for tag in result[\\'tags\\']:\\n            new_counts[tag.name] += 1\\n\\n    all_tags = set(old_counts.keys()) | set(new_counts.keys())\\n\\n    print(\"\\\\n=== TAG DISTRIBUTION CHANGES ===\")\\n    for tag in sorted(all_tags):\\n        old_count = old_counts[tag]\\n        new_count = new_counts[tag]\\n        if old_count != new_count:\\n            change = new_count - old_count\\n            print(f\"{tag}: {old_count}  {new_count} ({change:+d})\")\\n\\n# Usage when you add new rules:\\n# old_results = tagger.tag_conversations(conversations)\\n# tagger.add_base_rule(\\'new_rule\\', some_function)\\n# new_results = tagger.tag_conversations(conversations)\\n# compare_tag_distributions(old_results, new_results)\\n\\n# ===== SPECIFIC ISSUE INVESTIGATION =====\\n\\n# If you suspect certain conversations should have gizmo/plugin tags but don\\'t:\\ndef investigate_specific_conversation(conversation_id_or_title):\\n    \"\"\"Deep dive into a specific conversation.\"\"\"\\n    target_conv = None\\n    for conv in conversations:\\n        if (conv.get(\\'conversation_id\\') == conversation_id_or_title or \\n            conversation_id_or_title.lower() in conv.get(\\'title\\', \\'\\').lower()):\\n            target_conv = conv\\n            break\\n\\n    if not target_conv:\\n        print(f\"Conversation \\'{conversation_id_or_title}\\' not found\")\\n        return\\n\\n    print(f\"\\\\n=== INVESTIGATING: {target_conv.get(\\'title\\', \\'Untitled\\')} ===\")\\n\\n    # Full conversation structure\\n    print(\"\\\\nConversation structure:\")\\n    print(f\"  Title: {target_conv.get(\\'title\\')}\")\\n    print(f\"  ID: {target_conv.get(\\'conversation_id\\')}\")\\n    print(f\"  Top-level gizmo_id: {target_conv.get(\\'gizmo_id\\')}\")\\n    print(f\"  Top-level plugin_ids: {target_conv.get(\\'plugin_ids\\')}\")\\n\\n    # Message analysis\\n    mapping = target_conv.get(\\'mapping\\', {})\\n    print(f\"\\\\nMessage mapping has {len(mapping)} nodes\")\\n\\n    for node_id, node in mapping.items():\\n        message = node.get(\\'message\\')\\n        if message:\\n            metadata = message.get(\\'metadata\\', {})\\n            gizmo_fields = {k: v for k, v in metadata.items() if \\'gizmo\\' in k.lower()}\\n            plugin_fields = {k: v for k, v in metadata.items() if \\'plugin\\' in k.lower()}\\n\\n            if gizmo_fields or plugin_fields:\\n                print(f\"  Message {node_id[:8]}...\")\\n                if gizmo_fields:\\n                    print(f\"    Gizmo fields: {gizmo_fields}\")\\n                if plugin_fields:\\n                    print(f\"    Plugin fields: {plugin_fields}\")\\n\\n    # Test tagging\\n    result = tagger.tag_conversation(target_conv)\\n    print(f\"\\\\nGenerated tags:\")\\n    for tag in result[\\'tags\\']:\\n        print(f\"  {tag}\")\\n\\n    # Show debug info\\n    print(f\"\\\\nDebug info:\")\\n    for rule in result[\\'debug_info\\'].get(\\'applied_rules\\', []):\\n        print(f\"   {rule}\")\\n    for error in result[\\'debug_info\\'].get(\\'errors\\', []):\\n        print(f\"    {error}\")\\n\\n# Example usage:\\n# investigate_specific_conversation(\"some conversation title\")\\n# investigate_specific_conversation(\"conv-id-12345\")\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation_tagger.py\n",
    "\"\"\"\n",
    "Enhanced conversation tagging system with structured tags supporting key-value attributes.\n",
    "Tags can be simple strings or objects with metadata for better analysis and filtering.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Any, List, Callable, Set, Union\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "class Tag:\n",
    "    \"\"\"Represents a tag with optional key-value attributes.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, **attributes):\n",
    "        self.name = name\n",
    "        self.attributes = attributes\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.attributes:\n",
    "            attrs_str = \", \".join(f\"{k}={v}\" for k, v in self.attributes.items())\n",
    "            return f\"{self.name}({attrs_str})\"\n",
    "        return self.name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Tag('{self.name}', {self.attributes})\"\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, str):\n",
    "            return self.name == other\n",
    "        elif isinstance(other, Tag):\n",
    "            return self.name == other.name and self.attributes == other.attributes\n",
    "        return False\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.name, tuple(sorted(self.attributes.items()))))\n",
    "    \n",
    "    def matches(self, name: str, **criteria) -> bool:\n",
    "        \"\"\"Check if tag matches name and optional attribute criteria.\"\"\"\n",
    "        if self.name != name:\n",
    "            return False\n",
    "        \n",
    "        for key, value in criteria.items():\n",
    "            if key not in self.attributes:\n",
    "                return False\n",
    "            \n",
    "            attr_value = self.attributes[key]\n",
    "            \n",
    "            # Support comparison operators\n",
    "            if isinstance(value, dict):\n",
    "                for op, target in value.items():\n",
    "                    if op == 'gt' and not (attr_value > target):\n",
    "                        return False\n",
    "                    elif op == 'gte' and not (attr_value >= target):\n",
    "                        return False\n",
    "                    elif op == 'lt' and not (attr_value < target):\n",
    "                        return False\n",
    "                    elif op == 'lte' and not (attr_value <= target):\n",
    "                        return False\n",
    "                    elif op == 'eq' and not (attr_value == target):\n",
    "                        return False\n",
    "                    elif op == 'ne' and not (attr_value != target):\n",
    "                        return False\n",
    "                    elif op == 'in' and not (attr_value in target):\n",
    "                        return False\n",
    "            else:\n",
    "                # Direct equality\n",
    "                if attr_value != value:\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "\n",
    "class ConversationTagger:\n",
    "    \"\"\"\n",
    "    Enhanced tagging system supporting structured tags with attributes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_rules: Dict[str, Callable] = {}\n",
    "        self.multi_tag_rules: Dict[str, Callable] = {}\n",
    "        self.supplemental_rules: Dict[str, Callable] = {}\n",
    "        self.rule_descriptions: Dict[str, str] = {}\n",
    "    \n",
    "    def add_base_rule(self, tag_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"Add a base tagging rule that returns bool or Tag object.\"\"\"\n",
    "        self.base_rules[tag_name] = rule_function\n",
    "        self.rule_descriptions[tag_name] = description\n",
    "    \n",
    "    def add_multi_tag_rule(self, rule_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"Add a rule that returns multiple tags (strings or Tag objects).\"\"\"\n",
    "        self.multi_tag_rules[rule_name] = rule_function\n",
    "        self.rule_descriptions[rule_name] = description\n",
    "    \n",
    "    def add_supplemental_rule(self, tag_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"Add a supplemental rule that depends on existing tags.\"\"\"\n",
    "        self.supplemental_rules[tag_name] = rule_function\n",
    "        self.rule_descriptions[tag_name] = description\n",
    "    \n",
    "    def _normalize_tag(self, tag: Union[str, Tag]) -> Tag:\n",
    "        \"\"\"Convert string tags to Tag objects.\"\"\"\n",
    "        if isinstance(tag, str):\n",
    "            return Tag(tag)\n",
    "        return tag\n",
    "    \n",
    "    def tag_conversation(self, conversation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Apply all tagging rules to a conversation.\"\"\"\n",
    "        tags = set()\n",
    "        debug_info = defaultdict(list)\n",
    "        \n",
    "        # Apply base rules\n",
    "        for tag_name, rule_func in self.base_rules.items():\n",
    "            try:\n",
    "                result = rule_func(conversation)\n",
    "                if result:\n",
    "                    if isinstance(result, bool):\n",
    "                        tag = Tag(tag_name)\n",
    "                    else:\n",
    "                        tag = self._normalize_tag(result)\n",
    "                    tags.add(tag)\n",
    "                    debug_info['applied_rules'].append(f\"BASE: {tag}\")\n",
    "                else:\n",
    "                    debug_info['skipped_rules'].append(f\"BASE: {tag_name}\")\n",
    "            except Exception as e:\n",
    "                debug_info['errors'].append(f\"BASE: {tag_name} - {str(e)}\")\n",
    "        \n",
    "        # Apply multi-tag rules\n",
    "        for rule_name, rule_func in self.multi_tag_rules.items():\n",
    "            try:\n",
    "                new_tags = rule_func(conversation)\n",
    "                if new_tags:\n",
    "                    normalized_tags = [self._normalize_tag(tag) for tag in new_tags]\n",
    "                    tags.update(normalized_tags)\n",
    "                    debug_info['applied_rules'].append(f\"MULTI: {rule_name} -> {[str(t) for t in normalized_tags]}\")\n",
    "                else:\n",
    "                    debug_info['skipped_rules'].append(f\"MULTI: {rule_name}\")\n",
    "            except Exception as e:\n",
    "                debug_info['errors'].append(f\"MULTI: {rule_name} - {str(e)}\")\n",
    "        \n",
    "        # Apply supplemental rules\n",
    "        max_iterations = 5\n",
    "        for iteration in range(max_iterations):\n",
    "            initial_tag_count = len(tags)\n",
    "            \n",
    "            for tag_name, rule_func in self.supplemental_rules.items():\n",
    "                # Check if tag already exists\n",
    "                if any(tag.name == tag_name for tag in tags):\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    result = rule_func(conversation, tags)\n",
    "                    if result:\n",
    "                        if isinstance(result, bool):\n",
    "                            tag = Tag(tag_name)\n",
    "                        else:\n",
    "                            tag = self._normalize_tag(result)\n",
    "                        tags.add(tag)\n",
    "                        debug_info['applied_rules'].append(f\"SUPP: {tag} (iter {iteration})\")\n",
    "                    else:\n",
    "                        debug_info['skipped_rules'].append(f\"SUPP: {tag_name} (iter {iteration})\")\n",
    "                except Exception as e:\n",
    "                    debug_info['errors'].append(f\"SUPP: {tag_name} - {str(e)}\")\n",
    "            \n",
    "            if len(tags) == initial_tag_count:\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'conversation_id': conversation.get('conversation_id', conversation.get('id', 'unknown')),\n",
    "            'title': conversation.get('title', 'Untitled'),\n",
    "            'tags': list(tags),\n",
    "            'debug_info': dict(debug_info),\n",
    "            'conversation': conversation\n",
    "        }\n",
    "    \n",
    "    def tag_conversations(self, conversations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Tag multiple conversations.\"\"\"\n",
    "        return [self.tag_conversation(conv) for conv in conversations]\n",
    "    \n",
    "    def filter_by_tags(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                      include_tags: List[Union[str, Dict]] = None,\n",
    "                      exclude_tags: List[Union[str, Dict]] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Filter conversations by tags with attribute support.\n",
    "        \n",
    "        Args:\n",
    "            include_tags: List of tag names or dicts with criteria\n",
    "                Examples: ['web_search', {'name': 'gizmo', 'type': 'dalle'}]\n",
    "            exclude_tags: Similar format for exclusions\n",
    "        \"\"\"\n",
    "        filtered = []\n",
    "        \n",
    "        for tagged_conv in tagged_conversations:\n",
    "            tags = tagged_conv['tags']\n",
    "            \n",
    "            # Check exclusions first\n",
    "            if exclude_tags:\n",
    "                should_exclude = False\n",
    "                for exclude_criterion in exclude_tags:\n",
    "                    if self._matches_criterion(tags, exclude_criterion):\n",
    "                        should_exclude = True\n",
    "                        break\n",
    "                if should_exclude:\n",
    "                    continue\n",
    "            \n",
    "            # Check inclusions\n",
    "            if include_tags:\n",
    "                should_include = True\n",
    "                for include_criterion in include_tags:\n",
    "                    if not self._matches_criterion(tags, include_criterion):\n",
    "                        should_include = False\n",
    "                        break\n",
    "                if not should_include:\n",
    "                    continue\n",
    "            \n",
    "            filtered.append(tagged_conv)\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def _matches_criterion(self, tags: List[Tag], criterion: Union[str, Dict]) -> bool:\n",
    "        \"\"\"Check if any tag matches the given criterion.\"\"\"\n",
    "        if isinstance(criterion, str):\n",
    "            return any(tag.name == criterion for tag in tags)\n",
    "        \n",
    "        elif isinstance(criterion, dict):\n",
    "            name = criterion.get('name')\n",
    "            if not name:\n",
    "                return False\n",
    "            \n",
    "            criteria = {k: v for k, v in criterion.items() if k != 'name'}\n",
    "            return any(tag.matches(name, **criteria) for tag in tags)\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_tag_values(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                      tag_name: str, attribute: str) -> List[Any]:\n",
    "        \"\"\"Extract attribute values from tags across conversations.\"\"\"\n",
    "        values = []\n",
    "        for tagged_conv in tagged_conversations:\n",
    "            for tag in tagged_conv['tags']:\n",
    "                if tag.name == tag_name and attribute in tag.attributes:\n",
    "                    values.append(tag.attributes[attribute])\n",
    "        return values\n",
    "    \n",
    "    def print_summary(self, tagged_conversations: List[Dict[str, Any]], show_details: bool = True):\n",
    "        \"\"\"Print comprehensive summary with all tag types and optional details.\"\"\"\n",
    "        total = len(tagged_conversations)\n",
    "        tag_counts = defaultdict(int)\n",
    "        tag_attributes = defaultdict(lambda: defaultdict(list))\n",
    "        unique_structured_tags = defaultdict(set)\n",
    "        \n",
    "        # Collect all tag information\n",
    "        for tagged_conv in tagged_conversations:\n",
    "            for tag in tagged_conv['tags']:\n",
    "                tag_counts[tag.name] += 1\n",
    "                \n",
    "                # Collect attribute information\n",
    "                for attr_name, attr_value in tag.attributes.items():\n",
    "                    if isinstance(attr_value, (int, float)):\n",
    "                        tag_attributes[tag.name][attr_name].append(attr_value)\n",
    "                    else:\n",
    "                        # For non-numeric attributes, track unique values\n",
    "                        unique_structured_tags[tag.name].add(f\"{attr_name}={attr_value}\")\n",
    "        \n",
    "        print(f\"Tagged {total} conversations\")\n",
    "        print(f\"\\n=== TAG SUMMARY ===\")\n",
    "        \n",
    "        # Sort tags by frequency for better readability\n",
    "        sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for tag_name, count in sorted_tags:\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"{tag_name}: {count} ({percentage:.1f}%)\")\n",
    "            \n",
    "            if show_details:\n",
    "                # Show numeric attribute statistics\n",
    "                if tag_name in tag_attributes:\n",
    "                    for attr_name, values in tag_attributes[tag_name].items():\n",
    "                        if values:\n",
    "                            avg_val = sum(values) / len(values)\n",
    "                            min_val = min(values)\n",
    "                            max_val = max(values)\n",
    "                            print(f\"    {attr_name}: avg={avg_val:.1f}, range=[{min_val}, {max_val}]\")\n",
    "                \n",
    "                # Show unique structured values for non-numeric attributes\n",
    "                if tag_name in unique_structured_tags:\n",
    "                    unique_vals = sorted(unique_structured_tags[tag_name])\n",
    "                    if len(unique_vals) <= 10:  # Show all if not too many\n",
    "                        print(f\"    values: {', '.join(unique_vals)}\")\n",
    "                    else:  # Show top 10 most common\n",
    "                        print(f\"    values: {', '.join(unique_vals[:10])} ... (+{len(unique_vals)-10} more)\")\n",
    "    \n",
    "    def print_detailed_breakdown(self, tagged_conversations: List[Dict[str, Any]]):\n",
    "        \"\"\"Print detailed breakdown of specific tag types.\"\"\"\n",
    "        total = len(tagged_conversations)\n",
    "        \n",
    "        print(f\"\\n=== DETAILED BREAKDOWN ===\")\n",
    "        \n",
    "        # Gizmo usage breakdown\n",
    "        gizmo_counts = defaultdict(int)\n",
    "        plugin_counts = defaultdict(int)\n",
    "        \n",
    "        for tagged_conv in tagged_conversations:\n",
    "            for tag in tagged_conv['tags']:\n",
    "                if tag.name == 'gizmo':\n",
    "                    gizmo_id = tag.attributes.get('gizmo_id', 'unknown')\n",
    "                    gizmo_counts[gizmo_id] += 1\n",
    "                elif tag.name == 'plugin':\n",
    "                    plugin_id = tag.attributes.get('plugin_id', 'unknown')\n",
    "                    plugin_counts[plugin_id] += 1\n",
    "        \n",
    "        if gizmo_counts:\n",
    "            print(\"\\nGizmo Usage:\")\n",
    "            for gizmo_id, count in sorted(gizmo_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "                percentage = (count / total) * 100\n",
    "                print(f\"  {gizmo_id}: {count} ({percentage:.1f}%)\")\n",
    "        else:\n",
    "            print(\"\\nGizmo Usage: None detected\")\n",
    "        \n",
    "        if plugin_counts:\n",
    "            print(\"\\nPlugin Usage:\")\n",
    "            for plugin_id, count in sorted(plugin_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "                percentage = (count / total) * 100\n",
    "                print(f\"  {plugin_id}: {count} ({percentage:.1f}%)\")\n",
    "        else:\n",
    "            print(\"\\nPlugin Usage: None detected\")\n",
    "        \n",
    "        # Conversation length distribution\n",
    "        length_distribution = defaultdict(int)\n",
    "        prompt_length_distribution = defaultdict(int)\n",
    "        \n",
    "        for tagged_conv in tagged_conversations:\n",
    "            for tag in tagged_conv['tags']:\n",
    "                if tag.name == 'conversation_length':\n",
    "                    category = tag.attributes.get('category', 'unknown')\n",
    "                    length_distribution[category] += 1\n",
    "                elif tag.name == 'prompt_stats':\n",
    "                    length_cat = tag.attributes.get('length_category', 'unknown')\n",
    "                    prompt_length_distribution[length_cat] += 1\n",
    "        \n",
    "        if length_distribution:\n",
    "            print(\"\\nConversation Length Distribution:\")\n",
    "            for category, count in sorted(length_distribution.items(), key=lambda x: x[1], reverse=True):\n",
    "                percentage = (count / total) * 100\n",
    "                print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if prompt_length_distribution:\n",
    "            print(\"\\nPrompt Length Distribution:\")\n",
    "            for category, count in sorted(prompt_length_distribution.items(), key=lambda x: x[1], reverse=True):\n",
    "                percentage = (count / total) * 100\n",
    "                print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    def debug_tag_creation(self, conversations: List[Dict[str, Any]], max_conversations: int = 5):\n",
    "        \"\"\"Debug tag creation process for the first few conversations.\"\"\"\n",
    "        print(f\"\\n=== TAG CREATION DEBUG (first {max_conversations} conversations) ===\")\n",
    "        \n",
    "        for i, conv in enumerate(conversations[:max_conversations]):\n",
    "            print(f\"\\n--- Conversation {i+1}: {conv.get('title', 'Untitled')[:50]}... ---\")\n",
    "            result = self.tag_conversation(conv)\n",
    "            \n",
    "            print(\"Tags created:\")\n",
    "            for tag in result['tags']:\n",
    "                if tag.attributes:\n",
    "                    attrs = ', '.join(f\"{k}={v}\" for k, v in tag.attributes.items())\n",
    "                    print(f\"  {tag.name}({attrs})\")\n",
    "                else:\n",
    "                    print(f\"  {tag.name}\")\n",
    "            \n",
    "            # Show applied rules\n",
    "            applied_rules = result['debug_info'].get('applied_rules', [])\n",
    "            if applied_rules:\n",
    "                print(\"Applied rules:\")\n",
    "                for rule in applied_rules:\n",
    "                    print(f\"   {rule}\")\n",
    "            \n",
    "            # Show any errors\n",
    "            errors = result['debug_info'].get('errors', [])\n",
    "            if errors:\n",
    "                print(\"Errors:\")\n",
    "                for error in errors:\n",
    "                    print(f\"    {error}\")\n",
    "    \n",
    "    def validate_tag_system(self, tagged_conversations: List[Dict[str, Any]]):\n",
    "        \"\"\"Validate that the tag system is working correctly.\"\"\"\n",
    "        print(f\"\\n=== TAG SYSTEM VALIDATION ===\")\n",
    "        \n",
    "        # Check for conversations with no tags\n",
    "        no_tags = [conv for conv in tagged_conversations if not conv['tags']]\n",
    "        print(f\"Conversations with no tags: {len(no_tags)}\")\n",
    "        \n",
    "        # Check for missing expected structured tags\n",
    "        missing_length = [conv for conv in tagged_conversations \n",
    "                         if not any(tag.name == 'conversation_length' for tag in conv['tags'])]\n",
    "        print(f\"Conversations missing conversation_length tag: {len(missing_length)}\")\n",
    "        \n",
    "        missing_stats = [conv for conv in tagged_conversations \n",
    "                        if not any(tag.name == 'prompt_stats' for tag in conv['tags'])]\n",
    "        print(f\"Conversations missing prompt_stats tag: {len(missing_stats)}\")\n",
    "        \n",
    "        # Count rule types that fired\n",
    "        rule_types = defaultdict(int)\n",
    "        for conv in tagged_conversations:\n",
    "            for rule_info in conv['debug_info'].get('applied_rules', []):\n",
    "                rule_type = rule_info.split(':')[0]\n",
    "                rule_types[rule_type] += 1\n",
    "        \n",
    "        print(f\"\\nRule type activity:\")\n",
    "        for rule_type, count in sorted(rule_types.items()):\n",
    "            print(f\"  {rule_type}: {count} instances\")\n",
    "        \n",
    "        # Check for gizmo/plugin detection specifically\n",
    "        gizmo_plugin_conversations = []\n",
    "        for conv in tagged_conversations:\n",
    "            has_gizmo_plugin = any(tag.name in ['gizmo', 'plugin'] for tag in conv['tags'])\n",
    "            if has_gizmo_plugin:\n",
    "                gizmo_plugin_conversations.append(conv)\n",
    "        \n",
    "        print(f\"\\nConversations with gizmo/plugin tags: {len(gizmo_plugin_conversations)}\")\n",
    "        \n",
    "        if len(gizmo_plugin_conversations) == 0:\n",
    "            print(\"  No gizmo/plugin usage detected. This might indicate:\")\n",
    "            print(\"   - No conversations actually used gizmos/plugins\")\n",
    "            print(\"   - The gizmo/plugin detection logic needs debugging\")\n",
    "            print(\"   - Check raw conversation data for gizmo_id/plugin_ids fields\")\n",
    "    \n",
    "    def debug_conversation(self, conversation: Dict[str, Any]):\n",
    "        \"\"\"Debug a single conversation with enhanced tag details.\"\"\"\n",
    "        result = self.tag_conversation(conversation)\n",
    "        \n",
    "        print(f\"Conversation: {result['title'][:50]}...\")\n",
    "        print(f\"Tags applied:\")\n",
    "        for tag in result['tags']:\n",
    "            print(f\"  {tag}\")\n",
    "        \n",
    "        print(f\"\\nRule details:\")\n",
    "        for rule in result['debug_info'].get('applied_rules', []):\n",
    "            print(f\"   {rule}\")\n",
    "        \n",
    "        for error in result['debug_info'].get('errors', []):\n",
    "            print(f\"    ERROR: {error}\")\n",
    "\n",
    "\n",
    "# Enhanced rule functions for all previously discussed tagging rules\n",
    "\n",
    "def get_all_user_messages(conversation: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get all user messages in chronological order.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    user_messages = []\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        author = message.get('author', {})\n",
    "        if author.get('role') == 'user':\n",
    "            create_time = message.get('create_time') or 0\n",
    "            user_messages.append((create_time, message))\n",
    "    \n",
    "    user_messages.sort(key=lambda x: x[0])\n",
    "    return [msg for _, msg in user_messages]\n",
    "\n",
    "\n",
    "def get_first_user_message(conversation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Find the first user message in the conversation.\"\"\"\n",
    "    user_messages = get_all_user_messages(conversation)\n",
    "    return user_messages[0] if user_messages else None\n",
    "\n",
    "\n",
    "def create_conversation_length_tag(conversation: Dict[str, Any]) -> Tag:\n",
    "    \"\"\"Create structured tag for conversation length.\"\"\"\n",
    "    user_count = len(get_all_user_messages(conversation))\n",
    "    \n",
    "    # Determine category\n",
    "    if user_count == 1:\n",
    "        category = 'single'\n",
    "    elif user_count <= 3:\n",
    "        category = 'short'\n",
    "    elif user_count <= 10:\n",
    "        category = 'medium'\n",
    "    elif user_count <= 25:\n",
    "        category = 'long'\n",
    "    else:\n",
    "        category = 'very_long'\n",
    "    \n",
    "    return Tag('conversation_length', count=user_count, category=category)\n",
    "\n",
    "\n",
    "def create_prompt_stats_tag(conversation: Dict[str, Any]) -> Tag:\n",
    "    \"\"\"Create structured tag for prompt statistics.\"\"\"\n",
    "    user_messages = get_all_user_messages(conversation)\n",
    "    \n",
    "    if not user_messages:\n",
    "        return Tag('prompt_stats', count=0, mean=0, median=0, variance=0, \n",
    "                  length_category='none', consistency='none')\n",
    "    \n",
    "    # Calculate message lengths\n",
    "    lengths = []\n",
    "    for message in user_messages:\n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        lengths.append(len(all_text))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_length = sum(lengths) / len(lengths)\n",
    "    sorted_lengths = sorted(lengths)\n",
    "    n = len(sorted_lengths)\n",
    "    median_length = (sorted_lengths[n//2] if n % 2 == 1 \n",
    "                    else (sorted_lengths[n//2-1] + sorted_lengths[n//2]) / 2)\n",
    "    variance = sum((x - mean_length) ** 2 for x in lengths) / len(lengths) if len(lengths) > 1 else 0\n",
    "    \n",
    "    # Determine categories\n",
    "    if mean_length < 50:\n",
    "        length_category = 'very_short'\n",
    "    elif mean_length < 200:\n",
    "        length_category = 'short'\n",
    "    elif mean_length < 1000:\n",
    "        length_category = 'medium'\n",
    "    elif mean_length < 3000:\n",
    "        length_category = 'long'\n",
    "    else:\n",
    "        length_category = 'very_long'\n",
    "    \n",
    "    if variance < 1000:\n",
    "        consistency = 'consistent'\n",
    "    elif variance < 10000:\n",
    "        consistency = 'mixed'\n",
    "    else:\n",
    "        consistency = 'variable'\n",
    "    \n",
    "    return Tag('prompt_stats', \n",
    "               count=len(lengths),\n",
    "               mean=round(mean_length, 1),\n",
    "               median=round(median_length, 1),\n",
    "               variance=round(variance, 1),\n",
    "               length_category=length_category,\n",
    "               consistency=consistency)\n",
    "\n",
    "\n",
    "def create_gizmo_plugin_tags(conversation: Dict[str, Any]) -> List[Tag]:\n",
    "    \"\"\"Create structured tags for gizmos and plugins.\"\"\"\n",
    "    tags = []\n",
    "    gizmos = set()\n",
    "    plugins = set()\n",
    "    \n",
    "    # Check conversation-level\n",
    "    if conversation.get('gizmo_id'):\n",
    "        gizmos.add(conversation['gizmo_id'])\n",
    "    \n",
    "    plugin_ids = conversation.get('plugin_ids', [])\n",
    "    if plugin_ids:\n",
    "        plugins.update(plugin_ids)\n",
    "    \n",
    "    # Check message-level\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        \n",
    "        # Invoked plugins\n",
    "        invoked_plugin = metadata.get('invoked_plugin', {})\n",
    "        if invoked_plugin:\n",
    "            if invoked_plugin.get('plugin_id'):\n",
    "                plugins.add(invoked_plugin['plugin_id'])\n",
    "            if invoked_plugin.get('namespace'):\n",
    "                plugins.add(invoked_plugin['namespace'])\n",
    "        \n",
    "        # Gizmo usage\n",
    "        if metadata.get('gizmo_id'):\n",
    "            gizmos.add(metadata['gizmo_id'])\n",
    "    \n",
    "    # Create tags - FIX: Use different attribute name to avoid conflict\n",
    "    for gizmo in gizmos:\n",
    "        tags.append(Tag('gizmo', gizmo_id=gizmo))\n",
    "    \n",
    "    for plugin in plugins:\n",
    "        tags.append(Tag('plugin', plugin_id=plugin))\n",
    "    \n",
    "    return tags\n",
    "\n",
    "\n",
    "# Boolean rule functions for basic content analysis\n",
    "def has_large_content(conversation: Dict[str, Any], min_length: int = 2000) -> bool:\n",
    "    \"\"\"Check if conversation has unusually large content anywhere.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        if len(text) > min_length:\n",
    "            return True\n",
    "            \n",
    "        parts = content.get('parts', [])\n",
    "        for part in parts:\n",
    "            if isinstance(part, str) and len(part) > min_length:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_code_patterns(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for clear code patterns anywhere in conversation.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "        \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        \n",
    "        # Strong code indicators\n",
    "        code_indicators = [\n",
    "            '```',  # Code blocks\n",
    "            'def ', 'function ', 'class ',  # Function/class definitions\n",
    "            'import ', 'from ', 'require(',  # Import statements\n",
    "            '#!/bin/', '#include', 'using namespace',  # Script headers\n",
    "        ]\n",
    "        \n",
    "        if any(indicator in all_text for indicator in code_indicators):\n",
    "            return True\n",
    "            \n",
    "        # Also check for high density of coding keywords\n",
    "        coding_keywords = ['function', 'class', 'import', 'def ', 'const ', 'let ', 'var ', 'return', 'if ', 'for ', 'while ']\n",
    "        keyword_count = sum(1 for keyword in coding_keywords if keyword in all_text.lower())\n",
    "        if len(all_text) > 1000 and keyword_count >= 3:  # Multiple coding keywords in large text suggest actual code\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_github_repos(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if GitHub repositories were selected for context.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        repos = metadata.get('selected_github_repos', [])\n",
    "        if repos:  # Non-empty list\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_canvas_operations(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for canvas/document operations.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if metadata.get('canvas'):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_web_search(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for web search operations.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if (metadata.get('search_queries') or \n",
    "            metadata.get('search_result_groups') or\n",
    "            metadata.get('content_references')):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_reasoning_thoughts(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for reasoning/thinking patterns.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        content = message.get('content', {})\n",
    "        if content.get('thoughts'):  # Reasoning thoughts\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_code_execution(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for code execution artifacts.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if (metadata.get('aggregate_result') or \n",
    "            metadata.get('jupyter_messages')):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "# First user message specific rules\n",
    "def first_user_has_large_content(conversation: Dict[str, Any], min_length: int = 2000) -> bool:\n",
    "    \"\"\"Check if the first user message has large content.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    content = first_message.get('content', {})\n",
    "    text = content.get('text', '')\n",
    "    if len(text) > min_length:\n",
    "        return True\n",
    "        \n",
    "    parts = content.get('parts', [])\n",
    "    for part in parts:\n",
    "        if isinstance(part, str) and len(part) > min_length:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def first_user_has_code_patterns(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message contains code patterns.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    content = first_message.get('content', {})\n",
    "    text = content.get('text', '')\n",
    "    parts = content.get('parts', [])\n",
    "    all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "    \n",
    "    # Strong code indicators\n",
    "    code_indicators = [\n",
    "        '```',  # Code blocks\n",
    "        'def ', 'function ', 'class ',  # Definitions\n",
    "        'import ', 'from ', 'require(',  # Imports\n",
    "        '#!/bin/', '#include',  # Script headers\n",
    "    ]\n",
    "    \n",
    "    return any(indicator in all_text for indicator in code_indicators)\n",
    "\n",
    "\n",
    "def first_user_has_attachments(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message has attachments.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    metadata = first_message.get('metadata', {})\n",
    "    attachments = metadata.get('attachments', [])\n",
    "    return len(attachments) > 0\n",
    "\n",
    "\n",
    "def first_user_has_code_attachments(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message has code-related attachments.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    metadata = first_message.get('metadata', {})\n",
    "    attachments = metadata.get('attachments', [])\n",
    "    \n",
    "    for attachment in attachments:\n",
    "        mime_type = attachment.get('mime_type', '').lower()\n",
    "        name = attachment.get('name', '').lower()\n",
    "        \n",
    "        # Check for code file extensions\n",
    "        code_extensions = ['.py', '.js', '.java', '.cpp', '.c', '.go', '.rs', '.ts', '.jsx', '.tsx', '.sql', '.sh', '.rb', '.php']\n",
    "        if any(ext in name for ext in code_extensions):\n",
    "            return True\n",
    "            \n",
    "        # Check for code-related MIME types\n",
    "        code_mimes = ['text/x-python', 'text/x-java', 'application/javascript', 'text/x-script']\n",
    "        if any(mime in mime_type for mime in code_mimes):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def create_default_tagger() -> ConversationTagger:\n",
    "    \"\"\"Create a tagger with all previously discussed rules in the enhanced structured framework.\"\"\"\n",
    "    tagger = ConversationTagger()\n",
    "    \n",
    "    # ===== BASIC CONTENT ANALYSIS RULES (Boolean) =====\n",
    "    tagger.add_base_rule(\n",
    "        'large_content', \n",
    "        lambda conv: has_large_content(conv, 2000),\n",
    "        'Content longer than 2000 characters anywhere in conversation'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'code_patterns', \n",
    "        has_code_patterns,\n",
    "        'Contains clear code patterns (```, def, function, etc.)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'github_context',\n",
    "        has_github_repos,\n",
    "        'GitHub repositories selected for context'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'canvas_operations',\n",
    "        has_canvas_operations,\n",
    "        'Uses canvas/document features'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'web_search',\n",
    "        has_web_search,\n",
    "        'Includes web search functionality'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'reasoning',\n",
    "        has_reasoning_thoughts,\n",
    "        'Contains reasoning/thinking content'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'code_execution',\n",
    "        has_code_execution,\n",
    "        'Contains code execution (Jupyter, aggregate results)'\n",
    "    )\n",
    "    \n",
    "    # ===== FIRST USER MESSAGE RULES (Boolean) =====\n",
    "    tagger.add_base_rule(\n",
    "        'starts_large_content',\n",
    "        lambda conv: first_user_has_large_content(conv, 2000),\n",
    "        'First user message has large content (>2000 chars)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_code_patterns',\n",
    "        first_user_has_code_patterns,\n",
    "        'First user message contains code patterns'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_with_attachments',\n",
    "        first_user_has_attachments,\n",
    "        'First user message has any attachments'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_code_attachments',\n",
    "        first_user_has_code_attachments,\n",
    "        'First user message has code-related attachments'\n",
    "    )\n",
    "    \n",
    "    # ===== STRUCTURED TAG RULES =====\n",
    "    tagger.add_base_rule(\n",
    "        'conversation_length',\n",
    "        create_conversation_length_tag,\n",
    "        'Conversation length with count and category'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'prompt_stats',\n",
    "        create_prompt_stats_tag,\n",
    "        'User message statistics (length, variance, etc.)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_multi_tag_rule(\n",
    "        'gizmo_plugin_usage',\n",
    "        create_gizmo_plugin_tags,\n",
    "        'Specific gizmos and plugins used in conversation'\n",
    "    )\n",
    "    \n",
    "    # ===== SUPPLEMENTAL RULES (Based on existing tags) =====\n",
    "    \n",
    "    # Coding assistance detection\n",
    "    tagger.add_supplemental_rule(\n",
    "        'coding_assistance',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['code_patterns', 'github_context', 'code_execution'] for tag in tags)\n",
    "        ),\n",
    "        'Likely coding assistance (code patterns, GitHub, or execution)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'coding_assistance_start',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['starts_code_patterns', 'starts_large_content', \n",
    "                           'starts_with_attachments', 'starts_code_attachments'] for tag in tags)\n",
    "        ),\n",
    "        'Likely coding assistance based on how conversation starts'\n",
    "    )\n",
    "    \n",
    "    # Research and analysis patterns  \n",
    "    tagger.add_supplemental_rule(\n",
    "        'research_session',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'web_search' for tag in tags) and \n",
    "            any(tag.name == 'large_content' for tag in tags)\n",
    "        ),\n",
    "        'Research session (web search + substantial content)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'complex_analysis',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'reasoning' for tag in tags) and \n",
    "            len([tag for tag in tags if tag.name in ['web_search', 'large_content', 'canvas_operations']]) >= 2\n",
    "        ),\n",
    "        'Complex analysis (reasoning + multiple advanced features)'\n",
    "    )\n",
    "    \n",
    "    # Context and interaction patterns\n",
    "    tagger.add_supplemental_rule(\n",
    "        'context_heavy_start',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['starts_large_content', 'starts_with_attachments'] for tag in tags)\n",
    "        ),\n",
    "        'Conversation starts with substantial context'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'enhanced_conversation',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['gizmo', 'plugin'] for tag in tags)\n",
    "        ),\n",
    "        'Uses enhanced features (gizmos or plugins)'\n",
    "    )\n",
    "    \n",
    "    # Length-based classifications using structured tags\n",
    "    tagger.add_supplemental_rule(\n",
    "        'brief_interaction',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('category') in ['single', 'short'] for tag in tags)\n",
    "        ),\n",
    "        'Brief interaction (1-3 user messages)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'extended_conversation',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('category') in ['long', 'very_long'] for tag in tags)\n",
    "        ),\n",
    "        'Extended conversation (11+ user messages)'\n",
    "    )\n",
    "    \n",
    "    # Prompt pattern classifications using structured tags\n",
    "    tagger.add_supplemental_rule(\n",
    "        'long_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('length_category') in ['long', 'very_long'] for tag in tags)\n",
    "        ),\n",
    "        'Conversation has consistently long prompts'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'short_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('length_category') in ['very_short', 'short'] for tag in tags)\n",
    "        ),\n",
    "        'Conversation has consistently short prompts'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'consistent_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'consistent' for tag in tags)\n",
    "        ),\n",
    "        'User prompts are consistent in length'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'variable_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'variable' for tag in tags)\n",
    "        ),\n",
    "        'User prompts vary significantly in length'\n",
    "    )\n",
    "    \n",
    "    # Combined patterns for specific use cases\n",
    "    tagger.add_supplemental_rule(\n",
    "        'context_dump',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('category') in ['single', 'short'] for tag in tags) and\n",
    "            any(tag.name in ['starts_large_content', 'starts_with_attachments'] for tag in tags)\n",
    "        ),\n",
    "        'Short conversation starting with large context (likely context dump)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'interactive_session',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('count', 0) >= 5 for tag in tags) and\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'consistent' for tag in tags)\n",
    "        ),\n",
    "        'Extended back-and-forth conversation with consistent prompt style'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'evolving_discussion',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('count', 0) >= 5 for tag in tags) and\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'variable' for tag in tags)\n",
    "        ),\n",
    "        'Extended conversation where prompt style evolves'\n",
    "    )\n",
    "    \n",
    "    return tagger\n",
    "\n",
    "\n",
    "# Enhanced debugging and analysis examples\n",
    "\"\"\"\n",
    "# Create enhanced tagger with all rules\n",
    "tagger = create_default_tagger()\n",
    "tagged_results = tagger.tag_conversations(conversations)\n",
    "\n",
    "# ===== COMPREHENSIVE SUMMARY WITH DEBUGGING =====\n",
    "\n",
    "# Basic summary (shows all tags with counts and percentages)\n",
    "tagger.print_summary(tagged_results)\n",
    "\n",
    "# Detailed breakdown (shows gizmo/plugin specifics, distributions)\n",
    "tagger.print_detailed_breakdown(tagged_results)\n",
    "\n",
    "# Debug tag creation for first few conversations\n",
    "tagger.debug_tag_creation(conversations, max_conversations=3)\n",
    "\n",
    "# Validate the tag system is working correctly\n",
    "tagger.validate_tag_system(tagged_results)\n",
    "\n",
    "# ===== INVESTIGATE MISSING GIZMO/PLUGIN TAGS =====\n",
    "\n",
    "# If no gizmo/plugin tags show up, let's debug why\n",
    "print(\"\\\\n=== INVESTIGATING GIZMO/PLUGIN DETECTION ===\")\n",
    "\n",
    "# Check raw data for gizmo/plugin fields in first few conversations\n",
    "for i, conv in enumerate(conversations[:5]):\n",
    "    print(f\"\\\\nConversation {i+1}: {conv.get('title', 'Untitled')[:50]}...\")\n",
    "    \n",
    "    # Check conversation-level gizmo/plugin fields\n",
    "    print(f\"  gizmo_id: {conv.get('gizmo_id')}\")\n",
    "    print(f\"  plugin_ids: {conv.get('plugin_ids')}\")\n",
    "    \n",
    "    # Check message-level fields\n",
    "    mapping = conv.get('mapping', {})\n",
    "    gizmo_messages = []\n",
    "    plugin_messages = []\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if metadata.get('gizmo_id'):\n",
    "            gizmo_messages.append(metadata['gizmo_id'])\n",
    "        if metadata.get('invoked_plugin'):\n",
    "            plugin_messages.append(metadata['invoked_plugin'])\n",
    "    \n",
    "    if gizmo_messages:\n",
    "        print(f\"  message-level gizmos: {gizmo_messages}\")\n",
    "    if plugin_messages:\n",
    "        print(f\"  message-level plugins: {plugin_messages}\")\n",
    "    \n",
    "    if not any([conv.get('gizmo_id'), conv.get('plugin_ids'), gizmo_messages, plugin_messages]):\n",
    "        print(f\"   No gizmo/plugin usage detected in this conversation\")\n",
    "\n",
    "# Test the gizmo/plugin detection function directly\n",
    "print(\"\\\\n=== TESTING GIZMO/PLUGIN DETECTION FUNCTION ===\")\n",
    "for i, conv in enumerate(conversations[:3]):\n",
    "    print(f\"\\\\nConversation {i+1}:\")\n",
    "    gizmo_plugin_tags = create_gizmo_plugin_tags(conv)\n",
    "    if gizmo_plugin_tags:\n",
    "        print(f\"  Generated tags: {[str(tag) for tag in gizmo_plugin_tags]}\")\n",
    "    else:\n",
    "        print(f\"  No gizmo/plugin tags generated\")\n",
    "\n",
    "# ===== ADD CUSTOM DEBUGGING RULES =====\n",
    "\n",
    "# Add a rule to detect any field that might contain gizmo/plugin info\n",
    "def debug_gizmo_plugin_fields(conversation: Dict[str, Any]) -> List[str]:\n",
    "    \\\"\\\"\\\"Debug function to find any gizmo/plugin-related fields.\\\"\\\"\\\"\n",
    "    found_fields = []\n",
    "    \n",
    "    # Check top-level fields\n",
    "    for key, value in conversation.items():\n",
    "        if 'gizmo' in key.lower() or 'plugin' in key.lower():\n",
    "            found_fields.append(f\"top_level.{key}={value}\")\n",
    "    \n",
    "    # Check message metadata\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        for key, value in metadata.items():\n",
    "            if 'gizmo' in key.lower() or 'plugin' in key.lower():\n",
    "                found_fields.append(f\"metadata.{key}={value}\")\n",
    "    \n",
    "    return found_fields\n",
    "\n",
    "# Test field detection\n",
    "print(\"\\\\n=== SEARCHING FOR ANY GIZMO/PLUGIN FIELDS ===\")\n",
    "for i, conv in enumerate(conversations[:10]):\n",
    "    fields = debug_gizmo_plugin_fields(conv)\n",
    "    if fields:\n",
    "        print(f\"Conversation {i+1}: {fields}\")\n",
    "\n",
    "# ===== ENHANCED SUMMARY USAGE =====\n",
    "\n",
    "# Quick summary without details\n",
    "tagger.print_summary(tagged_results, show_details=False)\n",
    "\n",
    "# Full detailed analysis\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"FULL DETAILED ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tagger.print_summary(tagged_results, show_details=True)\n",
    "tagger.print_detailed_breakdown(tagged_results)\n",
    "tagger.validate_tag_system(tagged_results)\n",
    "\n",
    "# ===== CONTINUOUS MONITORING =====\n",
    "\n",
    "# Function to monitor tag distribution changes when you add new rules\n",
    "def compare_tag_distributions(old_results, new_results):\n",
    "    \\\"\\\"\\\"Compare tag distributions between two runs.\\\"\\\"\\\"\n",
    "    old_counts = defaultdict(int)\n",
    "    new_counts = defaultdict(int)\n",
    "    \n",
    "    for result in old_results:\n",
    "        for tag in result['tags']:\n",
    "            old_counts[tag.name] += 1\n",
    "    \n",
    "    for result in new_results:\n",
    "        for tag in result['tags']:\n",
    "            new_counts[tag.name] += 1\n",
    "    \n",
    "    all_tags = set(old_counts.keys()) | set(new_counts.keys())\n",
    "    \n",
    "    print(\"\\\\n=== TAG DISTRIBUTION CHANGES ===\")\n",
    "    for tag in sorted(all_tags):\n",
    "        old_count = old_counts[tag]\n",
    "        new_count = new_counts[tag]\n",
    "        if old_count != new_count:\n",
    "            change = new_count - old_count\n",
    "            print(f\"{tag}: {old_count}  {new_count} ({change:+d})\")\n",
    "\n",
    "# Usage when you add new rules:\n",
    "# old_results = tagger.tag_conversations(conversations)\n",
    "# tagger.add_base_rule('new_rule', some_function)\n",
    "# new_results = tagger.tag_conversations(conversations)\n",
    "# compare_tag_distributions(old_results, new_results)\n",
    "\n",
    "# ===== SPECIFIC ISSUE INVESTIGATION =====\n",
    "\n",
    "# If you suspect certain conversations should have gizmo/plugin tags but don't:\n",
    "def investigate_specific_conversation(conversation_id_or_title):\n",
    "    \\\"\\\"\\\"Deep dive into a specific conversation.\\\"\\\"\\\"\n",
    "    target_conv = None\n",
    "    for conv in conversations:\n",
    "        if (conv.get('conversation_id') == conversation_id_or_title or \n",
    "            conversation_id_or_title.lower() in conv.get('title', '').lower()):\n",
    "            target_conv = conv\n",
    "            break\n",
    "    \n",
    "    if not target_conv:\n",
    "        print(f\"Conversation '{conversation_id_or_title}' not found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\\\n=== INVESTIGATING: {target_conv.get('title', 'Untitled')} ===\")\n",
    "    \n",
    "    # Full conversation structure\n",
    "    print(\"\\\\nConversation structure:\")\n",
    "    print(f\"  Title: {target_conv.get('title')}\")\n",
    "    print(f\"  ID: {target_conv.get('conversation_id')}\")\n",
    "    print(f\"  Top-level gizmo_id: {target_conv.get('gizmo_id')}\")\n",
    "    print(f\"  Top-level plugin_ids: {target_conv.get('plugin_ids')}\")\n",
    "    \n",
    "    # Message analysis\n",
    "    mapping = target_conv.get('mapping', {})\n",
    "    print(f\"\\\\nMessage mapping has {len(mapping)} nodes\")\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if message:\n",
    "            metadata = message.get('metadata', {})\n",
    "            gizmo_fields = {k: v for k, v in metadata.items() if 'gizmo' in k.lower()}\n",
    "            plugin_fields = {k: v for k, v in metadata.items() if 'plugin' in k.lower()}\n",
    "            \n",
    "            if gizmo_fields or plugin_fields:\n",
    "                print(f\"  Message {node_id[:8]}...\")\n",
    "                if gizmo_fields:\n",
    "                    print(f\"    Gizmo fields: {gizmo_fields}\")\n",
    "                if plugin_fields:\n",
    "                    print(f\"    Plugin fields: {plugin_fields}\")\n",
    "    \n",
    "    # Test tagging\n",
    "    result = tagger.tag_conversation(target_conv)\n",
    "    print(f\"\\\\nGenerated tags:\")\n",
    "    for tag in result['tags']:\n",
    "        print(f\"  {tag}\")\n",
    "    \n",
    "    # Show debug info\n",
    "    print(f\"\\\\nDebug info:\")\n",
    "    for rule in result['debug_info'].get('applied_rules', []):\n",
    "        print(f\"   {rule}\")\n",
    "    for error in result['debug_info'].get('errors', []):\n",
    "        print(f\"    {error}\")\n",
    "\n",
    "# Example usage:\n",
    "# investigate_specific_conversation(\"some conversation title\")\n",
    "# investigate_specific_conversation(\"conv-id-12345\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9514b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged 1673 conversations\n",
      "\n",
      "=== TAG SUMMARY ===\n",
      "prompt_stats: 1673 (100.0%)\n",
      "    count: avg=7.6, range=[1, 160]\n",
      "    mean: avg=1142.4, range=[1.0, 184662.5]\n",
      "    median: avg=814.2, range=[1.0, 184662.5]\n",
      "    variance: avg=51563065.2, range=[0, 34065162056.2]\n",
      "    values: consistency=consistent, consistency=mixed, consistency=variable, length_category=long, length_category=medium, length_category=short, length_category=very_long, length_category=very_short\n",
      "conversation_length: 1673 (100.0%)\n",
      "    count: avg=7.6, range=[1, 160]\n",
      "    values: category=long, category=medium, category=short, category=single, category=very_long\n",
      "coding_assistance: 1551 (92.7%)\n",
      "code_patterns: 1546 (92.4%)\n",
      "large_content: 1362 (81.4%)\n",
      "short_prompts: 1261 (75.4%)\n",
      "brief_interaction: 906 (54.2%)\n",
      "consistent_prompts: 896 (53.6%)\n",
      "enhanced_conversation: 681 (40.7%)\n",
      "gizmo: 676 (40.4%)\n",
      "    values: gizmo_id=g-IibMsD7w8, gizmo_id=g-KpF6lTka3, gizmo_id=g-QsUj0Smzg, gizmo_id=g-WiEAUBGzb, gizmo_id=g-bWPVPw7oK, gizmo_id=g-pYtHuQdGh\n",
      "variable_prompts: 405 (24.2%)\n",
      "extended_conversation: 304 (18.2%)\n",
      "coding_assistance_start: 260 (15.5%)\n",
      "evolving_discussion: 247 (14.8%)\n",
      "starts_code_patterns: 240 (14.3%)\n",
      "interactive_session: 184 (11.0%)\n",
      "long_prompts: 151 (9.0%)\n",
      "context_heavy_start: 127 (7.6%)\n",
      "starts_large_content: 119 (7.1%)\n",
      "web_search: 66 (3.9%)\n",
      "context_dump: 64 (3.8%)\n",
      "research_session: 53 (3.2%)\n",
      "code_execution: 26 (1.6%)\n",
      "reasoning: 12 (0.7%)\n",
      "plugin: 11 (0.7%)\n",
      "    values: plugin_id=AskTheCode, plugin_id=chatgpt_production_alltrails_com__jit_plugin, plugin_id=chatwithvideo, plugin_id=code_repo_interaction, plugin_id=g-eaf0e78bb71845ae7b86a9268d1374c168f5bd85, plugin_id=plugin-25ce675b-c2c4-4460-ad33-8e641653498c, plugin_id=plugin-3cf29a7b-2fcb-42aa-b762-4d418b543a8b, plugin_id=plugin-b82d4d0b-79b8-4548-b1b4-65b218376d6f, plugin_id=plugin-e5009c08-c6c8-4195-977f-16f39a7d3b7b, plugin_id=scholar_assist\n",
      "starts_with_attachments: 8 (0.5%)\n",
      "complex_analysis: 5 (0.3%)\n",
      "canvas_operations: 3 (0.2%)\n"
     ]
    }
   ],
   "source": [
    "tagger = create_default_tagger()\n",
    "tagged_results = tagger.tag_conversations(convs)\n",
    "tagger.print_summary(tagged_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49f2fbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== INVESTIGATING GIZMO/PLUGIN DETECTION ===\n",
      "\\nConversation 1: SCOTUS Justices Info...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 2: MOTU Meaning Explained...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 3: Poker Face Glasses Inquiry...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 4: OPM Proposed Rule Summary...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 5: Post Tracking Fixes...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 6: Reddit Data Pipeline...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 7: JAX SLURM Setup...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 8: Amplifying Propaganda through Sharing...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 9: Gohlke Python Binaries Website...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 10: Daryl Davis Klan Conversion...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 11: Sing-songy Meter Analysis...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 12: Salami Tactics Overview...\n",
      "  gizmo_id: g-IibMsD7w8\n",
      "  plugin_ids: None\n",
      "  message-level gizmos: ['g-IibMsD7w8', 'g-IibMsD7w8', 'g-IibMsD7w8', 'g-IibMsD7w8']\n",
      "\\nConversation 13: Hugging Face Company Type...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 14: Squash Merge Methods...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 15: Shortening Manipulation Phrase...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 16: Universality Classes Overview...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 17: On-Policy vs Off-Policy RL...\n",
      "  gizmo_id: g-IibMsD7w8\n",
      "  plugin_ids: None\n",
      "  message-level gizmos: ['g-IibMsD7w8']\n",
      "\\nConversation 18: IQ Estimate from Conversation...\n",
      "  gizmo_id: g-IibMsD7w8\n",
      "  plugin_ids: None\n",
      "  message-level gizmos: ['g-IibMsD7w8']\n",
      "\\nConversation 19: IQ Estimation Discussion...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 20: Diseases Characterized After 1960...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 21: Verteidigungsschnapsdrossel Erklrung...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 22: Miyazaki AI Comments Update...\n",
      "  gizmo_id: None\n",
      "  plugin_ids: None\n",
      "   No gizmo/plugin usage detected in this conversation\n",
      "\\nConversation 23: Index for Unitarizable Supermodules...\n",
      "  gizmo_id: g-IibMsD7w8\n",
      "  plugin_ids: None\n",
      "  message-level gizmos: ['g-IibMsD7w8', 'g-IibMsD7w8', 'g-IibMsD7w8', 'g-IibMsD7w8', 'g-IibMsD7w8']\n",
      "\\nConversation 24: Entropy and Arrow of Time...\n",
      "  gizmo_id: g-IibMsD7w8\n",
      "  plugin_ids: None\n",
      "  message-level gizmos: ['g-IibMsD7w8']\n",
      "\\nConversation 25: Potential in Vector Fields...\n",
      "  gizmo_id: g-IibMsD7w8\n",
      "  plugin_ids: None\n",
      "  message-level gizmos: ['g-IibMsD7w8']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If no gizmo/plugin tags show up, let's debug why\n",
    "print(\"\\\\n=== INVESTIGATING GIZMO/PLUGIN DETECTION ===\")\n",
    "\n",
    "# Check raw data for gizmo/plugin fields in first few conversations\n",
    "for i, conv in enumerate(convs[:25]):\n",
    "    print(f\"\\\\nConversation {i+1}: {conv.get('title', 'Untitled')[:50]}...\")\n",
    "    \n",
    "    # Check conversation-level gizmo/plugin fields\n",
    "    print(f\"  gizmo_id: {conv.get('gizmo_id')}\")\n",
    "    print(f\"  plugin_ids: {conv.get('plugin_ids')}\")\n",
    "    \n",
    "    # Check message-level fields\n",
    "    mapping = conv.get('mapping', {})\n",
    "    gizmo_messages = []\n",
    "    plugin_messages = []\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if metadata.get('gizmo_id'):\n",
    "            gizmo_messages.append(metadata['gizmo_id'])\n",
    "        if metadata.get('invoked_plugin'):\n",
    "            plugin_messages.append(metadata['invoked_plugin'])\n",
    "    \n",
    "    if gizmo_messages:\n",
    "        print(f\"  message-level gizmos: {gizmo_messages}\")\n",
    "    if plugin_messages:\n",
    "        print(f\"  message-level plugins: {plugin_messages}\")\n",
    "    \n",
    "    if not any([conv.get('gizmo_id'), conv.get('plugin_ids'), gizmo_messages, plugin_messages]):\n",
    "        print(f\"   No gizmo/plugin usage detected in this conversation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0d482e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11216153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: Salami Tactics Overview\n",
      "Gizmo ID: g-IibMsD7w8\n",
      "Exception in create_gizmo_plugin_tags: Tag.__init__() got multiple values for argument 'name'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/l6/nkl_x29x4n37bxnfcr7rt68m0000gn/T/ipykernel_43229/744364887.py\", line 7, in <module>\n",
      "    tags = create_gizmo_plugin_tags(test_conv)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/l6/nkl_x29x4n37bxnfcr7rt68m0000gn/T/ipykernel_43229/3884093179.py\", line 578, in create_gizmo_plugin_tags\n",
      "    tags.append(Tag('gizmo', name=gizmo))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Tag.__init__() got multiple values for argument 'name'\n"
     ]
    }
   ],
   "source": [
    "# Quick test of gizmo detection\n",
    "test_conv = next(conv for conv in conversations if conv.get('gizmo_id'))\n",
    "print(f\"Testing: {test_conv.get('title')}\")\n",
    "print(f\"Gizmo ID: {test_conv.get('gizmo_id')}\")\n",
    "\n",
    "try:\n",
    "    tags = create_gizmo_plugin_tags(test_conv)\n",
    "    print(f\"Function returned: {tags}\")\n",
    "    print(f\"Tag types: {[type(tag) for tag in tags]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Exception in create_gizmo_plugin_tags: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74ee7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhanced_conversation_tagger.py\n",
    "\"\"\"\n",
    "Enhanced conversation tagging system with faceting capabilities.\n",
    "Allows analysis of tag distributions across different facets (gizmos, conversation types, etc.).\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Any, List, Callable, Set, Union, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "class Tag:\n",
    "    \"\"\"Represents a tag with optional key-value attributes.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, **attributes):\n",
    "        self.name = name\n",
    "        self.attributes = attributes\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.attributes:\n",
    "            attrs_str = \", \".join(f\"{k}={v}\" for k, v in self.attributes.items())\n",
    "            return f\"{self.name}({attrs_str})\"\n",
    "        return self.name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Tag('{self.name}', {self.attributes})\"\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, str):\n",
    "            return self.name == other\n",
    "        elif isinstance(other, Tag):\n",
    "            return self.name == other.name and self.attributes == other.attributes\n",
    "        return False\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.name, tuple(sorted(self.attributes.items()))))\n",
    "    \n",
    "    def matches(self, name: str, **criteria) -> bool:\n",
    "        \"\"\"Check if tag matches name and optional attribute criteria.\"\"\"\n",
    "        if self.name != name:\n",
    "            return False\n",
    "        \n",
    "        for key, value in criteria.items():\n",
    "            if key not in self.attributes:\n",
    "                return False\n",
    "            \n",
    "            attr_value = self.attributes[key]\n",
    "            \n",
    "            # Support comparison operators\n",
    "            if isinstance(value, dict):\n",
    "                for op, target in value.items():\n",
    "                    if op == 'gt' and not (attr_value > target):\n",
    "                        return False\n",
    "                    elif op == 'gte' and not (attr_value >= target):\n",
    "                        return False\n",
    "                    elif op == 'lt' and not (attr_value < target):\n",
    "                        return False\n",
    "                    elif op == 'lte' and not (attr_value <= target):\n",
    "                        return False\n",
    "                    elif op == 'eq' and not (attr_value == target):\n",
    "                        return False\n",
    "                    elif op == 'ne' and not (attr_value != target):\n",
    "                        return False\n",
    "                    elif op == 'in' and not (attr_value in target):\n",
    "                        return False\n",
    "            else:\n",
    "                # Direct equality\n",
    "                if attr_value != value:\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "\n",
    "class ConversationTagger:\n",
    "    \"\"\"\n",
    "    Enhanced tagging system supporting structured tags with attributes and faceting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_rules: Dict[str, Callable] = {}\n",
    "        self.multi_tag_rules: Dict[str, Callable] = {}\n",
    "        self.supplemental_rules: Dict[str, Callable] = {}\n",
    "        self.rule_descriptions: Dict[str, str] = {}\n",
    "    \n",
    "    def add_base_rule(self, tag_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"Add a base tagging rule that returns bool or Tag object.\"\"\"\n",
    "        self.base_rules[tag_name] = rule_function\n",
    "        self.rule_descriptions[tag_name] = description\n",
    "    \n",
    "    def add_multi_tag_rule(self, rule_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"Add a rule that returns multiple tags (strings or Tag objects).\"\"\"\n",
    "        self.multi_tag_rules[rule_name] = rule_function\n",
    "        self.rule_descriptions[rule_name] = description\n",
    "    \n",
    "    def add_supplemental_rule(self, tag_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"Add a supplemental rule that depends on existing tags.\"\"\"\n",
    "        self.supplemental_rules[tag_name] = rule_function\n",
    "        self.rule_descriptions[tag_name] = description\n",
    "    \n",
    "    def _normalize_tag(self, tag: Union[str, Tag]) -> Tag:\n",
    "        \"\"\"Convert string tags to Tag objects.\"\"\"\n",
    "        if isinstance(tag, str):\n",
    "            return Tag(tag)\n",
    "        return tag\n",
    "    \n",
    "    def tag_conversation(self, conversation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Apply all tagging rules to a conversation.\"\"\"\n",
    "        tags = set()\n",
    "        debug_info = defaultdict(list)\n",
    "        \n",
    "        # Apply base rules\n",
    "        for tag_name, rule_func in self.base_rules.items():\n",
    "            try:\n",
    "                result = rule_func(conversation)\n",
    "                if result:\n",
    "                    if isinstance(result, bool):\n",
    "                        tag = Tag(tag_name)\n",
    "                    else:\n",
    "                        tag = self._normalize_tag(result)\n",
    "                    tags.add(tag)\n",
    "                    debug_info['applied_rules'].append(f\"BASE: {tag}\")\n",
    "                else:\n",
    "                    debug_info['skipped_rules'].append(f\"BASE: {tag_name}\")\n",
    "            except Exception as e:\n",
    "                debug_info['errors'].append(f\"BASE: {tag_name} - {str(e)}\")\n",
    "        \n",
    "        # Apply multi-tag rules\n",
    "        for rule_name, rule_func in self.multi_tag_rules.items():\n",
    "            try:\n",
    "                new_tags = rule_func(conversation)\n",
    "                if new_tags:\n",
    "                    normalized_tags = [self._normalize_tag(tag) for tag in new_tags]\n",
    "                    tags.update(normalized_tags)\n",
    "                    debug_info['applied_rules'].append(f\"MULTI: {rule_name} -> {[str(t) for t in normalized_tags]}\")\n",
    "                else:\n",
    "                    debug_info['skipped_rules'].append(f\"MULTI: {rule_name}\")\n",
    "            except Exception as e:\n",
    "                debug_info['errors'].append(f\"MULTI: {rule_name} - {str(e)}\")\n",
    "        \n",
    "        # Apply supplemental rules\n",
    "        max_iterations = 5\n",
    "        for iteration in range(max_iterations):\n",
    "            initial_tag_count = len(tags)\n",
    "            \n",
    "            for tag_name, rule_func in self.supplemental_rules.items():\n",
    "                # Check if tag already exists\n",
    "                if any(tag.name == tag_name for tag in tags):\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    result = rule_func(conversation, tags)\n",
    "                    if result:\n",
    "                        if isinstance(result, bool):\n",
    "                            tag = Tag(tag_name)\n",
    "                        else:\n",
    "                            tag = self._normalize_tag(result)\n",
    "                        tags.add(tag)\n",
    "                        debug_info['applied_rules'].append(f\"SUPP: {tag} (iter {iteration})\")\n",
    "                    else:\n",
    "                        debug_info['skipped_rules'].append(f\"SUPP: {tag_name} (iter {iteration})\")\n",
    "                except Exception as e:\n",
    "                    debug_info['errors'].append(f\"SUPP: {tag_name} - {str(e)}\")\n",
    "            \n",
    "            if len(tags) == initial_tag_count:\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'conversation_id': conversation.get('conversation_id', conversation.get('id', 'unknown')),\n",
    "            'title': conversation.get('title', 'Untitled'),\n",
    "            'tags': list(tags),\n",
    "            'debug_info': dict(debug_info),\n",
    "            'conversation': conversation\n",
    "        }\n",
    "    \n",
    "    def tag_conversations(self, conversations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Tag multiple conversations.\"\"\"\n",
    "        return [self.tag_conversation(conv) for conv in conversations]\n",
    "    \n",
    "    def filter_by_tags(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                      include_tags: List[Union[str, Dict]] = None,\n",
    "                      exclude_tags: List[Union[str, Dict]] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Filter conversations by tags with attribute support.\n",
    "        \n",
    "        Args:\n",
    "            include_tags: List of tag names or dicts with criteria\n",
    "                Examples: ['web_search', {'name': 'gizmo', 'type': 'dalle'}]\n",
    "            exclude_tags: Similar format for exclusions\n",
    "        \"\"\"\n",
    "        filtered = []\n",
    "        \n",
    "        for tagged_conv in tagged_conversations:\n",
    "            tags = tagged_conv['tags']\n",
    "            \n",
    "            # Check exclusions first\n",
    "            if exclude_tags:\n",
    "                should_exclude = False\n",
    "                for exclude_criterion in exclude_tags:\n",
    "                    if self._matches_criterion(tags, exclude_criterion):\n",
    "                        should_exclude = True\n",
    "                        break\n",
    "                if should_exclude:\n",
    "                    continue\n",
    "            \n",
    "            # Check inclusions\n",
    "            if include_tags:\n",
    "                should_include = True\n",
    "                for include_criterion in include_tags:\n",
    "                    if not self._matches_criterion(tags, include_criterion):\n",
    "                        should_include = False\n",
    "                        break\n",
    "                if not should_include:\n",
    "                    continue\n",
    "            \n",
    "            filtered.append(tagged_conv)\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def _matches_criterion(self, tags: List[Tag], criterion: Union[str, Dict]) -> bool:\n",
    "        \"\"\"Check if any tag matches the given criterion.\"\"\"\n",
    "        if isinstance(criterion, str):\n",
    "            return any(tag.name == criterion for tag in tags)\n",
    "        \n",
    "        elif isinstance(criterion, dict):\n",
    "            name = criterion.get('name')\n",
    "            if not name:\n",
    "                return False\n",
    "            \n",
    "            criteria = {k: v for k, v in criterion.items() if k != 'name'}\n",
    "            return any(tag.matches(name, **criteria) for tag in tags)\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_facet_value(self, tags: List[Tag], facet_tag_name: str, \n",
    "                       facet_attribute: Optional[str] = None) -> str:\n",
    "        \"\"\"Extract facet value from a conversation's tags.\"\"\"\n",
    "        matching_tags = [tag for tag in tags if tag.name == facet_tag_name]\n",
    "        \n",
    "        if not matching_tags:\n",
    "            return \"<none>\"\n",
    "        \n",
    "        if facet_attribute is None:\n",
    "            # Just check for presence of the tag\n",
    "            return f\"has_{facet_tag_name}\"\n",
    "        \n",
    "        # Extract specific attribute values\n",
    "        values = []\n",
    "        for tag in matching_tags:\n",
    "            if facet_attribute in tag.attributes:\n",
    "                values.append(str(tag.attributes[facet_attribute]))\n",
    "        \n",
    "        if not values:\n",
    "            return f\"<{facet_tag_name}_no_{facet_attribute}>\"\n",
    "        \n",
    "        # If multiple values, join them\n",
    "        return \"; \".join(sorted(set(values)))\n",
    "    \n",
    "    def facet_conversations(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                           facet_tag_name: str, \n",
    "                           facet_attribute: Optional[str] = None,\n",
    "                           max_facets: int = 50) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Group conversations by facet values.\n",
    "        \n",
    "        Args:\n",
    "            facet_tag_name: Tag name to facet by (e.g., 'gizmo')\n",
    "            facet_attribute: Optional attribute within tag (e.g., 'gizmo_id')\n",
    "            max_facets: Maximum number of facet values to show\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping facet values to lists of conversations\n",
    "        \"\"\"\n",
    "        facets = defaultdict(list)\n",
    "        \n",
    "        for tagged_conv in tagged_conversations:\n",
    "            facet_value = self.get_facet_value(tagged_conv['tags'], facet_tag_name, facet_attribute)\n",
    "            facets[facet_value].append(tagged_conv)\n",
    "        \n",
    "        # Sort by facet size (largest first) and limit\n",
    "        sorted_facets = dict(sorted(facets.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "        \n",
    "        if len(sorted_facets) > max_facets:\n",
    "            # Keep top facets and group rest into \"others\"\n",
    "            items = list(sorted_facets.items())\n",
    "            top_facets = dict(items[:max_facets-1])\n",
    "            \n",
    "            other_conversations = []\n",
    "            for _, conversations in items[max_facets-1:]:\n",
    "                other_conversations.extend(conversations)\n",
    "            \n",
    "            if other_conversations:\n",
    "                top_facets[\"<other>\"] = other_conversations\n",
    "            \n",
    "            return top_facets\n",
    "        \n",
    "        return sorted_facets\n",
    "    \n",
    "    def print_faceted_summary(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                             facet_tag_name: str, \n",
    "                             facet_attribute: Optional[str] = None,\n",
    "                             show_details: bool = False,\n",
    "                             max_facets: int = 20,\n",
    "                             max_tags_per_facet: int = 15):\n",
    "        \"\"\"\n",
    "        Print tag summary broken down by facets.\n",
    "        \n",
    "        Args:\n",
    "            facet_tag_name: Tag to facet by\n",
    "            facet_attribute: Optional attribute within the tag  \n",
    "            show_details: Show detailed tag attribute info\n",
    "            max_facets: Maximum facets to show\n",
    "            max_tags_per_facet: Maximum tags to show per facet\n",
    "        \"\"\"\n",
    "        total = len(tagged_conversations)\n",
    "        facets = self.facet_conversations(tagged_conversations, facet_tag_name, facet_attribute, max_facets)\n",
    "        \n",
    "        print(f\"Tagged {total} conversations\")\n",
    "        print(f\"Faceted by: {facet_tag_name}\" + \n",
    "              (f\".{facet_attribute}\" if facet_attribute else \"\"))\n",
    "        print(f\"Found {len(facets)} facet values\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FACETED TAG SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for facet_value, facet_conversations in facets.items():\n",
    "            facet_size = len(facet_conversations)\n",
    "            facet_percentage = (facet_size / total) * 100\n",
    "            \n",
    "            print(f\"\\n FACET: {facet_value}\")\n",
    "            print(f\"    Conversations: {facet_size} ({facet_percentage:.1f}% of total)\")\n",
    "            print(f\"    {'-' * 60}\")\n",
    "            \n",
    "            # Calculate tag statistics for this facet\n",
    "            tag_counts = defaultdict(int)\n",
    "            tag_attributes = defaultdict(lambda: defaultdict(list))\n",
    "            unique_structured_tags = defaultdict(set)\n",
    "            \n",
    "            for tagged_conv in facet_conversations:\n",
    "                for tag in tagged_conv['tags']:\n",
    "                    tag_counts[tag.name] += 1\n",
    "                    \n",
    "                    # Collect attribute information\n",
    "                    for attr_name, attr_value in tag.attributes.items():\n",
    "                        if isinstance(attr_value, (int, float)):\n",
    "                            tag_attributes[tag.name][attr_name].append(attr_value)\n",
    "                        else:\n",
    "                            unique_structured_tags[tag.name].add(f\"{attr_name}={attr_value}\")\n",
    "            \n",
    "            # Sort and limit tags for this facet\n",
    "            sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            display_tags = sorted_tags[:max_tags_per_facet]\n",
    "            \n",
    "            for tag_name, count in display_tags:\n",
    "                percentage = (count / facet_size) * 100\n",
    "                print(f\"    {tag_name}: {count} ({percentage:.1f}%)\")\n",
    "                \n",
    "                if show_details:\n",
    "                    # Show numeric attribute statistics\n",
    "                    if tag_name in tag_attributes:\n",
    "                        for attr_name, values in tag_attributes[tag_name].items():\n",
    "                            if values:\n",
    "                                avg_val = sum(values) / len(values)\n",
    "                                min_val = min(values)\n",
    "                                max_val = max(values)\n",
    "                                print(f\"        {attr_name}: avg={avg_val:.1f}, range=[{min_val}, {max_val}]\")\n",
    "                    \n",
    "                    # Show unique structured values\n",
    "                    if tag_name in unique_structured_tags:\n",
    "                        unique_vals = sorted(unique_structured_tags[tag_name])\n",
    "                        if len(unique_vals) <= 5:\n",
    "                            print(f\"        values: {', '.join(unique_vals)}\")\n",
    "                        else:\n",
    "                            print(f\"        values: {', '.join(unique_vals[:5])} ... (+{len(unique_vals)-5} more)\")\n",
    "            \n",
    "            if len(sorted_tags) > max_tags_per_facet:\n",
    "                remaining = len(sorted_tags) - max_tags_per_facet\n",
    "                print(f\"    ... and {remaining} more tags\")\n",
    "    \n",
    "    def compare_facets(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                      facet_tag_name: str, \n",
    "                      facet_attribute: Optional[str] = None,\n",
    "                      comparison_tags: List[str] = None,\n",
    "                      min_facet_size: int = 10) -> None:\n",
    "        \"\"\"\n",
    "        Compare specific tags across facets.\n",
    "        \n",
    "        Args:\n",
    "            comparison_tags: List of tags to compare across facets\n",
    "            min_facet_size: Minimum facet size to include in comparison\n",
    "        \"\"\"\n",
    "        facets = self.facet_conversations(tagged_conversations, facet_tag_name, facet_attribute)\n",
    "        \n",
    "        # Filter facets by minimum size\n",
    "        large_facets = {k: v for k, v in facets.items() if len(v) >= min_facet_size}\n",
    "        \n",
    "        if not large_facets:\n",
    "            print(f\"No facets with at least {min_facet_size} conversations found\")\n",
    "            return\n",
    "        \n",
    "        # If no specific tags provided, use most common tags overall\n",
    "        if comparison_tags is None:\n",
    "            overall_tag_counts = defaultdict(int)\n",
    "            for tagged_conv in tagged_conversations:\n",
    "                for tag in tagged_conv['tags']:\n",
    "                    overall_tag_counts[tag.name] += 1\n",
    "            \n",
    "            # Get top 10 most common tags\n",
    "            comparison_tags = [tag for tag, _ in \n",
    "                             sorted(overall_tag_counts.items(), key=lambda x: x[1], reverse=True)[:10]]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FACET COMPARISON\")\n",
    "        print(f\"Comparing tags: {', '.join(comparison_tags)}\")\n",
    "        print(f\"Across facets: {facet_tag_name}\" + \n",
    "              (f\".{facet_attribute}\" if facet_attribute else \"\"))\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Calculate percentages for each tag in each facet\n",
    "        results = {}\n",
    "        for facet_value, facet_conversations in large_facets.items():\n",
    "            facet_size = len(facet_conversations)\n",
    "            facet_tag_counts = defaultdict(int)\n",
    "            \n",
    "            for tagged_conv in facet_conversations:\n",
    "                for tag in tagged_conv['tags']:\n",
    "                    facet_tag_counts[tag.name] += 1\n",
    "            \n",
    "            results[facet_value] = {\n",
    "                'size': facet_size,\n",
    "                'percentages': {tag: (facet_tag_counts[tag] / facet_size) * 100 \n",
    "                               for tag in comparison_tags}\n",
    "            }\n",
    "        \n",
    "        # Print comparison table\n",
    "        print(f\"\\n{'Facet':<30} {'Size':<8} \" + \n",
    "              \"\".join(f\"{tag:<15}\" for tag in comparison_tags))\n",
    "        print(\"-\" * (30 + 8 + 15 * len(comparison_tags)))\n",
    "        \n",
    "        for facet_value, data in results.items():\n",
    "            facet_display = facet_value[:28] + \"..\" if len(facet_value) > 30 else facet_value\n",
    "            row = f\"{facet_display:<30} {data['size']:<8} \"\n",
    "            row += \"\".join(f\"{data['percentages'][tag]:<15.1f}\" for tag in comparison_tags)\n",
    "            print(row)\n",
    "        \n",
    "        # Highlight interesting differences\n",
    "        print(f\"\\n NOTABLE DIFFERENCES:\")\n",
    "        for tag in comparison_tags:\n",
    "            percentages = [results[facet]['percentages'][tag] for facet in results.keys()]\n",
    "            if max(percentages) - min(percentages) > 20:  # 20% difference threshold\n",
    "                max_facet = max(results.keys(), key=lambda f: results[f]['percentages'][tag])\n",
    "                min_facet = min(results.keys(), key=lambda f: results[f]['percentages'][tag])\n",
    "                print(f\"    {tag}: {max_facet} ({results[max_facet]['percentages'][tag]:.1f}%) vs \" +\n",
    "                      f\"{min_facet} ({results[min_facet]['percentages'][tag]:.1f}%)\")\n",
    "    \n",
    "    def get_tag_values(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                      tag_name: str, attribute: str) -> List[Any]:\n",
    "        \"\"\"Extract attribute values from tags across conversations.\"\"\"\n",
    "        values = []\n",
    "        for tagged_conv in tagged_conversations:\n",
    "            for tag in tagged_conv['tags']:\n",
    "                if tag.name == tag_name and attribute in tag.attributes:\n",
    "                    values.append(tag.attributes[attribute])\n",
    "        return values\n",
    "    \n",
    "    def print_summary(self, tagged_conversations: List[Dict[str, Any]], show_details: bool = True):\n",
    "        \"\"\"Print comprehensive summary with all tag types and optional details.\"\"\"\n",
    "        total = len(tagged_conversations)\n",
    "        tag_counts = defaultdict(int)\n",
    "        tag_attributes = defaultdict(lambda: defaultdict(list))\n",
    "        unique_structured_tags = defaultdict(set)\n",
    "        \n",
    "        # Collect all tag information\n",
    "        for tagged_conv in tagged_conversations:\n",
    "            for tag in tagged_conv['tags']:\n",
    "                tag_counts[tag.name] += 1\n",
    "                \n",
    "                # Collect attribute information\n",
    "                for attr_name, attr_value in tag.attributes.items():\n",
    "                    if isinstance(attr_value, (int, float)):\n",
    "                        tag_attributes[tag.name][attr_name].append(attr_value)\n",
    "                    else:\n",
    "                        # For non-numeric attributes, track unique values\n",
    "                        unique_structured_tags[tag.name].add(f\"{attr_name}={attr_value}\")\n",
    "        \n",
    "        print(f\"Tagged {total} conversations\")\n",
    "        print(f\"\\n=== TAG SUMMARY ===\")\n",
    "        \n",
    "        # Sort tags by frequency for better readability\n",
    "        sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for tag_name, count in sorted_tags:\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"{tag_name}: {count} ({percentage:.1f}%)\")\n",
    "            \n",
    "            if show_details:\n",
    "                # Show numeric attribute statistics\n",
    "                if tag_name in tag_attributes:\n",
    "                    for attr_name, values in tag_attributes[tag_name].items():\n",
    "                        if values:\n",
    "                            avg_val = sum(values) / len(values)\n",
    "                            min_val = min(values)\n",
    "                            max_val = max(values)\n",
    "                            print(f\"    {attr_name}: avg={avg_val:.1f}, range=[{min_val}, {max_val}]\")\n",
    "                \n",
    "                # Show unique structured values for non-numeric attributes\n",
    "                if tag_name in unique_structured_tags:\n",
    "                    unique_vals = sorted(unique_structured_tags[tag_name])\n",
    "                    if len(unique_vals) <= 10:  # Show all if not too many\n",
    "                        print(f\"    values: {', '.join(unique_vals)}\")\n",
    "                    else:  # Show top 10 most common\n",
    "                        print(f\"    values: {', '.join(unique_vals[:10])} ... (+{len(unique_vals)-10} more)\")\n",
    "\n",
    "\n",
    "# Example usage functions to demonstrate the faceting capabilities\n",
    "\n",
    "def demo_faceting_usage():\n",
    "    \"\"\"\n",
    "    Demonstrate how to use the new faceting capabilities.\n",
    "    \"\"\"\n",
    "    print(\"\"\"\n",
    "# FACETING USAGE EXAMPLES\n",
    "\n",
    "# 1. Facet by gizmo presence (with/without gizmos)\n",
    "tagger.print_faceted_summary(tagged_results, 'gizmo')\n",
    "\n",
    "# 2. Facet by specific gizmo IDs  \n",
    "tagger.print_faceted_summary(tagged_results, 'gizmo', 'gizmo_id')\n",
    "\n",
    "# 3. Facet by conversation length categories\n",
    "tagger.print_faceted_summary(tagged_results, 'conversation_length', 'category')\n",
    "\n",
    "# 4. Facet by prompt length categories\n",
    "tagger.print_faceted_summary(tagged_results, 'prompt_stats', 'length_category')\n",
    "\n",
    "# 5. Facet by prompt consistency patterns\n",
    "tagger.print_faceted_summary(tagged_results, 'prompt_stats', 'consistency')\n",
    "\n",
    "# 6. Compare coding patterns across gizmos\n",
    "tagger.compare_facets(tagged_results, 'gizmo', 'gizmo_id', \n",
    "                     comparison_tags=['coding_assistance', 'code_patterns', 'web_search'])\n",
    "\n",
    "# 7. Compare conversation patterns across length categories\n",
    "tagger.compare_facets(tagged_results, 'conversation_length', 'category',\n",
    "                     comparison_tags=['coding_assistance', 'web_search', 'large_content'])\n",
    "\n",
    "# 8. Show detailed faceted summary\n",
    "tagger.print_faceted_summary(tagged_results, 'gizmo', 'gizmo_id', \n",
    "                            show_details=True, max_facets=10)\n",
    "\n",
    "# 9. Focus on specific gizmo usage patterns\n",
    "gizmo_conversations = tagger.filter_by_tags(tagged_results, include_tags=['gizmo'])\n",
    "tagger.print_faceted_summary(gizmo_conversations, 'gizmo', 'gizmo_id')\n",
    "\n",
    "# 10. Analyze how conversation patterns differ by whether they start with code\n",
    "tagger.print_faceted_summary(tagged_results, 'starts_code_patterns')\n",
    "\n",
    "# 11. Multi-level analysis: first facet by conversation length, then by gizmo within each\n",
    "for length_cat in ['single', 'short', 'medium', 'long', 'very_long']:\n",
    "    length_conversations = tagger.filter_by_tags(\n",
    "        tagged_results, \n",
    "        include_tags=[{'name': 'conversation_length', 'category': length_cat}]\n",
    "    )\n",
    "    if length_conversations:\n",
    "        print(f\"\\\\n=== GIZMO USAGE WITHIN {length_cat.upper()} CONVERSATIONS ===\")\n",
    "        tagger.print_faceted_summary(length_conversations, 'gizmo', 'gizmo_id', max_facets=5)\n",
    "    \"\"\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     demo_faceting_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebc6f861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged 1673 conversations\n",
      "\n",
      "=== TAG SUMMARY ===\n",
      "prompt_stats: 1673 (100.0%)\n",
      "    count: avg=7.6, range=[1, 160]\n",
      "    mean: avg=1142.4, range=[1.0, 184662.5]\n",
      "    median: avg=814.2, range=[1.0, 184662.5]\n",
      "    variance: avg=51563065.2, range=[0, 34065162056.2]\n",
      "    values: consistency=consistent, consistency=mixed, consistency=variable, length_category=long, length_category=medium, length_category=short, length_category=very_long, length_category=very_short\n",
      "conversation_length: 1673 (100.0%)\n",
      "    count: avg=7.6, range=[1, 160]\n",
      "    values: category=long, category=medium, category=short, category=single, category=very_long\n",
      "coding_assistance: 1551 (92.7%)\n",
      "code_patterns: 1546 (92.4%)\n",
      "large_content: 1362 (81.4%)\n",
      "short_prompts: 1261 (75.4%)\n",
      "brief_interaction: 906 (54.2%)\n",
      "consistent_prompts: 896 (53.6%)\n",
      "enhanced_conversation: 681 (40.7%)\n",
      "gizmo: 676 (40.4%)\n",
      "    values: gizmo_id=g-IibMsD7w8, gizmo_id=g-KpF6lTka3, gizmo_id=g-QsUj0Smzg, gizmo_id=g-WiEAUBGzb, gizmo_id=g-bWPVPw7oK, gizmo_id=g-pYtHuQdGh\n",
      "variable_prompts: 405 (24.2%)\n",
      "extended_conversation: 304 (18.2%)\n",
      "coding_assistance_start: 260 (15.5%)\n",
      "evolving_discussion: 247 (14.8%)\n",
      "starts_code_patterns: 240 (14.3%)\n",
      "interactive_session: 184 (11.0%)\n",
      "long_prompts: 151 (9.0%)\n",
      "context_heavy_start: 127 (7.6%)\n",
      "starts_large_content: 119 (7.1%)\n",
      "web_search: 66 (3.9%)\n",
      "context_dump: 64 (3.8%)\n",
      "research_session: 53 (3.2%)\n",
      "code_execution: 26 (1.6%)\n",
      "reasoning: 12 (0.7%)\n",
      "plugin: 11 (0.7%)\n",
      "    values: plugin_id=AskTheCode, plugin_id=chatgpt_production_alltrails_com__jit_plugin, plugin_id=chatwithvideo, plugin_id=code_repo_interaction, plugin_id=g-eaf0e78bb71845ae7b86a9268d1374c168f5bd85, plugin_id=plugin-25ce675b-c2c4-4460-ad33-8e641653498c, plugin_id=plugin-3cf29a7b-2fcb-42aa-b762-4d418b543a8b, plugin_id=plugin-b82d4d0b-79b8-4548-b1b4-65b218376d6f, plugin_id=plugin-e5009c08-c6c8-4195-977f-16f39a7d3b7b, plugin_id=scholar_assist\n",
      "starts_with_attachments: 8 (0.5%)\n",
      "complex_analysis: 5 (0.3%)\n",
      "canvas_operations: 3 (0.2%)\n"
     ]
    }
   ],
   "source": [
    "tagger = create_default_tagger()\n",
    "tagged_results = tagger.tag_conversations(convs)\n",
    "tagger.print_summary(tagged_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caaecc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged 1673 conversations\n",
      "Faceted by: gizmo.gizmo_id\n",
      "Found 7 facet values\n",
      "\n",
      "================================================================================\n",
      "FACETED TAG SUMMARY\n",
      "================================================================================\n",
      "\n",
      " FACET: <none>\n",
      "    Conversations: 997 (59.6% of total)\n",
      "    ------------------------------------------------------------\n",
      "    prompt_stats: 997 (100.0%)\n",
      "    conversation_length: 997 (100.0%)\n",
      "    coding_assistance: 891 (89.4%)\n",
      "    code_patterns: 886 (88.9%)\n",
      "    large_content: 696 (69.8%)\n",
      "    short_prompts: 627 (62.9%)\n",
      "    brief_interaction: 606 (60.8%)\n",
      "    consistent_prompts: 427 (42.8%)\n",
      "    variable_prompts: 330 (33.1%)\n",
      "    coding_assistance_start: 236 (23.7%)\n",
      "    starts_code_patterns: 216 (21.7%)\n",
      "    evolving_discussion: 191 (19.2%)\n",
      "    long_prompts: 139 (13.9%)\n",
      "    context_heavy_start: 119 (11.9%)\n",
      "    extended_conversation: 112 (11.2%)\n",
      "    ... and 12 more tags\n",
      "\n",
      " FACET: g-IibMsD7w8\n",
      "    Conversations: 668 (39.9% of total)\n",
      "    ------------------------------------------------------------\n",
      "    gizmo: 668 (100.0%)\n",
      "    prompt_stats: 668 (100.0%)\n",
      "    enhanced_conversation: 668 (100.0%)\n",
      "    conversation_length: 668 (100.0%)\n",
      "    large_content: 664 (99.4%)\n",
      "    code_patterns: 656 (98.2%)\n",
      "    coding_assistance: 656 (98.2%)\n",
      "    short_prompts: 628 (94.0%)\n",
      "    consistent_prompts: 463 (69.3%)\n",
      "    brief_interaction: 294 (44.0%)\n",
      "    extended_conversation: 191 (28.6%)\n",
      "    interactive_session: 166 (24.9%)\n",
      "    variable_prompts: 74 (11.1%)\n",
      "    evolving_discussion: 55 (8.2%)\n",
      "    coding_assistance_start: 23 (3.4%)\n",
      "    ... and 7 more tags\n",
      "\n",
      " FACET: g-bWPVPw7oK\n",
      "    Conversations: 4 (0.2% of total)\n",
      "    ------------------------------------------------------------\n",
      "    gizmo: 4 (100.0%)\n",
      "    short_prompts: 4 (100.0%)\n",
      "    prompt_stats: 4 (100.0%)\n",
      "    enhanced_conversation: 4 (100.0%)\n",
      "    conversation_length: 4 (100.0%)\n",
      "    brief_interaction: 3 (75.0%)\n",
      "    consistent_prompts: 3 (75.0%)\n",
      "    coding_assistance: 2 (50.0%)\n",
      "    code_patterns: 2 (50.0%)\n",
      "    extended_conversation: 1 (25.0%)\n",
      "\n",
      " FACET: g-WiEAUBGzb\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    code_patterns: 1 (100.0%)\n",
      "    coding_assistance: 1 (100.0%)\n",
      "    brief_interaction: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    large_content: 1 (100.0%)\n",
      "    short_prompts: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "    consistent_prompts: 1 (100.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "\n",
      " FACET: g-pYtHuQdGh\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    code_patterns: 1 (100.0%)\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    coding_assistance: 1 (100.0%)\n",
      "    coding_assistance_start: 1 (100.0%)\n",
      "    research_session: 1 (100.0%)\n",
      "    starts_code_patterns: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "    evolving_discussion: 1 (100.0%)\n",
      "    web_search: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "    variable_prompts: 1 (100.0%)\n",
      "    large_content: 1 (100.0%)\n",
      "\n",
      " FACET: g-QsUj0Smzg\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    brief_interaction: 1 (100.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "    short_prompts: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "    consistent_prompts: 1 (100.0%)\n",
      "\n",
      " FACET: g-KpF6lTka3\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    plugin: 2 (200.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "    brief_interaction: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "    consistent_prompts: 1 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "tagger.print_faceted_summary(tagged_results, 'gizmo', 'gizmo_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2676a2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# FACETING USAGE EXAMPLES\n",
      "\n",
      "# 1. Facet by gizmo presence (with/without gizmos)\n",
      "tagger.print_faceted_summary(tagged_results, 'gizmo')\n",
      "\n",
      "# 2. Facet by specific gizmo IDs (now without truncation)\n",
      "tagger.print_faceted_summary(tagged_results, 'gizmo', 'gizmo_id')\n",
      "\n",
      "# 3. Facet by conversation length categories\n",
      "tagger.print_faceted_summary(tagged_results, 'conversation_length', 'category')\n",
      "\n",
      "# 4. Facet by prompt length categories\n",
      "tagger.print_faceted_summary(tagged_results, 'prompt_stats', 'length_category')\n",
      "\n",
      "# 5. Facet by prompt consistency patterns\n",
      "tagger.print_faceted_summary(tagged_results, 'prompt_stats', 'consistency')\n",
      "\n",
      "# 6. Compare coding patterns across gizmos (with improved detection)\n",
      "tagger.compare_facets(tagged_results, 'gizmo', 'gizmo_id', \n",
      "                     comparison_tags=['definite_coding_assistance', 'strict_code_patterns', \n",
      "                                    'documentation_generation', 'wiki_documentation'])\n",
      "\n",
      "# 7. Compare old vs new code detection\n",
      "tagger.compare_facets(tagged_results, 'gizmo', 'gizmo_id',\n",
      "                     comparison_tags=['code_patterns', 'strict_code_patterns', \n",
      "                                    'coding_assistance', 'definite_coding_assistance'])\n",
      "\n",
      "# 8. Investigate specific gizmo patterns\n",
      "tagger.print_faceted_summary(tagged_results, 'gizmo', 'gizmo_id', show_details=True)\n",
      "\n",
      "# 9. Focus on documentation vs coding patterns\n",
      "tagger.compare_facets(tagged_results, 'wiki_documentation', None,\n",
      "                     comparison_tags=['code_patterns', 'strict_code_patterns', 'large_content'])\n",
      "\n",
      "# 10. Multi-level analysis: documentation generation within each gizmo\n",
      "for gizmo_id in ['g-IibMsD7w8', 'g-KpF6lTka3', 'g-QsUj0Smzg']:  # Replace with actual IDs\n",
      "    gizmo_conversations = tagger.filter_by_tags(\n",
      "        tagged_results, \n",
      "        include_tags=[{'name': 'gizmo', 'gizmo_id': gizmo_id}]\n",
      "    )\n",
      "    if gizmo_conversations:\n",
      "        print(f\"\\n=== CONTENT ANALYSIS FOR GIZMO {gizmo_id} ===\")\n",
      "        tagger.print_faceted_summary(gizmo_conversations, 'wiki_documentation')\n",
      "\n",
      "# 11. Debug improved code detection for wiki gizmo\n",
      "wiki_gizmo_conversations = tagger.filter_by_tags(\n",
      "    tagged_results,\n",
      "    include_tags=[{'name': 'gizmo', 'gizmo_id': 'g-IibMsD7w8'}]  # Replace with actual wiki gizmo ID\n",
      ")\n",
      "\n",
      "print(\"\\n=== WIKI GIZMO CODE DETECTION ANALYSIS ===\")\n",
      "print(\"Before improvements vs After improvements:\")\n",
      "\n",
      "old_code_count = len([c for c in wiki_gizmo_conversations \n",
      "                     if any(t.name == 'code_patterns' for t in c['tags'])])\n",
      "new_strict_count = len([c for c in wiki_gizmo_conversations \n",
      "                       if any(t.name == 'strict_code_patterns' for t in c['tags'])])\n",
      "wiki_doc_count = len([c for c in wiki_gizmo_conversations \n",
      "                     if any(t.name == 'wiki_documentation' for t in c['tags'])])\n",
      "\n",
      "print(f\"Original code_patterns detection: {old_code_count} conversations\")\n",
      "print(f\"Improved strict_code_patterns: {new_strict_count} conversations\") \n",
      "print(f\"Wiki/documentation detection: {wiki_doc_count} conversations\")\n",
      "print(f\"Reduction in false positives: {old_code_count - new_strict_count} conversations\")\n",
      "    \n",
      "\n",
      "# DEBUGGING CODE DETECTION IMPROVEMENTS\n",
      "\n",
      "# Test improved detection on wiki gizmo conversations\n",
      "def test_detection_on_sample(conversations, sample_size=10):\n",
      "    for i, conv in enumerate(conversations[:sample_size]):\n",
      "        print(f\"\\n--- Conversation {i+1}: {conv.get('title', 'Untitled')[:50]}... ---\")\n",
      "\n",
      "        # Test old detection\n",
      "        old_result = has_code_patterns_old(conv)  # You'd need to save the old function\n",
      "\n",
      "        # Test new detection  \n",
      "        new_result = has_code_patterns(conv)\n",
      "        strict_result = has_strict_code_patterns(conv)\n",
      "        wiki_result = has_wiki_documentation_patterns(conv)\n",
      "\n",
      "        print(f\"Old code detection: {old_result}\")\n",
      "        print(f\"New code detection: {new_result}\")\n",
      "        print(f\"Strict code detection: {strict_result}\")\n",
      "        print(f\"Wiki documentation: {wiki_result}\")\n",
      "\n",
      "        if old_result != new_result:\n",
      "            print(\" DETECTION CHANGED!\")\n",
      "\n",
      "        if old_result and wiki_result:\n",
      "            print(\" Likely false positive caught by wiki detection\")\n",
      "\n",
      "# Usage:\n",
      "# wiki_conversations = tagger.filter_by_tags(tagged_results, \n",
      "#                                           include_tags=[{'name': 'gizmo', 'gizmo_id': 'g-IibMsD7w8'}])\n",
      "# test_detection_on_sample(wiki_conversations)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# enhanced_conversation_tagger.py\n",
    "\"\"\"\n",
    "Enhanced conversation tagging system with faceting capabilities.\n",
    "Allows analysis of tag distributions across different facets (gizmos, conversation types, etc.).\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Any, List, Callable, Set, Union, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "class Tag:\n",
    "    \"\"\"Represents a tag with optional key-value attributes.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, **attributes):\n",
    "        self.name = name\n",
    "        self.attributes = attributes\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.attributes:\n",
    "            attrs_str = \", \".join(f\"{k}={v}\" for k, v in self.attributes.items())\n",
    "            return f\"{self.name}({attrs_str})\"\n",
    "        return self.name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Tag('{self.name}', {self.attributes})\"\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, str):\n",
    "            return self.name == other\n",
    "        elif isinstance(other, Tag):\n",
    "            return self.name == other.name and self.attributes == other.attributes\n",
    "        return False\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.name, tuple(sorted(self.attributes.items()))))\n",
    "    \n",
    "    def matches(self, name: str, **criteria) -> bool:\n",
    "        \"\"\"Check if tag matches name and optional attribute criteria.\"\"\"\n",
    "        if self.name != name:\n",
    "            return False\n",
    "        \n",
    "        for key, value in criteria.items():\n",
    "            if key not in self.attributes:\n",
    "                return False\n",
    "            \n",
    "            attr_value = self.attributes[key]\n",
    "            \n",
    "            # Support comparison operators\n",
    "            if isinstance(value, dict):\n",
    "                for op, target in value.items():\n",
    "                    if op == 'gt' and not (attr_value > target):\n",
    "                        return False\n",
    "                    elif op == 'gte' and not (attr_value >= target):\n",
    "                        return False\n",
    "                    elif op == 'lt' and not (attr_value < target):\n",
    "                        return False\n",
    "                    elif op == 'lte' and not (attr_value <= target):\n",
    "                        return False\n",
    "                    elif op == 'eq' and not (attr_value == target):\n",
    "                        return False\n",
    "                    elif op == 'ne' and not (attr_value != target):\n",
    "                        return False\n",
    "                    elif op == 'in' and not (attr_value in target):\n",
    "                        return False\n",
    "            else:\n",
    "                # Direct equality\n",
    "                if attr_value != value:\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "\n",
    "class ConversationTagger:\n",
    "    \"\"\"\n",
    "    Enhanced tagging system supporting structured tags with attributes and faceting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_rules: Dict[str, Callable] = {}\n",
    "        self.multi_tag_rules: Dict[str, Callable] = {}\n",
    "        self.supplemental_rules: Dict[str, Callable] = {}\n",
    "        self.rule_descriptions: Dict[str, str] = {}\n",
    "    \n",
    "    def add_base_rule(self, tag_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"Add a base tagging rule that returns bool or Tag object.\"\"\"\n",
    "        self.base_rules[tag_name] = rule_function\n",
    "        self.rule_descriptions[tag_name] = description\n",
    "    \n",
    "    def add_multi_tag_rule(self, rule_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"Add a rule that returns multiple tags (strings or Tag objects).\"\"\"\n",
    "        self.multi_tag_rules[rule_name] = rule_function\n",
    "        self.rule_descriptions[rule_name] = description\n",
    "    \n",
    "    def add_supplemental_rule(self, tag_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"Add a supplemental rule that depends on existing tags.\"\"\"\n",
    "        self.supplemental_rules[tag_name] = rule_function\n",
    "        self.rule_descriptions[tag_name] = description\n",
    "    \n",
    "    def _normalize_tag(self, tag: Union[str, Tag]) -> Tag:\n",
    "        \"\"\"Convert string tags to Tag objects.\"\"\"\n",
    "        if isinstance(tag, str):\n",
    "            return Tag(tag)\n",
    "        return tag\n",
    "    \n",
    "    def tag_conversation(self, conversation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Apply all tagging rules to a conversation.\"\"\"\n",
    "        tags = set()\n",
    "        debug_info = defaultdict(list)\n",
    "        \n",
    "        # Apply base rules\n",
    "        for tag_name, rule_func in self.base_rules.items():\n",
    "            try:\n",
    "                result = rule_func(conversation)\n",
    "                if result:\n",
    "                    if isinstance(result, bool):\n",
    "                        tag = Tag(tag_name)\n",
    "                    else:\n",
    "                        tag = self._normalize_tag(result)\n",
    "                    tags.add(tag)\n",
    "                    debug_info['applied_rules'].append(f\"BASE: {tag}\")\n",
    "                else:\n",
    "                    debug_info['skipped_rules'].append(f\"BASE: {tag_name}\")\n",
    "            except Exception as e:\n",
    "                debug_info['errors'].append(f\"BASE: {tag_name} - {str(e)}\")\n",
    "        \n",
    "        # Apply multi-tag rules\n",
    "        for rule_name, rule_func in self.multi_tag_rules.items():\n",
    "            try:\n",
    "                new_tags = rule_func(conversation)\n",
    "                if new_tags:\n",
    "                    normalized_tags = [self._normalize_tag(tag) for tag in new_tags]\n",
    "                    tags.update(normalized_tags)\n",
    "                    debug_info['applied_rules'].append(f\"MULTI: {rule_name} -> {[str(t) for t in normalized_tags]}\")\n",
    "                else:\n",
    "                    debug_info['skipped_rules'].append(f\"MULTI: {rule_name}\")\n",
    "            except Exception as e:\n",
    "                debug_info['errors'].append(f\"MULTI: {rule_name} - {str(e)}\")\n",
    "        \n",
    "        # Apply supplemental rules\n",
    "        max_iterations = 5\n",
    "        for iteration in range(max_iterations):\n",
    "            initial_tag_count = len(tags)\n",
    "            \n",
    "            for tag_name, rule_func in self.supplemental_rules.items():\n",
    "                # Check if tag already exists\n",
    "                if any(tag.name == tag_name for tag in tags):\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    result = rule_func(conversation, tags)\n",
    "                    if result:\n",
    "                        if isinstance(result, bool):\n",
    "                            tag = Tag(tag_name)\n",
    "                        else:\n",
    "                            tag = self._normalize_tag(result)\n",
    "                        tags.add(tag)\n",
    "                        debug_info['applied_rules'].append(f\"SUPP: {tag} (iter {iteration})\")\n",
    "                    else:\n",
    "                        debug_info['skipped_rules'].append(f\"SUPP: {tag_name} (iter {iteration})\")\n",
    "                except Exception as e:\n",
    "                    debug_info['errors'].append(f\"SUPP: {tag_name} - {str(e)}\")\n",
    "            \n",
    "            if len(tags) == initial_tag_count:\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'conversation_id': conversation.get('conversation_id', conversation.get('id', 'unknown')),\n",
    "            'title': conversation.get('title', 'Untitled'),\n",
    "            'tags': list(tags),\n",
    "            'debug_info': dict(debug_info),\n",
    "            'conversation': conversation\n",
    "        }\n",
    "    \n",
    "    def tag_conversations(self, conversations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Tag multiple conversations.\"\"\"\n",
    "        return [self.tag_conversation(conv) for conv in conversations]\n",
    "    \n",
    "    def filter_by_tags(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                      include_tags: List[Union[str, Dict]] = None,\n",
    "                      exclude_tags: List[Union[str, Dict]] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Filter conversations by tags with attribute support.\n",
    "        \n",
    "        Args:\n",
    "            include_tags: List of tag names or dicts with criteria\n",
    "                Examples: ['web_search', {'name': 'gizmo', 'type': 'dalle'}]\n",
    "            exclude_tags: Similar format for exclusions\n",
    "        \"\"\"\n",
    "        filtered = []\n",
    "        \n",
    "        for tagged_conv in tagged_conversations:\n",
    "            tags = tagged_conv['tags']\n",
    "            \n",
    "            # Check exclusions first\n",
    "            if exclude_tags:\n",
    "                should_exclude = False\n",
    "                for exclude_criterion in exclude_tags:\n",
    "                    if self._matches_criterion(tags, exclude_criterion):\n",
    "                        should_exclude = True\n",
    "                        break\n",
    "                if should_exclude:\n",
    "                    continue\n",
    "            \n",
    "            # Check inclusions\n",
    "            if include_tags:\n",
    "                should_include = True\n",
    "                for include_criterion in include_tags:\n",
    "                    if not self._matches_criterion(tags, include_criterion):\n",
    "                        should_include = False\n",
    "                        break\n",
    "                if not should_include:\n",
    "                    continue\n",
    "            \n",
    "            filtered.append(tagged_conv)\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def _matches_criterion(self, tags: List[Tag], criterion: Union[str, Dict]) -> bool:\n",
    "        \"\"\"Check if any tag matches the given criterion.\"\"\"\n",
    "        if isinstance(criterion, str):\n",
    "            return any(tag.name == criterion for tag in tags)\n",
    "        \n",
    "        elif isinstance(criterion, dict):\n",
    "            name = criterion.get('name')\n",
    "            if not name:\n",
    "                return False\n",
    "            \n",
    "            criteria = {k: v for k, v in criterion.items() if k != 'name'}\n",
    "            return any(tag.matches(name, **criteria) for tag in tags)\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_facet_value(self, tags: List[Tag], facet_tag_name: str, \n",
    "                       facet_attribute: Optional[str] = None) -> str:\n",
    "        \"\"\"Extract facet value from a conversation's tags.\"\"\"\n",
    "        matching_tags = [tag for tag in tags if tag.name == facet_tag_name]\n",
    "        \n",
    "        if not matching_tags:\n",
    "            return \"<none>\"\n",
    "        \n",
    "        if facet_attribute is None:\n",
    "            # Just check for presence of the tag\n",
    "            return f\"has_{facet_tag_name}\"\n",
    "        \n",
    "        # Extract specific attribute values\n",
    "        values = []\n",
    "        for tag in matching_tags:\n",
    "            if facet_attribute in tag.attributes:\n",
    "                values.append(str(tag.attributes[facet_attribute]))\n",
    "        \n",
    "        if not values:\n",
    "            return f\"<{facet_tag_name}_no_{facet_attribute}>\"\n",
    "        \n",
    "        # If multiple values, join them\n",
    "        return \"; \".join(sorted(set(values)))\n",
    "    \n",
    "    def facet_conversations(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                           facet_tag_name: str, \n",
    "                           facet_attribute: Optional[str] = None,\n",
    "                           max_facets: int = 50) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Group conversations by facet values.\n",
    "        \n",
    "        Args:\n",
    "            facet_tag_name: Tag name to facet by (e.g., 'gizmo')\n",
    "            facet_attribute: Optional attribute within tag (e.g., 'gizmo_id')\n",
    "            max_facets: Maximum number of facet values to show\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping facet values to lists of conversations\n",
    "        \"\"\"\n",
    "        facets = defaultdict(list)\n",
    "        \n",
    "        for tagged_conv in tagged_conversations:\n",
    "            facet_value = self.get_facet_value(tagged_conv['tags'], facet_tag_name, facet_attribute)\n",
    "            facets[facet_value].append(tagged_conv)\n",
    "        \n",
    "        # Sort by facet size (largest first) and limit\n",
    "        sorted_facets = dict(sorted(facets.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "        \n",
    "        if len(sorted_facets) > max_facets:\n",
    "            # Keep top facets and group rest into \"others\"\n",
    "            items = list(sorted_facets.items())\n",
    "            top_facets = dict(items[:max_facets-1])\n",
    "            \n",
    "            other_conversations = []\n",
    "            for _, conversations in items[max_facets-1:]:\n",
    "                other_conversations.extend(conversations)\n",
    "            \n",
    "            if other_conversations:\n",
    "                top_facets[\"<other>\"] = other_conversations\n",
    "            \n",
    "            return top_facets\n",
    "        \n",
    "        return sorted_facets\n",
    "    \n",
    "    def print_faceted_summary(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                             facet_tag_name: str, \n",
    "                             facet_attribute: Optional[str] = None,\n",
    "                             show_details: bool = False,\n",
    "                             max_facets: int = 20):\n",
    "        \"\"\"\n",
    "        Print tag summary broken down by facets.\n",
    "        \n",
    "        Args:\n",
    "            facet_tag_name: Tag to facet by\n",
    "            facet_attribute: Optional attribute within the tag  \n",
    "            show_details: Show detailed tag attribute info\n",
    "            max_facets: Maximum facets to show\n",
    "        \"\"\"\n",
    "        total = len(tagged_conversations)\n",
    "        facets = self.facet_conversations(tagged_conversations, facet_tag_name, facet_attribute, max_facets)\n",
    "        \n",
    "        print(f\"Tagged {total} conversations\")\n",
    "        print(f\"Faceted by: {facet_tag_name}\" + \n",
    "              (f\".{facet_attribute}\" if facet_attribute else \"\"))\n",
    "        print(f\"Found {len(facets)} facet values\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FACETED TAG SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for facet_value, facet_conversations in facets.items():\n",
    "            facet_size = len(facet_conversations)\n",
    "            facet_percentage = (facet_size / total) * 100\n",
    "            \n",
    "            print(f\"\\n FACET: {facet_value}\")\n",
    "            print(f\"    Conversations: {facet_size} ({facet_percentage:.1f}% of total)\")\n",
    "            print(f\"    {'-' * 60}\")\n",
    "            \n",
    "            # Calculate tag statistics for this facet\n",
    "            tag_counts = defaultdict(int)\n",
    "            tag_attributes = defaultdict(lambda: defaultdict(list))\n",
    "            unique_structured_tags = defaultdict(set)\n",
    "            \n",
    "            for tagged_conv in facet_conversations:\n",
    "                for tag in tagged_conv['tags']:\n",
    "                    tag_counts[tag.name] += 1\n",
    "                    \n",
    "                    # Collect attribute information\n",
    "                    for attr_name, attr_value in tag.attributes.items():\n",
    "                        if isinstance(attr_value, (int, float)):\n",
    "                            tag_attributes[tag.name][attr_name].append(attr_value)\n",
    "                        else:\n",
    "                            unique_structured_tags[tag.name].add(f\"{attr_name}={attr_value}\")\n",
    "            \n",
    "            # Sort tags for this facet and show all (no truncation)\n",
    "            sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for tag_name, count in sorted_tags:\n",
    "                percentage = (count / facet_size) * 100\n",
    "                print(f\"    {tag_name}: {count} ({percentage:.1f}%)\")\n",
    "                \n",
    "                if show_details:\n",
    "                    # Show numeric attribute statistics\n",
    "                    if tag_name in tag_attributes:\n",
    "                        for attr_name, values in tag_attributes[tag_name].items():\n",
    "                            if values:\n",
    "                                avg_val = sum(values) / len(values)\n",
    "                                min_val = min(values)\n",
    "                                max_val = max(values)\n",
    "                                print(f\"        {attr_name}: avg={avg_val:.1f}, range=[{min_val}, {max_val}]\")\n",
    "                    \n",
    "                    # Show unique structured values\n",
    "                    if tag_name in unique_structured_tags:\n",
    "                        unique_vals = sorted(unique_structured_tags[tag_name])\n",
    "                        if len(unique_vals) <= 5:\n",
    "                            print(f\"        values: {', '.join(unique_vals)}\")\n",
    "                        else:\n",
    "                            print(f\"        values: {', '.join(unique_vals[:5])} ... (+{len(unique_vals)-5} more)\")\n",
    "    \n",
    "    def compare_facets(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                      facet_tag_name: str, \n",
    "                      facet_attribute: Optional[str] = None,\n",
    "                      comparison_tags: List[str] = None,\n",
    "                      min_facet_size: int = 10) -> None:\n",
    "        \"\"\"\n",
    "        Compare specific tags across facets.\n",
    "        \n",
    "        Args:\n",
    "            comparison_tags: List of tags to compare across facets\n",
    "            min_facet_size: Minimum facet size to include in comparison\n",
    "        \"\"\"\n",
    "        facets = self.facet_conversations(tagged_conversations, facet_tag_name, facet_attribute)\n",
    "        \n",
    "        # Filter facets by minimum size\n",
    "        large_facets = {k: v for k, v in facets.items() if len(v) >= min_facet_size}\n",
    "        \n",
    "        if not large_facets:\n",
    "            print(f\"No facets with at least {min_facet_size} conversations found\")\n",
    "            return\n",
    "        \n",
    "        # If no specific tags provided, use most common tags overall\n",
    "        if comparison_tags is None:\n",
    "            overall_tag_counts = defaultdict(int)\n",
    "            for tagged_conv in tagged_conversations:\n",
    "                for tag in tagged_conv['tags']:\n",
    "                    overall_tag_counts[tag.name] += 1\n",
    "            \n",
    "            # Get top 10 most common tags\n",
    "            comparison_tags = [tag for tag, _ in \n",
    "                             sorted(overall_tag_counts.items(), key=lambda x: x[1], reverse=True)[:10]]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FACET COMPARISON\")\n",
    "        print(f\"Comparing tags: {', '.join(comparison_tags)}\")\n",
    "        print(f\"Across facets: {facet_tag_name}\" + \n",
    "              (f\".{facet_attribute}\" if facet_attribute else \"\"))\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Calculate percentages for each tag in each facet\n",
    "        results = {}\n",
    "        for facet_value, facet_conversations in large_facets.items():\n",
    "            facet_size = len(facet_conversations)\n",
    "            facet_tag_counts = defaultdict(int)\n",
    "            \n",
    "            for tagged_conv in facet_conversations:\n",
    "                for tag in tagged_conv['tags']:\n",
    "                    facet_tag_counts[tag.name] += 1\n",
    "            \n",
    "            results[facet_value] = {\n",
    "                'size': facet_size,\n",
    "                'percentages': {tag: (facet_tag_counts[tag] / facet_size) * 100 \n",
    "                               for tag in comparison_tags}\n",
    "            }\n",
    "        \n",
    "        # Print comparison table\n",
    "        print(f\"\\n{'Facet':<30} {'Size':<8} \" + \n",
    "              \"\".join(f\"{tag:<15}\" for tag in comparison_tags))\n",
    "        print(\"-\" * (30 + 8 + 15 * len(comparison_tags)))\n",
    "        \n",
    "        for facet_value, data in results.items():\n",
    "            facet_display = facet_value[:28] + \"..\" if len(facet_value) > 30 else facet_value\n",
    "            row = f\"{facet_display:<30} {data['size']:<8} \"\n",
    "            row += \"\".join(f\"{data['percentages'][tag]:<15.1f}\" for tag in comparison_tags)\n",
    "            print(row)\n",
    "        \n",
    "        # Highlight interesting differences\n",
    "        print(f\"\\n NOTABLE DIFFERENCES:\")\n",
    "        for tag in comparison_tags:\n",
    "            percentages = [results[facet]['percentages'][tag] for facet in results.keys()]\n",
    "            if max(percentages) - min(percentages) > 20:  # 20% difference threshold\n",
    "                max_facet = max(results.keys(), key=lambda f: results[f]['percentages'][tag])\n",
    "                min_facet = min(results.keys(), key=lambda f: results[f]['percentages'][tag])\n",
    "                print(f\"    {tag}: {max_facet} ({results[max_facet]['percentages'][tag]:.1f}%) vs \" +\n",
    "                      f\"{min_facet} ({results[min_facet]['percentages'][tag]:.1f}%)\")\n",
    "    \n",
    "    def get_tag_values(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                      tag_name: str, attribute: str) -> List[Any]:\n",
    "        \"\"\"Extract attribute values from tags across conversations.\"\"\"\n",
    "        values = []\n",
    "        for tagged_conv in tagged_conversations:\n",
    "            for tag in tagged_conv['tags']:\n",
    "                if tag.name == tag_name and attribute in tag.attributes:\n",
    "                    values.append(tag.attributes[attribute])\n",
    "        return values\n",
    "    \n",
    "    def print_summary(self, tagged_conversations: List[Dict[str, Any]], show_details: bool = True):\n",
    "        \"\"\"Print comprehensive summary with all tag types and optional details.\"\"\"\n",
    "        total = len(tagged_conversations)\n",
    "        tag_counts = defaultdict(int)\n",
    "        tag_attributes = defaultdict(lambda: defaultdict(list))\n",
    "        unique_structured_tags = defaultdict(set)\n",
    "        \n",
    "        # Collect all tag information\n",
    "        for tagged_conv in tagged_conversations:\n",
    "            for tag in tagged_conv['tags']:\n",
    "                tag_counts[tag.name] += 1\n",
    "                \n",
    "                # Collect attribute information\n",
    "                for attr_name, attr_value in tag.attributes.items():\n",
    "                    if isinstance(attr_value, (int, float)):\n",
    "                        tag_attributes[tag.name][attr_name].append(attr_value)\n",
    "                    else:\n",
    "                        # For non-numeric attributes, track unique values\n",
    "                        unique_structured_tags[tag.name].add(f\"{attr_name}={attr_value}\")\n",
    "        \n",
    "        print(f\"Tagged {total} conversations\")\n",
    "        print(f\"\\n=== TAG SUMMARY ===\")\n",
    "        \n",
    "        # Sort tags by frequency for better readability\n",
    "        sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for tag_name, count in sorted_tags:\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"{tag_name}: {count} ({percentage:.1f}%)\")\n",
    "            \n",
    "            if show_details:\n",
    "                # Show numeric attribute statistics\n",
    "                if tag_name in tag_attributes:\n",
    "                    for attr_name, values in tag_attributes[tag_name].items():\n",
    "                        if values:\n",
    "                            avg_val = sum(values) / len(values)\n",
    "                            min_val = min(values)\n",
    "                            max_val = max(values)\n",
    "                            print(f\"    {attr_name}: avg={avg_val:.1f}, range=[{min_val}, {max_val}]\")\n",
    "                \n",
    "                # Show unique structured values for non-numeric attributes\n",
    "                if tag_name in unique_structured_tags:\n",
    "                    unique_vals = sorted(unique_structured_tags[tag_name])\n",
    "                    if len(unique_vals) <= 10:  # Show all if not too many\n",
    "                        print(f\"    values: {', '.join(unique_vals)}\")\n",
    "                    else:  # Show top 10 most common\n",
    "                        print(f\"    values: {', '.join(unique_vals[:10])} ... (+{len(unique_vals)-10} more)\")\n",
    "\n",
    "\n",
    "# Example usage functions to demonstrate the faceting capabilities\n",
    "\n",
    "def demo_faceting_usage():\n",
    "    \"\"\"\n",
    "    Demonstrate how to use the new faceting capabilities and improved code detection.\n",
    "    \"\"\"\n",
    "    print(\"\"\"\n",
    "# FACETING USAGE EXAMPLES\n",
    "\n",
    "# 1. Facet by gizmo presence (with/without gizmos)\n",
    "tagger.print_faceted_summary(tagged_results, 'gizmo')\n",
    "\n",
    "# 2. Facet by specific gizmo IDs (now without truncation)\n",
    "tagger.print_faceted_summary(tagged_results, 'gizmo', 'gizmo_id')\n",
    "\n",
    "# 3. Facet by conversation length categories\n",
    "tagger.print_faceted_summary(tagged_results, 'conversation_length', 'category')\n",
    "\n",
    "# 4. Facet by prompt length categories\n",
    "tagger.print_faceted_summary(tagged_results, 'prompt_stats', 'length_category')\n",
    "\n",
    "# 5. Facet by prompt consistency patterns\n",
    "tagger.print_faceted_summary(tagged_results, 'prompt_stats', 'consistency')\n",
    "\n",
    "# 6. Compare coding patterns across gizmos (with improved detection)\n",
    "tagger.compare_facets(tagged_results, 'gizmo', 'gizmo_id', \n",
    "                     comparison_tags=['definite_coding_assistance', 'strict_code_patterns', \n",
    "                                    'documentation_generation', 'wiki_documentation'])\n",
    "\n",
    "# 7. Compare old vs new code detection\n",
    "tagger.compare_facets(tagged_results, 'gizmo', 'gizmo_id',\n",
    "                     comparison_tags=['code_patterns', 'strict_code_patterns', \n",
    "                                    'coding_assistance', 'definite_coding_assistance'])\n",
    "\n",
    "# 8. Investigate specific gizmo patterns\n",
    "tagger.print_faceted_summary(tagged_results, 'gizmo', 'gizmo_id', show_details=True)\n",
    "\n",
    "# 9. Focus on documentation vs coding patterns\n",
    "tagger.compare_facets(tagged_results, 'wiki_documentation', None,\n",
    "                     comparison_tags=['code_patterns', 'strict_code_patterns', 'large_content'])\n",
    "\n",
    "# 10. Multi-level analysis: documentation generation within each gizmo\n",
    "for gizmo_id in ['g-IibMsD7w8', 'g-KpF6lTka3', 'g-QsUj0Smzg']:  # Replace with actual IDs\n",
    "    gizmo_conversations = tagger.filter_by_tags(\n",
    "        tagged_results, \n",
    "        include_tags=[{'name': 'gizmo', 'gizmo_id': gizmo_id}]\n",
    "    )\n",
    "    if gizmo_conversations:\n",
    "        print(f\"\\\\n=== CONTENT ANALYSIS FOR GIZMO {gizmo_id} ===\")\n",
    "        tagger.print_faceted_summary(gizmo_conversations, 'wiki_documentation')\n",
    "\n",
    "# 11. Debug improved code detection for wiki gizmo\n",
    "wiki_gizmo_conversations = tagger.filter_by_tags(\n",
    "    tagged_results,\n",
    "    include_tags=[{'name': 'gizmo', 'gizmo_id': 'g-IibMsD7w8'}]  # Replace with actual wiki gizmo ID\n",
    ")\n",
    "\n",
    "print(\"\\\\n=== WIKI GIZMO CODE DETECTION ANALYSIS ===\")\n",
    "print(\"Before improvements vs After improvements:\")\n",
    "\n",
    "old_code_count = len([c for c in wiki_gizmo_conversations \n",
    "                     if any(t.name == 'code_patterns' for t in c['tags'])])\n",
    "new_strict_count = len([c for c in wiki_gizmo_conversations \n",
    "                       if any(t.name == 'strict_code_patterns' for t in c['tags'])])\n",
    "wiki_doc_count = len([c for c in wiki_gizmo_conversations \n",
    "                     if any(t.name == 'wiki_documentation' for t in c['tags'])])\n",
    "\n",
    "print(f\"Original code_patterns detection: {old_code_count} conversations\")\n",
    "print(f\"Improved strict_code_patterns: {new_strict_count} conversations\") \n",
    "print(f\"Wiki/documentation detection: {wiki_doc_count} conversations\")\n",
    "print(f\"Reduction in false positives: {old_code_count - new_strict_count} conversations\")\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def debug_code_detection_improvements():\n",
    "    \"\"\"\n",
    "    Debug function to compare old vs new code detection on specific conversations.\n",
    "    \"\"\"\n",
    "    print(\"\"\"\n",
    "# DEBUGGING CODE DETECTION IMPROVEMENTS\n",
    "\n",
    "# Test improved detection on wiki gizmo conversations\n",
    "def test_detection_on_sample(conversations, sample_size=10):\n",
    "    for i, conv in enumerate(conversations[:sample_size]):\n",
    "        print(f\"\\\\n--- Conversation {i+1}: {conv.get('title', 'Untitled')[:50]}... ---\")\n",
    "        \n",
    "        # Test old detection\n",
    "        old_result = has_code_patterns_old(conv)  # You'd need to save the old function\n",
    "        \n",
    "        # Test new detection  \n",
    "        new_result = has_code_patterns(conv)\n",
    "        strict_result = has_strict_code_patterns(conv)\n",
    "        wiki_result = has_wiki_documentation_patterns(conv)\n",
    "        \n",
    "        print(f\"Old code detection: {old_result}\")\n",
    "        print(f\"New code detection: {new_result}\")\n",
    "        print(f\"Strict code detection: {strict_result}\")\n",
    "        print(f\"Wiki documentation: {wiki_result}\")\n",
    "        \n",
    "        if old_result != new_result:\n",
    "            print(\" DETECTION CHANGED!\")\n",
    "            \n",
    "        if old_result and wiki_result:\n",
    "            print(\" Likely false positive caught by wiki detection\")\n",
    "\n",
    "# Usage:\n",
    "# wiki_conversations = tagger.filter_by_tags(tagged_results, \n",
    "#                                           include_tags=[{'name': 'gizmo', 'gizmo_id': 'g-IibMsD7w8'}])\n",
    "# test_detection_on_sample(wiki_conversations)\n",
    "    \"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_faceting_usage()\n",
    "    debug_code_detection_improvements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a670de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enhanced rule functions for all previously discussed tagging rules\n",
    "\n",
    "def get_all_user_messages(conversation: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get all user messages in chronological order.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    user_messages = []\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        author = message.get('author', {})\n",
    "        if author.get('role') == 'user':\n",
    "            create_time = message.get('create_time') or 0\n",
    "            user_messages.append((create_time, message))\n",
    "    \n",
    "    user_messages.sort(key=lambda x: x[0])\n",
    "    return [msg for _, msg in user_messages]\n",
    "\n",
    "\n",
    "def get_first_user_message(conversation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Find the first user message in the conversation.\"\"\"\n",
    "    user_messages = get_all_user_messages(conversation)\n",
    "    return user_messages[0] if user_messages else None\n",
    "\n",
    "\n",
    "def create_conversation_length_tag(conversation: Dict[str, Any]) -> Tag:\n",
    "    \"\"\"Create structured tag for conversation length.\"\"\"\n",
    "    user_count = len(get_all_user_messages(conversation))\n",
    "    \n",
    "    # Determine category\n",
    "    if user_count == 1:\n",
    "        category = 'single'\n",
    "    elif user_count <= 3:\n",
    "        category = 'short'\n",
    "    elif user_count <= 10:\n",
    "        category = 'medium'\n",
    "    elif user_count <= 25:\n",
    "        category = 'long'\n",
    "    else:\n",
    "        category = 'very_long'\n",
    "    \n",
    "    return Tag('conversation_length', count=user_count, category=category)\n",
    "\n",
    "\n",
    "def create_prompt_stats_tag(conversation: Dict[str, Any]) -> Tag:\n",
    "    \"\"\"Create structured tag for prompt statistics.\"\"\"\n",
    "    user_messages = get_all_user_messages(conversation)\n",
    "    \n",
    "    if not user_messages:\n",
    "        return Tag('prompt_stats', count=0, mean=0, median=0, variance=0, \n",
    "                  length_category='none', consistency='none')\n",
    "    \n",
    "    # Calculate message lengths\n",
    "    lengths = []\n",
    "    for message in user_messages:\n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        lengths.append(len(all_text))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_length = sum(lengths) / len(lengths)\n",
    "    sorted_lengths = sorted(lengths)\n",
    "    n = len(sorted_lengths)\n",
    "    median_length = (sorted_lengths[n//2] if n % 2 == 1 \n",
    "                    else (sorted_lengths[n//2-1] + sorted_lengths[n//2]) / 2)\n",
    "    variance = sum((x - mean_length) ** 2 for x in lengths) / len(lengths) if len(lengths) > 1 else 0\n",
    "    \n",
    "    # Determine categories\n",
    "    if mean_length < 50:\n",
    "        length_category = 'very_short'\n",
    "    elif mean_length < 200:\n",
    "        length_category = 'short'\n",
    "    elif mean_length < 1000:\n",
    "        length_category = 'medium'\n",
    "    elif mean_length < 3000:\n",
    "        length_category = 'long'\n",
    "    else:\n",
    "        length_category = 'very_long'\n",
    "    \n",
    "    if variance < 1000:\n",
    "        consistency = 'consistent'\n",
    "    elif variance < 10000:\n",
    "        consistency = 'mixed'\n",
    "    else:\n",
    "        consistency = 'variable'\n",
    "    \n",
    "    return Tag('prompt_stats', \n",
    "               count=len(lengths),\n",
    "               mean=round(mean_length, 1),\n",
    "               median=round(median_length, 1),\n",
    "               variance=round(variance, 1),\n",
    "               length_category=length_category,\n",
    "               consistency=consistency)\n",
    "\n",
    "\n",
    "def create_gizmo_plugin_tags(conversation: Dict[str, Any]) -> List[Tag]:\n",
    "    \"\"\"Create structured tags for gizmos and plugins.\"\"\"\n",
    "    tags = []\n",
    "    gizmos = set()\n",
    "    plugins = set()\n",
    "    \n",
    "    # Check conversation-level\n",
    "    if conversation.get('gizmo_id'):\n",
    "        gizmos.add(conversation['gizmo_id'])\n",
    "    \n",
    "    plugin_ids = conversation.get('plugin_ids', [])\n",
    "    if plugin_ids:\n",
    "        plugins.update(plugin_ids)\n",
    "    \n",
    "    # Check message-level\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        \n",
    "        # Invoked plugins\n",
    "        invoked_plugin = metadata.get('invoked_plugin', {})\n",
    "        if invoked_plugin:\n",
    "            if invoked_plugin.get('plugin_id'):\n",
    "                plugins.add(invoked_plugin['plugin_id'])\n",
    "            if invoked_plugin.get('namespace'):\n",
    "                plugins.add(invoked_plugin['namespace'])\n",
    "        \n",
    "        # Gizmo usage\n",
    "        if metadata.get('gizmo_id'):\n",
    "            gizmos.add(metadata['gizmo_id'])\n",
    "    \n",
    "    # Create tags - FIX: Use different attribute name to avoid conflict\n",
    "    for gizmo in gizmos:\n",
    "        tags.append(Tag('gizmo', gizmo_id=gizmo))\n",
    "    \n",
    "    for plugin in plugins:\n",
    "        tags.append(Tag('plugin', plugin_id=plugin))\n",
    "    \n",
    "    return tags\n",
    "\n",
    "\n",
    "# Boolean rule functions for basic content analysis\n",
    "def has_large_content(conversation: Dict[str, Any], min_length: int = 2000) -> bool:\n",
    "    \"\"\"Check if conversation has unusually large content anywhere.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        if len(text) > min_length:\n",
    "            return True\n",
    "            \n",
    "        parts = content.get('parts', [])\n",
    "        for part in parts:\n",
    "            if isinstance(part, str) and len(part) > min_length:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_code_patterns(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for clear code patterns anywhere in conversation.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "        \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        \n",
    "        # Strong code indicators\n",
    "        code_indicators = [\n",
    "            '```',  # Code blocks\n",
    "            'def ', 'function ', 'class ',  # Function/class definitions\n",
    "            'import ', 'from ', 'require(',  # Import statements\n",
    "            '#!/bin/', '#include', 'using namespace',  # Script headers\n",
    "        ]\n",
    "        \n",
    "        if any(indicator in all_text for indicator in code_indicators):\n",
    "            return True\n",
    "            \n",
    "        # Also check for high density of coding keywords\n",
    "        coding_keywords = ['function', 'class', 'import', 'def ', 'const ', 'let ', 'var ', 'return', 'if ', 'for ', 'while ']\n",
    "        keyword_count = sum(1 for keyword in coding_keywords if keyword in all_text.lower())\n",
    "        if len(all_text) > 1000 and keyword_count >= 3:  # Multiple coding keywords in large text suggest actual code\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_github_repos(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if GitHub repositories were selected for context.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        repos = metadata.get('selected_github_repos', [])\n",
    "        if repos:  # Non-empty list\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_canvas_operations(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for canvas/document operations.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if metadata.get('canvas'):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_web_search(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for web search operations.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if (metadata.get('search_queries') or \n",
    "            metadata.get('search_result_groups') or\n",
    "            metadata.get('content_references')):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_reasoning_thoughts(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for reasoning/thinking patterns.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        content = message.get('content', {})\n",
    "        if content.get('thoughts'):  # Reasoning thoughts\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_code_execution(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for code execution artifacts.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if (metadata.get('aggregate_result') or \n",
    "            metadata.get('jupyter_messages')):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "# First user message specific rules\n",
    "def first_user_has_large_content(conversation: Dict[str, Any], min_length: int = 2000) -> bool:\n",
    "    \"\"\"Check if the first user message has large content.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    content = first_message.get('content', {})\n",
    "    text = content.get('text', '')\n",
    "    if len(text) > min_length:\n",
    "        return True\n",
    "        \n",
    "    parts = content.get('parts', [])\n",
    "    for part in parts:\n",
    "        if isinstance(part, str) and len(part) > min_length:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def first_user_has_code_patterns(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message contains code patterns.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    content = first_message.get('content', {})\n",
    "    text = content.get('text', '')\n",
    "    parts = content.get('parts', [])\n",
    "    all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "    \n",
    "    # Strong code indicators\n",
    "    code_indicators = [\n",
    "        '```',  # Code blocks\n",
    "        'def ', 'function ', 'class ',  # Definitions\n",
    "        'import ', 'from ', 'require(',  # Imports\n",
    "        '#!/bin/', '#include',  # Script headers\n",
    "    ]\n",
    "    \n",
    "    return any(indicator in all_text for indicator in code_indicators)\n",
    "\n",
    "\n",
    "def first_user_has_attachments(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message has attachments.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    metadata = first_message.get('metadata', {})\n",
    "    attachments = metadata.get('attachments', [])\n",
    "    return len(attachments) > 0\n",
    "\n",
    "\n",
    "def first_user_has_code_attachments(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message has code-related attachments.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    metadata = first_message.get('metadata', {})\n",
    "    attachments = metadata.get('attachments', [])\n",
    "    \n",
    "    for attachment in attachments:\n",
    "        mime_type = attachment.get('mime_type', '').lower()\n",
    "        name = attachment.get('name', '').lower()\n",
    "        \n",
    "        # Check for code file extensions\n",
    "        code_extensions = ['.py', '.js', '.java', '.cpp', '.c', '.go', '.rs', '.ts', '.jsx', '.tsx', '.sql', '.sh', '.rb', '.php']\n",
    "        if any(ext in name for ext in code_extensions):\n",
    "            return True\n",
    "            \n",
    "        # Check for code-related MIME types\n",
    "        code_mimes = ['text/x-python', 'text/x-java', 'application/javascript', 'text/x-script']\n",
    "        if any(mime in mime_type for mime in code_mimes):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def create_default_tagger() -> ConversationTagger:\n",
    "    \"\"\"Create a tagger with all previously discussed rules in the enhanced structured framework.\"\"\"\n",
    "    tagger = ConversationTagger()\n",
    "    \n",
    "    # ===== BASIC CONTENT ANALYSIS RULES (Boolean) =====\n",
    "    tagger.add_base_rule(\n",
    "        'large_content', \n",
    "        lambda conv: has_large_content(conv, 2000),\n",
    "        'Content longer than 2000 characters anywhere in conversation'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'code_patterns', \n",
    "        has_code_patterns,\n",
    "        'Contains clear code patterns (```, def, function, etc.)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'github_context',\n",
    "        has_github_repos,\n",
    "        'GitHub repositories selected for context'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'canvas_operations',\n",
    "        has_canvas_operations,\n",
    "        'Uses canvas/document features'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'web_search',\n",
    "        has_web_search,\n",
    "        'Includes web search functionality'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'reasoning',\n",
    "        has_reasoning_thoughts,\n",
    "        'Contains reasoning/thinking content'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'code_execution',\n",
    "        has_code_execution,\n",
    "        'Contains code execution (Jupyter, aggregate results)'\n",
    "    )\n",
    "    \n",
    "    # ===== FIRST USER MESSAGE RULES (Boolean) =====\n",
    "    tagger.add_base_rule(\n",
    "        'starts_large_content',\n",
    "        lambda conv: first_user_has_large_content(conv, 2000),\n",
    "        'First user message has large content (>2000 chars)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_code_patterns',\n",
    "        first_user_has_code_patterns,\n",
    "        'First user message contains code patterns'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_with_attachments',\n",
    "        first_user_has_attachments,\n",
    "        'First user message has any attachments'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_code_attachments',\n",
    "        first_user_has_code_attachments,\n",
    "        'First user message has code-related attachments'\n",
    "    )\n",
    "    \n",
    "    # ===== STRUCTURED TAG RULES =====\n",
    "    tagger.add_base_rule(\n",
    "        'conversation_length',\n",
    "        create_conversation_length_tag,\n",
    "        'Conversation length with count and category'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'prompt_stats',\n",
    "        create_prompt_stats_tag,\n",
    "        'User message statistics (length, variance, etc.)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_multi_tag_rule(\n",
    "        'gizmo_plugin_usage',\n",
    "        create_gizmo_plugin_tags,\n",
    "        'Specific gizmos and plugins used in conversation'\n",
    "    )\n",
    "    \n",
    "    # ===== SUPPLEMENTAL RULES (Based on existing tags) =====\n",
    "    \n",
    "    # Coding assistance detection\n",
    "    tagger.add_supplemental_rule(\n",
    "        'coding_assistance',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['code_patterns', 'github_context', 'code_execution'] for tag in tags)\n",
    "        ),\n",
    "        'Likely coding assistance (code patterns, GitHub, or execution)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'coding_assistance_start',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['starts_code_patterns', 'starts_large_content', \n",
    "                           'starts_with_attachments', 'starts_code_attachments'] for tag in tags)\n",
    "        ),\n",
    "        'Likely coding assistance based on how conversation starts'\n",
    "    )\n",
    "    \n",
    "    # Research and analysis patterns  \n",
    "    tagger.add_supplemental_rule(\n",
    "        'research_session',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'web_search' for tag in tags) and \n",
    "            any(tag.name == 'large_content' for tag in tags)\n",
    "        ),\n",
    "        'Research session (web search + substantial content)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'complex_analysis',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'reasoning' for tag in tags) and \n",
    "            len([tag for tag in tags if tag.name in ['web_search', 'large_content', 'canvas_operations']]) >= 2\n",
    "        ),\n",
    "        'Complex analysis (reasoning + multiple advanced features)'\n",
    "    )\n",
    "    \n",
    "    # Context and interaction patterns\n",
    "    tagger.add_supplemental_rule(\n",
    "        'context_heavy_start',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['starts_large_content', 'starts_with_attachments'] for tag in tags)\n",
    "        ),\n",
    "        'Conversation starts with substantial context'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'enhanced_conversation',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['gizmo', 'plugin'] for tag in tags)\n",
    "        ),\n",
    "        'Uses enhanced features (gizmos or plugins)'\n",
    "    )\n",
    "    \n",
    "    # Length-based classifications using structured tags\n",
    "    tagger.add_supplemental_rule(\n",
    "        'brief_interaction',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('category') in ['single', 'short'] for tag in tags)\n",
    "        ),\n",
    "        'Brief interaction (1-3 user messages)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'extended_conversation',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('category') in ['long', 'very_long'] for tag in tags)\n",
    "        ),\n",
    "        'Extended conversation (11+ user messages)'\n",
    "    )\n",
    "    \n",
    "    # Prompt pattern classifications using structured tags\n",
    "    tagger.add_supplemental_rule(\n",
    "        'long_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('length_category') in ['long', 'very_long'] for tag in tags)\n",
    "        ),\n",
    "        'Conversation has consistently long prompts'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'short_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('length_category') in ['very_short', 'short'] for tag in tags)\n",
    "        ),\n",
    "        'Conversation has consistently short prompts'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'consistent_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'consistent' for tag in tags)\n",
    "        ),\n",
    "        'User prompts are consistent in length'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'variable_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'variable' for tag in tags)\n",
    "        ),\n",
    "        'User prompts vary significantly in length'\n",
    "    )\n",
    "    \n",
    "    # Combined patterns for specific use cases\n",
    "    tagger.add_supplemental_rule(\n",
    "        'context_dump',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('category') in ['single', 'short'] for tag in tags) and\n",
    "            any(tag.name in ['starts_large_content', 'starts_with_attachments'] for tag in tags)\n",
    "        ),\n",
    "        'Short conversation starting with large context (likely context dump)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'interactive_session',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('count', 0) >= 5 for tag in tags) and\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'consistent' for tag in tags)\n",
    "        ),\n",
    "        'Extended back-and-forth conversation with consistent prompt style'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'evolving_discussion',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('count', 0) >= 5 for tag in tags) and\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'variable' for tag in tags)\n",
    "        ),\n",
    "        'Extended conversation where prompt style evolves'\n",
    "    )\n",
    "    \n",
    "    return tagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76720992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged 1673 conversations\n",
      "\n",
      "=== TAG SUMMARY ===\n",
      "prompt_stats: 1673 (100.0%)\n",
      "    count: avg=7.6, range=[1, 160]\n",
      "    mean: avg=1142.4, range=[1.0, 184662.5]\n",
      "    median: avg=814.2, range=[1.0, 184662.5]\n",
      "    variance: avg=51563065.2, range=[0, 34065162056.2]\n",
      "    values: consistency=consistent, consistency=mixed, consistency=variable, length_category=long, length_category=medium, length_category=short, length_category=very_long, length_category=very_short\n",
      "conversation_length: 1673 (100.0%)\n",
      "    count: avg=7.6, range=[1, 160]\n",
      "    values: category=long, category=medium, category=short, category=single, category=very_long\n",
      "coding_assistance: 1551 (92.7%)\n",
      "code_patterns: 1546 (92.4%)\n",
      "large_content: 1362 (81.4%)\n",
      "short_prompts: 1261 (75.4%)\n",
      "brief_interaction: 906 (54.2%)\n",
      "consistent_prompts: 896 (53.6%)\n",
      "enhanced_conversation: 681 (40.7%)\n",
      "gizmo: 676 (40.4%)\n",
      "    values: gizmo_id=g-IibMsD7w8, gizmo_id=g-KpF6lTka3, gizmo_id=g-QsUj0Smzg, gizmo_id=g-WiEAUBGzb, gizmo_id=g-bWPVPw7oK, gizmo_id=g-pYtHuQdGh\n",
      "variable_prompts: 405 (24.2%)\n",
      "extended_conversation: 304 (18.2%)\n",
      "coding_assistance_start: 260 (15.5%)\n",
      "evolving_discussion: 247 (14.8%)\n",
      "starts_code_patterns: 240 (14.3%)\n",
      "interactive_session: 184 (11.0%)\n",
      "long_prompts: 151 (9.0%)\n",
      "context_heavy_start: 127 (7.6%)\n",
      "starts_large_content: 119 (7.1%)\n",
      "web_search: 66 (3.9%)\n",
      "context_dump: 64 (3.8%)\n",
      "research_session: 53 (3.2%)\n",
      "code_execution: 26 (1.6%)\n",
      "reasoning: 12 (0.7%)\n",
      "plugin: 11 (0.7%)\n",
      "    values: plugin_id=AskTheCode, plugin_id=chatgpt_production_alltrails_com__jit_plugin, plugin_id=chatwithvideo, plugin_id=code_repo_interaction, plugin_id=g-eaf0e78bb71845ae7b86a9268d1374c168f5bd85, plugin_id=plugin-25ce675b-c2c4-4460-ad33-8e641653498c, plugin_id=plugin-3cf29a7b-2fcb-42aa-b762-4d418b543a8b, plugin_id=plugin-b82d4d0b-79b8-4548-b1b4-65b218376d6f, plugin_id=plugin-e5009c08-c6c8-4195-977f-16f39a7d3b7b, plugin_id=scholar_assist\n",
      "starts_with_attachments: 8 (0.5%)\n",
      "complex_analysis: 5 (0.3%)\n",
      "canvas_operations: 3 (0.2%)\n",
      "Tagged 1673 conversations\n",
      "Faceted by: gizmo.gizmo_id\n",
      "Found 7 facet values\n",
      "\n",
      "================================================================================\n",
      "FACETED TAG SUMMARY\n",
      "================================================================================\n",
      "\n",
      " FACET: <none>\n",
      "    Conversations: 997 (59.6% of total)\n",
      "    ------------------------------------------------------------\n",
      "    prompt_stats: 997 (100.0%)\n",
      "    conversation_length: 997 (100.0%)\n",
      "    coding_assistance: 891 (89.4%)\n",
      "    code_patterns: 886 (88.9%)\n",
      "    large_content: 696 (69.8%)\n",
      "    short_prompts: 627 (62.9%)\n",
      "    brief_interaction: 606 (60.8%)\n",
      "    consistent_prompts: 427 (42.8%)\n",
      "    variable_prompts: 330 (33.1%)\n",
      "    coding_assistance_start: 236 (23.7%)\n",
      "    starts_code_patterns: 216 (21.7%)\n",
      "    evolving_discussion: 191 (19.2%)\n",
      "    long_prompts: 139 (13.9%)\n",
      "    context_heavy_start: 119 (11.9%)\n",
      "    extended_conversation: 112 (11.2%)\n",
      "    starts_large_content: 111 (11.1%)\n",
      "    context_dump: 60 (6.0%)\n",
      "    web_search: 54 (5.4%)\n",
      "    research_session: 41 (4.1%)\n",
      "    code_execution: 26 (2.6%)\n",
      "    interactive_session: 18 (1.8%)\n",
      "    reasoning: 12 (1.2%)\n",
      "    plugin: 9 (0.9%)\n",
      "    starts_with_attachments: 8 (0.8%)\n",
      "    complex_analysis: 5 (0.5%)\n",
      "    enhanced_conversation: 5 (0.5%)\n",
      "    canvas_operations: 3 (0.3%)\n",
      "\n",
      " FACET: g-IibMsD7w8\n",
      "    Conversations: 668 (39.9% of total)\n",
      "    ------------------------------------------------------------\n",
      "    gizmo: 668 (100.0%)\n",
      "    prompt_stats: 668 (100.0%)\n",
      "    enhanced_conversation: 668 (100.0%)\n",
      "    conversation_length: 668 (100.0%)\n",
      "    large_content: 664 (99.4%)\n",
      "    code_patterns: 656 (98.2%)\n",
      "    coding_assistance: 656 (98.2%)\n",
      "    short_prompts: 628 (94.0%)\n",
      "    consistent_prompts: 463 (69.3%)\n",
      "    brief_interaction: 294 (44.0%)\n",
      "    extended_conversation: 191 (28.6%)\n",
      "    interactive_session: 166 (24.9%)\n",
      "    variable_prompts: 74 (11.1%)\n",
      "    evolving_discussion: 55 (8.2%)\n",
      "    coding_assistance_start: 23 (3.4%)\n",
      "    starts_code_patterns: 23 (3.4%)\n",
      "    long_prompts: 12 (1.8%)\n",
      "    research_session: 11 (1.6%)\n",
      "    web_search: 11 (1.6%)\n",
      "    starts_large_content: 8 (1.2%)\n",
      "    context_heavy_start: 8 (1.2%)\n",
      "    context_dump: 4 (0.6%)\n",
      "\n",
      " FACET: g-bWPVPw7oK\n",
      "    Conversations: 4 (0.2% of total)\n",
      "    ------------------------------------------------------------\n",
      "    gizmo: 4 (100.0%)\n",
      "    short_prompts: 4 (100.0%)\n",
      "    prompt_stats: 4 (100.0%)\n",
      "    enhanced_conversation: 4 (100.0%)\n",
      "    conversation_length: 4 (100.0%)\n",
      "    brief_interaction: 3 (75.0%)\n",
      "    consistent_prompts: 3 (75.0%)\n",
      "    coding_assistance: 2 (50.0%)\n",
      "    code_patterns: 2 (50.0%)\n",
      "    extended_conversation: 1 (25.0%)\n",
      "\n",
      " FACET: g-WiEAUBGzb\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    code_patterns: 1 (100.0%)\n",
      "    coding_assistance: 1 (100.0%)\n",
      "    brief_interaction: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    large_content: 1 (100.0%)\n",
      "    short_prompts: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "    consistent_prompts: 1 (100.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "\n",
      " FACET: g-pYtHuQdGh\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    code_patterns: 1 (100.0%)\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    coding_assistance: 1 (100.0%)\n",
      "    coding_assistance_start: 1 (100.0%)\n",
      "    research_session: 1 (100.0%)\n",
      "    starts_code_patterns: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "    evolving_discussion: 1 (100.0%)\n",
      "    web_search: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "    variable_prompts: 1 (100.0%)\n",
      "    large_content: 1 (100.0%)\n",
      "\n",
      " FACET: g-QsUj0Smzg\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    brief_interaction: 1 (100.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "    short_prompts: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "    consistent_prompts: 1 (100.0%)\n",
      "\n",
      " FACET: g-KpF6lTka3\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    plugin: 2 (200.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "    brief_interaction: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "    consistent_prompts: 1 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "tagger = create_default_tagger()\n",
    "tagged_results = tagger.tag_conversations(convs)\n",
    "tagger.print_summary(tagged_results)\n",
    "tagger.print_faceted_summary(tagged_results, 'gizmo', 'gizmo_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67fb315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced rule functions for all previously discussed tagging rules\n",
    "\n",
    "def get_all_user_messages(conversation: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get all user messages in chronological order.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    user_messages = []\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        author = message.get('author', {})\n",
    "        if author.get('role') == 'user':\n",
    "            create_time = message.get('create_time') or 0\n",
    "            user_messages.append((create_time, message))\n",
    "    \n",
    "    user_messages.sort(key=lambda x: x[0])\n",
    "    return [msg for _, msg in user_messages]\n",
    "\n",
    "\n",
    "def get_first_user_message(conversation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Find the first user message in the conversation.\"\"\"\n",
    "    user_messages = get_all_user_messages(conversation)\n",
    "    return user_messages[0] if user_messages else None\n",
    "\n",
    "\n",
    "def create_conversation_length_tag(conversation: Dict[str, Any]) -> Tag:\n",
    "    \"\"\"Create structured tag for conversation length.\"\"\"\n",
    "    user_count = len(get_all_user_messages(conversation))\n",
    "    \n",
    "    # Determine category\n",
    "    if user_count == 1:\n",
    "        category = 'single'\n",
    "    elif user_count <= 3:\n",
    "        category = 'short'\n",
    "    elif user_count <= 10:\n",
    "        category = 'medium'\n",
    "    elif user_count <= 25:\n",
    "        category = 'long'\n",
    "    else:\n",
    "        category = 'very_long'\n",
    "    \n",
    "    return Tag('conversation_length', count=user_count, category=category)\n",
    "\n",
    "\n",
    "def create_prompt_stats_tag(conversation: Dict[str, Any]) -> Tag:\n",
    "    \"\"\"Create structured tag for prompt statistics.\"\"\"\n",
    "    user_messages = get_all_user_messages(conversation)\n",
    "    \n",
    "    if not user_messages:\n",
    "        return Tag('prompt_stats', count=0, mean=0, median=0, variance=0, \n",
    "                  length_category='none', consistency='none')\n",
    "    \n",
    "    # Calculate message lengths\n",
    "    lengths = []\n",
    "    for message in user_messages:\n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        lengths.append(len(all_text))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_length = sum(lengths) / len(lengths)\n",
    "    sorted_lengths = sorted(lengths)\n",
    "    n = len(sorted_lengths)\n",
    "    median_length = (sorted_lengths[n//2] if n % 2 == 1 \n",
    "                    else (sorted_lengths[n//2-1] + sorted_lengths[n//2]) / 2)\n",
    "    variance = sum((x - mean_length) ** 2 for x in lengths) / len(lengths) if len(lengths) > 1 else 0\n",
    "    \n",
    "    # Determine categories\n",
    "    if mean_length < 50:\n",
    "        length_category = 'very_short'\n",
    "    elif mean_length < 200:\n",
    "        length_category = 'short'\n",
    "    elif mean_length < 1000:\n",
    "        length_category = 'medium'\n",
    "    elif mean_length < 3000:\n",
    "        length_category = 'long'\n",
    "    else:\n",
    "        length_category = 'very_long'\n",
    "    \n",
    "    if variance < 1000:\n",
    "        consistency = 'consistent'\n",
    "    elif variance < 10000:\n",
    "        consistency = 'mixed'\n",
    "    else:\n",
    "        consistency = 'variable'\n",
    "    \n",
    "    return Tag('prompt_stats', \n",
    "               count=len(lengths),\n",
    "               mean=round(mean_length, 1),\n",
    "               median=round(median_length, 1),\n",
    "               variance=round(variance, 1),\n",
    "               length_category=length_category,\n",
    "               consistency=consistency)\n",
    "\n",
    "\n",
    "def create_gizmo_plugin_tags(conversation: Dict[str, Any]) -> List[Tag]:\n",
    "    \"\"\"Create structured tags for gizmos and plugins.\"\"\"\n",
    "    tags = []\n",
    "    gizmos = set()\n",
    "    plugins = set()\n",
    "    \n",
    "    # Check conversation-level\n",
    "    if conversation.get('gizmo_id'):\n",
    "        gizmos.add(conversation['gizmo_id'])\n",
    "    \n",
    "    plugin_ids = conversation.get('plugin_ids', [])\n",
    "    if plugin_ids:\n",
    "        plugins.update(plugin_ids)\n",
    "    \n",
    "    # Check message-level\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        \n",
    "        # Invoked plugins\n",
    "        invoked_plugin = metadata.get('invoked_plugin', {})\n",
    "        if invoked_plugin:\n",
    "            if invoked_plugin.get('plugin_id'):\n",
    "                plugins.add(invoked_plugin['plugin_id'])\n",
    "            if invoked_plugin.get('namespace'):\n",
    "                plugins.add(invoked_plugin['namespace'])\n",
    "        \n",
    "        # Gizmo usage\n",
    "        if metadata.get('gizmo_id'):\n",
    "            gizmos.add(metadata['gizmo_id'])\n",
    "    \n",
    "    # Create tags\n",
    "    for gizmo in gizmos:\n",
    "        tags.append(Tag('gizmo', gizmo_id=gizmo))\n",
    "    \n",
    "    for plugin in plugins:\n",
    "        tags.append(Tag('plugin', plugin_id=plugin))\n",
    "    \n",
    "    return tags\n",
    "\n",
    "\n",
    "# Boolean rule functions for basic content analysis\n",
    "def has_large_content(conversation: Dict[str, Any], min_length: int = 2000) -> bool:\n",
    "    \"\"\"Check if conversation has unusually large content anywhere.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        if len(text) > min_length:\n",
    "            return True\n",
    "            \n",
    "        parts = content.get('parts', [])\n",
    "        for part in parts:\n",
    "            if isinstance(part, str) and len(part) > min_length:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_code_patterns(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for clear code patterns anywhere in conversation.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    # Get gizmo context for smarter detection\n",
    "    gizmo_ids = set()\n",
    "    if conversation.get('gizmo_id'):\n",
    "        gizmo_ids.add(conversation['gizmo_id'])\n",
    "    \n",
    "    # Known gizmos that should have stricter code detection\n",
    "    wiki_writing_gizmos = {\n",
    "        'g-IibMsD7w8',  # Your wiki gizmo\n",
    "        'g-pYtHuQdGh',  # Potentially another writing gizmo\n",
    "        # Add other known writing/article gizmos here\n",
    "    }\n",
    "    \n",
    "    is_writing_gizmo = bool(gizmo_ids & wiki_writing_gizmos)\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "        \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        \n",
    "        # Very strong code indicators (reliable across all gizmos)\n",
    "        very_strong_indicators = [\n",
    "            '```',  # Code blocks\n",
    "            '#!/bin/',  # Script headers\n",
    "            '#include',  # C includes\n",
    "            'using namespace',  # C++ using\n",
    "        ]\n",
    "        \n",
    "        if any(indicator in all_text for indicator in very_strong_indicators):\n",
    "            return True\n",
    "        \n",
    "        # Medium strength indicators (require more context)\n",
    "        medium_indicators = [\n",
    "            'def ', 'function ', 'class ',  # Function/class definitions\n",
    "            'import ', 'from ', 'require(',  # Import statements\n",
    "        ]\n",
    "        \n",
    "        if is_writing_gizmo:\n",
    "            # For writing gizmos, require multiple strong indicators or very specific patterns\n",
    "            strong_indicator_count = sum(1 for indicator in medium_indicators if indicator in all_text)\n",
    "            if strong_indicator_count >= 3:  # Multiple function definitions, etc.\n",
    "                return True\n",
    "            \n",
    "            # Look for actual code structure patterns, not just keywords\n",
    "            if ('def ' in all_text and '(' in all_text and ':' in all_text and \n",
    "                'return' in all_text):  # Function definition pattern\n",
    "                return True\n",
    "                \n",
    "        else:\n",
    "            # For non-writing gizmos, use original logic\n",
    "            if any(indicator in all_text for indicator in medium_indicators):\n",
    "                return True\n",
    "            \n",
    "            # Check for high density of coding keywords (but stricter for writing gizmos)\n",
    "            coding_keywords = ['function', 'class', 'import', 'def ', 'const ', 'let ', 'var ', 'return', 'if ', 'for ', 'while ']\n",
    "            keyword_count = sum(1 for keyword in coding_keywords if keyword in all_text.lower())\n",
    "            \n",
    "            # Higher threshold for writing gizmos\n",
    "            min_keywords = 5 if is_writing_gizmo else 3\n",
    "            if len(all_text) > 1000 and keyword_count >= min_keywords:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_github_repos(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if GitHub repositories were selected for context.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        repos = metadata.get('selected_github_repos', [])\n",
    "        if repos:  # Non-empty list\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_canvas_operations(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for canvas/document operations.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if metadata.get('canvas'):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_web_search(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for web search operations.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if (metadata.get('search_queries') or \n",
    "            metadata.get('search_result_groups') or\n",
    "            metadata.get('content_references')):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_reasoning_thoughts(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for reasoning/thinking patterns.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        content = message.get('content', {})\n",
    "        if content.get('thoughts'):  # Reasoning thoughts\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_code_execution(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for code execution artifacts.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if (metadata.get('aggregate_result') or \n",
    "            metadata.get('jupyter_messages')):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "# First user message specific rules\n",
    "def first_user_has_large_content(conversation: Dict[str, Any], min_length: int = 2000) -> bool:\n",
    "    \"\"\"Check if the first user message has large content.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    content = first_message.get('content', {})\n",
    "    text = content.get('text', '')\n",
    "    if len(text) > min_length:\n",
    "        return True\n",
    "        \n",
    "    parts = content.get('parts', [])\n",
    "    for part in parts:\n",
    "        if isinstance(part, str) and len(part) > min_length:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def first_user_has_code_patterns(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message contains code patterns.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    content = first_message.get('content', {})\n",
    "    text = content.get('text', '')\n",
    "    parts = content.get('parts', [])\n",
    "    all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "    \n",
    "    # Strong code indicators\n",
    "    code_indicators = [\n",
    "        '```',  # Code blocks\n",
    "        'def ', 'function ', 'class ',  # Definitions\n",
    "        'import ', 'from ', 'require(',  # Imports\n",
    "        '#!/bin/', '#include',  # Script headers\n",
    "    ]\n",
    "    \n",
    "    return any(indicator in all_text for indicator in code_indicators)\n",
    "\n",
    "\n",
    "def first_user_has_attachments(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message has attachments.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    metadata = first_message.get('metadata', {})\n",
    "    attachments = metadata.get('attachments', [])\n",
    "    return len(attachments) > 0\n",
    "\n",
    "\n",
    "def first_user_has_code_attachments(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message has code-related attachments.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    metadata = first_message.get('metadata', {})\n",
    "    attachments = metadata.get('attachments', [])\n",
    "    \n",
    "    for attachment in attachments:\n",
    "        mime_type = attachment.get('mime_type', '').lower()\n",
    "        name = attachment.get('name', '').lower()\n",
    "        \n",
    "        # Check for code file extensions\n",
    "        code_extensions = ['.py', '.js', '.java', '.cpp', '.c', '.go', '.rs', '.ts', '.jsx', '.tsx', '.sql', '.sh', '.rb', '.php']\n",
    "        if any(ext in name for ext in code_extensions):\n",
    "            return True\n",
    "            \n",
    "        # Check for code-related MIME types\n",
    "        code_mimes = ['text/x-python', 'text/x-java', 'application/javascript', 'text/x-script']\n",
    "        if any(mime in mime_type for mime in code_mimes):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def create_default_tagger() -> ConversationTagger:\n",
    "    \"\"\"Create a tagger with all previously discussed rules in the enhanced structured framework.\"\"\"\n",
    "    tagger = ConversationTagger()\n",
    "    \n",
    "    # ===== BASIC CONTENT ANALYSIS RULES (Boolean) =====\n",
    "    tagger.add_base_rule(\n",
    "        'large_content', \n",
    "        lambda conv: has_large_content(conv, 2000),\n",
    "        'Content longer than 2000 characters anywhere in conversation'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'code_patterns', \n",
    "        has_code_patterns,\n",
    "        'Contains clear code patterns (```, def, function, etc.)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'github_context',\n",
    "        has_github_repos,\n",
    "        'GitHub repositories selected for context'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'canvas_operations',\n",
    "        has_canvas_operations,\n",
    "        'Uses canvas/document features'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'web_search',\n",
    "        has_web_search,\n",
    "        'Includes web search functionality'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'reasoning',\n",
    "        has_reasoning_thoughts,\n",
    "        'Contains reasoning/thinking content'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'code_execution',\n",
    "        has_code_execution,\n",
    "        'Contains code execution (Jupyter, aggregate results)'\n",
    "    )\n",
    "    \n",
    "    # ===== FIRST USER MESSAGE RULES (Boolean) =====\n",
    "    tagger.add_base_rule(\n",
    "        'starts_large_content',\n",
    "        lambda conv: first_user_has_large_content(conv, 2000),\n",
    "        'First user message has large content (>2000 chars)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_code_patterns',\n",
    "        first_user_has_code_patterns,\n",
    "        'First user message contains code patterns'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_with_attachments',\n",
    "        first_user_has_attachments,\n",
    "        'First user message has any attachments'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_code_attachments',\n",
    "        first_user_has_code_attachments,\n",
    "        'First user message has code-related attachments'\n",
    "    )\n",
    "    \n",
    "    # ===== STRUCTURED TAG RULES =====\n",
    "    tagger.add_base_rule(\n",
    "        'conversation_length',\n",
    "        create_conversation_length_tag,\n",
    "        'Conversation length with count and category'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'prompt_stats',\n",
    "        create_prompt_stats_tag,\n",
    "        'User message statistics (length, variance, etc.)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_multi_tag_rule(\n",
    "        'gizmo_plugin_usage',\n",
    "        create_gizmo_plugin_tags,\n",
    "        'Specific gizmos and plugins used in conversation'\n",
    "    )\n",
    "    \n",
    "    # ===== SUPPLEMENTAL RULES (Based on existing tags) =====\n",
    "    \n",
    "    # Coding assistance detection\n",
    "    tagger.add_supplemental_rule(\n",
    "        'coding_assistance',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['code_patterns', 'github_context', 'code_execution'] for tag in tags)\n",
    "        ),\n",
    "        'Likely coding assistance (code patterns, GitHub, or execution)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'coding_assistance_start',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['starts_code_patterns', 'starts_large_content', \n",
    "                           'starts_with_attachments', 'starts_code_attachments'] for tag in tags)\n",
    "        ),\n",
    "        'Likely coding assistance based on how conversation starts'\n",
    "    )\n",
    "    \n",
    "    # Research and analysis patterns  \n",
    "    tagger.add_supplemental_rule(\n",
    "        'research_session',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'web_search' for tag in tags) and \n",
    "            any(tag.name == 'large_content' for tag in tags)\n",
    "        ),\n",
    "        'Research session (web search + substantial content)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'complex_analysis',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'reasoning' for tag in tags) and \n",
    "            len([tag for tag in tags if tag.name in ['web_search', 'large_content', 'canvas_operations']]) >= 2\n",
    "        ),\n",
    "        'Complex analysis (reasoning + multiple advanced features)'\n",
    "    )\n",
    "    \n",
    "    # Context and interaction patterns\n",
    "    tagger.add_supplemental_rule(\n",
    "        'context_heavy_start',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['starts_large_content', 'starts_with_attachments'] for tag in tags)\n",
    "        ),\n",
    "        'Conversation starts with substantial context'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'enhanced_conversation',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['gizmo', 'plugin'] for tag in tags)\n",
    "        ),\n",
    "        'Uses enhanced features (gizmos or plugins)'\n",
    "    )\n",
    "    \n",
    "    # Length-based classifications using structured tags\n",
    "    tagger.add_supplemental_rule(\n",
    "        'brief_interaction',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('category') in ['single', 'short'] for tag in tags)\n",
    "        ),\n",
    "        'Brief interaction (1-3 user messages)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'extended_conversation',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('category') in ['long', 'very_long'] for tag in tags)\n",
    "        ),\n",
    "        'Extended conversation (11+ user messages)'\n",
    "    )\n",
    "    \n",
    "    # Prompt pattern classifications using structured tags\n",
    "    tagger.add_supplemental_rule(\n",
    "        'long_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('length_category') in ['long', 'very_long'] for tag in tags)\n",
    "        ),\n",
    "        'Conversation has consistently long prompts'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'short_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('length_category') in ['very_short', 'short'] for tag in tags)\n",
    "        ),\n",
    "        'Conversation has consistently short prompts'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'consistent_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'consistent' for tag in tags)\n",
    "        ),\n",
    "        'User prompts are consistent in length'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'variable_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'variable' for tag in tags)\n",
    "        ),\n",
    "        'User prompts vary significantly in length'\n",
    "    )\n",
    "    \n",
    "    # Combined patterns for specific use cases\n",
    "    tagger.add_supplemental_rule(\n",
    "        'context_dump',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('category') in ['single', 'short'] for tag in tags) and\n",
    "            any(tag.name in ['starts_large_content', 'starts_with_attachments'] for tag in tags)\n",
    "        ),\n",
    "        'Short conversation starting with large context (likely context dump)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'interactive_session',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('count', 0) >= 5 for tag in tags) and\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'consistent' for tag in tags)\n",
    "        ),\n",
    "        'Extended back-and-forth conversation with consistent prompt style'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'evolving_discussion',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('count', 0) >= 5 for tag in tags) and\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'variable' for tag in tags)\n",
    "        ),\n",
    "        'Extended conversation where prompt style evolves'\n",
    "    )\n",
    "    \n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcfad39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged 1673 conversations\n",
      "\n",
      "=== TAG SUMMARY ===\n",
      "prompt_stats: 1673 (100.0%)\n",
      "    count: avg=7.6, range=[1, 160]\n",
      "    mean: avg=1142.4, range=[1.0, 184662.5]\n",
      "    median: avg=814.2, range=[1.0, 184662.5]\n",
      "    variance: avg=51563065.2, range=[0, 34065162056.2]\n",
      "    values: consistency=consistent, consistency=mixed, consistency=variable, length_category=long, length_category=medium, length_category=short, length_category=very_long, length_category=very_short\n",
      "conversation_length: 1673 (100.0%)\n",
      "    count: avg=7.6, range=[1, 160]\n",
      "    values: category=long, category=medium, category=short, category=single, category=very_long\n",
      "large_content: 1362 (81.4%)\n",
      "short_prompts: 1261 (75.4%)\n",
      "coding_assistance: 1005 (60.1%)\n",
      "code_patterns: 1000 (59.8%)\n",
      "brief_interaction: 906 (54.2%)\n",
      "consistent_prompts: 896 (53.6%)\n",
      "enhanced_conversation: 681 (40.7%)\n",
      "gizmo: 676 (40.4%)\n",
      "    values: gizmo_id=g-IibMsD7w8, gizmo_id=g-KpF6lTka3, gizmo_id=g-QsUj0Smzg, gizmo_id=g-WiEAUBGzb, gizmo_id=g-bWPVPw7oK, gizmo_id=g-pYtHuQdGh\n",
      "variable_prompts: 405 (24.2%)\n",
      "extended_conversation: 304 (18.2%)\n",
      "coding_assistance_start: 260 (15.5%)\n",
      "evolving_discussion: 247 (14.8%)\n",
      "starts_code_patterns: 240 (14.3%)\n",
      "interactive_session: 184 (11.0%)\n",
      "long_prompts: 151 (9.0%)\n",
      "context_heavy_start: 127 (7.6%)\n",
      "starts_large_content: 119 (7.1%)\n",
      "web_search: 66 (3.9%)\n",
      "context_dump: 64 (3.8%)\n",
      "research_session: 53 (3.2%)\n",
      "code_execution: 26 (1.6%)\n",
      "reasoning: 12 (0.7%)\n",
      "plugin: 11 (0.7%)\n",
      "    values: plugin_id=AskTheCode, plugin_id=chatgpt_production_alltrails_com__jit_plugin, plugin_id=chatwithvideo, plugin_id=code_repo_interaction, plugin_id=g-eaf0e78bb71845ae7b86a9268d1374c168f5bd85, plugin_id=plugin-25ce675b-c2c4-4460-ad33-8e641653498c, plugin_id=plugin-3cf29a7b-2fcb-42aa-b762-4d418b543a8b, plugin_id=plugin-b82d4d0b-79b8-4548-b1b4-65b218376d6f, plugin_id=plugin-e5009c08-c6c8-4195-977f-16f39a7d3b7b, plugin_id=scholar_assist\n",
      "starts_with_attachments: 8 (0.5%)\n",
      "complex_analysis: 5 (0.3%)\n",
      "canvas_operations: 3 (0.2%)\n",
      "Tagged 1673 conversations\n",
      "Faceted by: gizmo.gizmo_id\n",
      "Found 7 facet values\n",
      "\n",
      "================================================================================\n",
      "FACETED TAG SUMMARY\n",
      "================================================================================\n",
      "\n",
      " FACET: <none>\n",
      "    Conversations: 997 (59.6% of total)\n",
      "    ------------------------------------------------------------\n",
      "    prompt_stats: 997 (100.0%)\n",
      "    conversation_length: 997 (100.0%)\n",
      "    coding_assistance: 891 (89.4%)\n",
      "    code_patterns: 886 (88.9%)\n",
      "    large_content: 696 (69.8%)\n",
      "    short_prompts: 627 (62.9%)\n",
      "    brief_interaction: 606 (60.8%)\n",
      "    consistent_prompts: 427 (42.8%)\n",
      "    variable_prompts: 330 (33.1%)\n",
      "    coding_assistance_start: 236 (23.7%)\n",
      "    starts_code_patterns: 216 (21.7%)\n",
      "    evolving_discussion: 191 (19.2%)\n",
      "    long_prompts: 139 (13.9%)\n",
      "    context_heavy_start: 119 (11.9%)\n",
      "    extended_conversation: 112 (11.2%)\n",
      "    starts_large_content: 111 (11.1%)\n",
      "    context_dump: 60 (6.0%)\n",
      "    web_search: 54 (5.4%)\n",
      "    research_session: 41 (4.1%)\n",
      "    code_execution: 26 (2.6%)\n",
      "    interactive_session: 18 (1.8%)\n",
      "    reasoning: 12 (1.2%)\n",
      "    plugin: 9 (0.9%)\n",
      "    starts_with_attachments: 8 (0.8%)\n",
      "    complex_analysis: 5 (0.5%)\n",
      "    enhanced_conversation: 5 (0.5%)\n",
      "    canvas_operations: 3 (0.3%)\n",
      "\n",
      " FACET: g-IibMsD7w8\n",
      "    Conversations: 668 (39.9% of total)\n",
      "    ------------------------------------------------------------\n",
      "    gizmo: 668 (100.0%)\n",
      "    prompt_stats: 668 (100.0%)\n",
      "    enhanced_conversation: 668 (100.0%)\n",
      "    conversation_length: 668 (100.0%)\n",
      "    large_content: 664 (99.4%)\n",
      "    short_prompts: 628 (94.0%)\n",
      "    consistent_prompts: 463 (69.3%)\n",
      "    brief_interaction: 294 (44.0%)\n",
      "    extended_conversation: 191 (28.6%)\n",
      "    interactive_session: 166 (24.9%)\n",
      "    code_patterns: 110 (16.5%)\n",
      "    coding_assistance: 110 (16.5%)\n",
      "    variable_prompts: 74 (11.1%)\n",
      "    evolving_discussion: 55 (8.2%)\n",
      "    coding_assistance_start: 23 (3.4%)\n",
      "    starts_code_patterns: 23 (3.4%)\n",
      "    long_prompts: 12 (1.8%)\n",
      "    research_session: 11 (1.6%)\n",
      "    web_search: 11 (1.6%)\n",
      "    starts_large_content: 8 (1.2%)\n",
      "    context_heavy_start: 8 (1.2%)\n",
      "    context_dump: 4 (0.6%)\n",
      "\n",
      " FACET: g-bWPVPw7oK\n",
      "    Conversations: 4 (0.2% of total)\n",
      "    ------------------------------------------------------------\n",
      "    gizmo: 4 (100.0%)\n",
      "    short_prompts: 4 (100.0%)\n",
      "    prompt_stats: 4 (100.0%)\n",
      "    enhanced_conversation: 4 (100.0%)\n",
      "    conversation_length: 4 (100.0%)\n",
      "    brief_interaction: 3 (75.0%)\n",
      "    consistent_prompts: 3 (75.0%)\n",
      "    coding_assistance: 2 (50.0%)\n",
      "    code_patterns: 2 (50.0%)\n",
      "    extended_conversation: 1 (25.0%)\n",
      "\n",
      " FACET: g-WiEAUBGzb\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    code_patterns: 1 (100.0%)\n",
      "    coding_assistance: 1 (100.0%)\n",
      "    brief_interaction: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    large_content: 1 (100.0%)\n",
      "    short_prompts: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "    consistent_prompts: 1 (100.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "\n",
      " FACET: g-pYtHuQdGh\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    code_patterns: 1 (100.0%)\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    coding_assistance: 1 (100.0%)\n",
      "    coding_assistance_start: 1 (100.0%)\n",
      "    research_session: 1 (100.0%)\n",
      "    starts_code_patterns: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "    evolving_discussion: 1 (100.0%)\n",
      "    web_search: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "    variable_prompts: 1 (100.0%)\n",
      "    large_content: 1 (100.0%)\n",
      "\n",
      " FACET: g-QsUj0Smzg\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    brief_interaction: 1 (100.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "    short_prompts: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "    consistent_prompts: 1 (100.0%)\n",
      "\n",
      " FACET: g-KpF6lTka3\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    plugin: 2 (200.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "    brief_interaction: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "    consistent_prompts: 1 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "tagger = create_default_tagger()\n",
    "tagged_results = tagger.tag_conversations(convs)\n",
    "tagger.print_summary(tagged_results)\n",
    "tagger.print_faceted_summary(tagged_results, 'gizmo', 'gizmo_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "153df6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhanced_conversation_tagger.py\n",
    "\"\"\"\n",
    "Enhanced conversation tagging system with faceting capabilities.\n",
    "Allows analysis of tag distributions across different facets (gizmos, conversation types, etc.).\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Any, List, Callable, Set, Union, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "class Tag:\n",
    "    \"\"\"Represents a tag with optional key-value attributes.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, **attributes):\n",
    "        self.name = name\n",
    "        self.attributes = attributes\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.attributes:\n",
    "            attrs_str = \", \".join(f\"{k}={v}\" for k, v in self.attributes.items())\n",
    "            return f\"{self.name}({attrs_str})\"\n",
    "        return self.name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Tag('{self.name}', {self.attributes})\"\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, str):\n",
    "            return self.name == other\n",
    "        elif isinstance(other, Tag):\n",
    "            return self.name == other.name and self.attributes == other.attributes\n",
    "        return False\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.name, tuple(sorted(self.attributes.items()))))\n",
    "    \n",
    "    def matches(self, name: str, **criteria) -> bool:\n",
    "        \"\"\"Check if tag matches name and optional attribute criteria.\"\"\"\n",
    "        if self.name != name:\n",
    "            return False\n",
    "        \n",
    "        for key, value in criteria.items():\n",
    "            if key not in self.attributes:\n",
    "                return False\n",
    "            \n",
    "            attr_value = self.attributes[key]\n",
    "            \n",
    "            # Support comparison operators\n",
    "            if isinstance(value, dict):\n",
    "                for op, target in value.items():\n",
    "                    if op == 'gt' and not (attr_value > target):\n",
    "                        return False\n",
    "                    elif op == 'gte' and not (attr_value >= target):\n",
    "                        return False\n",
    "                    elif op == 'lt' and not (attr_value < target):\n",
    "                        return False\n",
    "                    elif op == 'lte' and not (attr_value <= target):\n",
    "                        return False\n",
    "                    elif op == 'eq' and not (attr_value == target):\n",
    "                        return False\n",
    "                    elif op == 'ne' and not (attr_value != target):\n",
    "                        return False\n",
    "                    elif op == 'in' and not (attr_value in target):\n",
    "                        return False\n",
    "            else:\n",
    "                # Direct equality\n",
    "                if attr_value != value:\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "\n",
    "class ConversationTagger:\n",
    "    \"\"\"\n",
    "    Enhanced tagging system supporting structured tags with attributes and faceting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_rules: Dict[str, Callable] = {}\n",
    "        self.multi_tag_rules: Dict[str, Callable] = {}\n",
    "        self.supplemental_rules: Dict[str, Callable] = {}\n",
    "        self.rule_descriptions: Dict[str, str] = {}\n",
    "    \n",
    "    def add_base_rule(self, tag_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"Add a base tagging rule that returns bool or Tag object.\"\"\"\n",
    "        self.base_rules[tag_name] = rule_function\n",
    "        self.rule_descriptions[tag_name] = description\n",
    "    \n",
    "    def add_multi_tag_rule(self, rule_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"Add a rule that returns multiple tags (strings or Tag objects).\"\"\"\n",
    "        self.multi_tag_rules[rule_name] = rule_function\n",
    "        self.rule_descriptions[rule_name] = description\n",
    "    \n",
    "    def add_supplemental_rule(self, tag_name: str, rule_function: Callable, description: str = \"\"):\n",
    "        \"\"\"Add a supplemental rule that depends on existing tags.\"\"\"\n",
    "        self.supplemental_rules[tag_name] = rule_function\n",
    "        self.rule_descriptions[tag_name] = description\n",
    "    \n",
    "    def _normalize_tag(self, tag: Union[str, Tag]) -> Tag:\n",
    "        \"\"\"Convert string tags to Tag objects.\"\"\"\n",
    "        if isinstance(tag, str):\n",
    "            return Tag(tag)\n",
    "        return tag\n",
    "    \n",
    "    def tag_conversation(self, conversation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Apply all tagging rules to a conversation.\"\"\"\n",
    "        tags = set()\n",
    "        debug_info = defaultdict(list)\n",
    "        \n",
    "        # Apply base rules\n",
    "        for tag_name, rule_func in self.base_rules.items():\n",
    "            try:\n",
    "                result = rule_func(conversation)\n",
    "                if result:\n",
    "                    if isinstance(result, bool):\n",
    "                        tag = Tag(tag_name)\n",
    "                    else:\n",
    "                        tag = self._normalize_tag(result)\n",
    "                    tags.add(tag)\n",
    "                    debug_info['applied_rules'].append(f\"BASE: {tag}\")\n",
    "                else:\n",
    "                    debug_info['skipped_rules'].append(f\"BASE: {tag_name}\")\n",
    "            except Exception as e:\n",
    "                debug_info['errors'].append(f\"BASE: {tag_name} - {str(e)}\")\n",
    "        \n",
    "        # Apply multi-tag rules\n",
    "        for rule_name, rule_func in self.multi_tag_rules.items():\n",
    "            try:\n",
    "                new_tags = rule_func(conversation)\n",
    "                if new_tags:\n",
    "                    normalized_tags = [self._normalize_tag(tag) for tag in new_tags]\n",
    "                    tags.update(normalized_tags)\n",
    "                    debug_info['applied_rules'].append(f\"MULTI: {rule_name} -> {[str(t) for t in normalized_tags]}\")\n",
    "                else:\n",
    "                    debug_info['skipped_rules'].append(f\"MULTI: {rule_name}\")\n",
    "            except Exception as e:\n",
    "                debug_info['errors'].append(f\"MULTI: {rule_name} - {str(e)}\")\n",
    "        \n",
    "        # Apply supplemental rules\n",
    "        max_iterations = 5\n",
    "        for iteration in range(max_iterations):\n",
    "            initial_tag_count = len(tags)\n",
    "            \n",
    "            for tag_name, rule_func in self.supplemental_rules.items():\n",
    "                # Check if tag already exists\n",
    "                if any(tag.name == tag_name for tag in tags):\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    result = rule_func(conversation, tags)\n",
    "                    if result:\n",
    "                        if isinstance(result, bool):\n",
    "                            tag = Tag(tag_name)\n",
    "                        else:\n",
    "                            tag = self._normalize_tag(result)\n",
    "                        tags.add(tag)\n",
    "                        debug_info['applied_rules'].append(f\"SUPP: {tag} (iter {iteration})\")\n",
    "                    else:\n",
    "                        debug_info['skipped_rules'].append(f\"SUPP: {tag_name} (iter {iteration})\")\n",
    "                except Exception as e:\n",
    "                    debug_info['errors'].append(f\"SUPP: {tag_name} - {str(e)}\")\n",
    "            \n",
    "            if len(tags) == initial_tag_count:\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'conversation_id': conversation.get('conversation_id', conversation.get('id', 'unknown')),\n",
    "            'title': conversation.get('title', 'Untitled'),\n",
    "            'tags': list(tags),\n",
    "            'debug_info': dict(debug_info),\n",
    "            'conversation': conversation\n",
    "        }\n",
    "    \n",
    "    def tag_conversations(self, conversations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Tag multiple conversations.\"\"\"\n",
    "        return [self.tag_conversation(conv) for conv in conversations]\n",
    "    \n",
    "    def filter_by_tags(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                      include_tags: List[Union[str, Dict]] = None,\n",
    "                      exclude_tags: List[Union[str, Dict]] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Filter conversations by tags with attribute support.\n",
    "        \n",
    "        Args:\n",
    "            include_tags: List of tag names or dicts with criteria\n",
    "                Examples: ['web_search', {'name': 'gizmo', 'type': 'dalle'}]\n",
    "            exclude_tags: Similar format for exclusions\n",
    "        \"\"\"\n",
    "        filtered = []\n",
    "        \n",
    "        for tagged_conv in tagged_conversations:\n",
    "            tags = tagged_conv['tags']\n",
    "            \n",
    "            # Check exclusions first\n",
    "            if exclude_tags:\n",
    "                should_exclude = False\n",
    "                for exclude_criterion in exclude_tags:\n",
    "                    if self._matches_criterion(tags, exclude_criterion):\n",
    "                        should_exclude = True\n",
    "                        break\n",
    "                if should_exclude:\n",
    "                    continue\n",
    "            \n",
    "            # Check inclusions\n",
    "            if include_tags:\n",
    "                should_include = True\n",
    "                for include_criterion in include_tags:\n",
    "                    if not self._matches_criterion(tags, include_criterion):\n",
    "                        should_include = False\n",
    "                        break\n",
    "                if not should_include:\n",
    "                    continue\n",
    "            \n",
    "            filtered.append(tagged_conv)\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def _matches_criterion(self, tags: List[Tag], criterion: Union[str, Dict]) -> bool:\n",
    "        \"\"\"Check if any tag matches the given criterion.\"\"\"\n",
    "        if isinstance(criterion, str):\n",
    "            return any(tag.name == criterion for tag in tags)\n",
    "        \n",
    "        elif isinstance(criterion, dict):\n",
    "            name = criterion.get('name')\n",
    "            if not name:\n",
    "                return False\n",
    "            \n",
    "            criteria = {k: v for k, v in criterion.items() if k != 'name'}\n",
    "            return any(tag.matches(name, **criteria) for tag in tags)\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_facet_value(self, tags: List[Tag], facet_tag_name: str, \n",
    "                       facet_attribute: Optional[str] = None) -> str:\n",
    "        \"\"\"Extract facet value from a conversation's tags.\"\"\"\n",
    "        matching_tags = [tag for tag in tags if tag.name == facet_tag_name]\n",
    "        \n",
    "        if not matching_tags:\n",
    "            return \"<none>\"\n",
    "        \n",
    "        if facet_attribute is None:\n",
    "            # Just check for presence of the tag\n",
    "            return f\"has_{facet_tag_name}\"\n",
    "        \n",
    "        # Extract specific attribute values\n",
    "        values = []\n",
    "        for tag in matching_tags:\n",
    "            if facet_attribute in tag.attributes:\n",
    "                values.append(str(tag.attributes[facet_attribute]))\n",
    "        \n",
    "        if not values:\n",
    "            return f\"<{facet_tag_name}_no_{facet_attribute}>\"\n",
    "        \n",
    "        # If multiple values, join them\n",
    "        return \"; \".join(sorted(set(values)))\n",
    "    \n",
    "    def facet_conversations(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                           facet_tag_name: str, \n",
    "                           facet_attribute: Optional[str] = None,\n",
    "                           max_facets: int = 50) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Group conversations by facet values.\n",
    "        \n",
    "        Args:\n",
    "            facet_tag_name: Tag name to facet by (e.g., 'gizmo')\n",
    "            facet_attribute: Optional attribute within tag (e.g., 'gizmo_id')\n",
    "            max_facets: Maximum number of facet values to show\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping facet values to lists of conversations\n",
    "        \"\"\"\n",
    "        facets = defaultdict(list)\n",
    "        \n",
    "        for tagged_conv in tagged_conversations:\n",
    "            facet_value = self.get_facet_value(tagged_conv['tags'], facet_tag_name, facet_attribute)\n",
    "            facets[facet_value].append(tagged_conv)\n",
    "        \n",
    "        # Sort by facet size (largest first) and limit\n",
    "        sorted_facets = dict(sorted(facets.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "        \n",
    "        if len(sorted_facets) > max_facets:\n",
    "            # Keep top facets and group rest into \"others\"\n",
    "            items = list(sorted_facets.items())\n",
    "            top_facets = dict(items[:max_facets-1])\n",
    "            \n",
    "            other_conversations = []\n",
    "            for _, conversations in items[max_facets-1:]:\n",
    "                other_conversations.extend(conversations)\n",
    "            \n",
    "            if other_conversations:\n",
    "                top_facets[\"<other>\"] = other_conversations\n",
    "            \n",
    "            return top_facets\n",
    "        \n",
    "        return sorted_facets\n",
    "    \n",
    "    def print_faceted_summary(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                             facet_tag_name: str, \n",
    "                             facet_attribute: Optional[str] = None,\n",
    "                             show_details: bool = False,\n",
    "                             max_facets: int = 20):\n",
    "        \"\"\"\n",
    "        Print tag summary broken down by facets.\n",
    "        \n",
    "        Args:\n",
    "            facet_tag_name: Tag to facet by\n",
    "            facet_attribute: Optional attribute within the tag  \n",
    "            show_details: Show detailed tag attribute info\n",
    "            max_facets: Maximum facets to show\n",
    "            max_tags_per_facet: Maximum tags to show per facet\n",
    "        \"\"\"\n",
    "        total = len(tagged_conversations)\n",
    "        facets = self.facet_conversations(tagged_conversations, facet_tag_name, facet_attribute, max_facets)\n",
    "        \n",
    "        print(f\"Tagged {total} conversations\")\n",
    "        print(f\"Faceted by: {facet_tag_name}\" + \n",
    "              (f\".{facet_attribute}\" if facet_attribute else \"\"))\n",
    "        print(f\"Found {len(facets)} facet values\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FACETED TAG SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for facet_value, facet_conversations in facets.items():\n",
    "            facet_size = len(facet_conversations)\n",
    "            facet_percentage = (facet_size / total) * 100\n",
    "            \n",
    "            print(f\"\\n FACET: {facet_value}\")\n",
    "            print(f\"    Conversations: {facet_size} ({facet_percentage:.1f}% of total)\")\n",
    "            print(f\"    {'-' * 60}\")\n",
    "            \n",
    "            # Calculate tag statistics for this facet\n",
    "            tag_counts = defaultdict(int)\n",
    "            tag_attributes = defaultdict(lambda: defaultdict(list))\n",
    "            unique_structured_tags = defaultdict(set)\n",
    "            \n",
    "            for tagged_conv in facet_conversations:\n",
    "                for tag in tagged_conv['tags']:\n",
    "                    tag_counts[tag.name] += 1\n",
    "                    \n",
    "                    # Collect attribute information\n",
    "                    for attr_name, attr_value in tag.attributes.items():\n",
    "                        if isinstance(attr_value, (int, float)):\n",
    "                            tag_attributes[tag.name][attr_name].append(attr_value)\n",
    "                        else:\n",
    "                            unique_structured_tags[tag.name].add(f\"{attr_name}={attr_value}\")\n",
    "            \n",
    "            # Sort tags for this facet (show all tags)\n",
    "            sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for tag_name, count in sorted_tags:\n",
    "                percentage = (count / facet_size) * 100\n",
    "                print(f\"    {tag_name}: {count} ({percentage:.1f}%)\")\n",
    "                \n",
    "                if show_details:\n",
    "                    # Show numeric attribute statistics\n",
    "                    if tag_name in tag_attributes:\n",
    "                        for attr_name, values in tag_attributes[tag_name].items():\n",
    "                            if values:\n",
    "                                avg_val = sum(values) / len(values)\n",
    "                                min_val = min(values)\n",
    "                                max_val = max(values)\n",
    "                                print(f\"        {attr_name}: avg={avg_val:.1f}, range=[{min_val}, {max_val}]\")\n",
    "                    \n",
    "                    # Show unique structured values\n",
    "                    if tag_name in unique_structured_tags:\n",
    "                        unique_vals = sorted(unique_structured_tags[tag_name])\n",
    "                        if len(unique_vals) <= 5:\n",
    "                            print(f\"        values: {', '.join(unique_vals)}\")\n",
    "                        else:\n",
    "                            print(f\"        values: {', '.join(unique_vals[:5])} ... (+{len(unique_vals)-5} more)\")\n",
    "    \n",
    "    def compare_facets(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                      facet_tag_name: str, \n",
    "                      facet_attribute: Optional[str] = None,\n",
    "                      comparison_tags: List[str] = None,\n",
    "                      min_facet_size: int = 10) -> None:\n",
    "        \"\"\"\n",
    "        Compare specific tags across facets.\n",
    "        \n",
    "        Args:\n",
    "            comparison_tags: List of tags to compare across facets\n",
    "            min_facet_size: Minimum facet size to include in comparison\n",
    "        \"\"\"\n",
    "        facets = self.facet_conversations(tagged_conversations, facet_tag_name, facet_attribute)\n",
    "        \n",
    "        # Filter facets by minimum size\n",
    "        large_facets = {k: v for k, v in facets.items() if len(v) >= min_facet_size}\n",
    "        \n",
    "        if not large_facets:\n",
    "            print(f\"No facets with at least {min_facet_size} conversations found\")\n",
    "            return\n",
    "        \n",
    "        # If no specific tags provided, use most common tags overall\n",
    "        if comparison_tags is None:\n",
    "            overall_tag_counts = defaultdict(int)\n",
    "            for tagged_conv in tagged_conversations:\n",
    "                for tag in tagged_conv['tags']:\n",
    "                    overall_tag_counts[tag.name] += 1\n",
    "            \n",
    "            # Get top 10 most common tags\n",
    "            comparison_tags = [tag for tag, _ in \n",
    "                             sorted(overall_tag_counts.items(), key=lambda x: x[1], reverse=True)[:10]]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FACET COMPARISON\")\n",
    "        print(f\"Comparing tags: {', '.join(comparison_tags)}\")\n",
    "        print(f\"Across facets: {facet_tag_name}\" + \n",
    "              (f\".{facet_attribute}\" if facet_attribute else \"\"))\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Calculate percentages for each tag in each facet\n",
    "        results = {}\n",
    "        for facet_value, facet_conversations in large_facets.items():\n",
    "            facet_size = len(facet_conversations)\n",
    "            facet_tag_counts = defaultdict(int)\n",
    "            \n",
    "            for tagged_conv in facet_conversations:\n",
    "                for tag in tagged_conv['tags']:\n",
    "                    facet_tag_counts[tag.name] += 1\n",
    "            \n",
    "            results[facet_value] = {\n",
    "                'size': facet_size,\n",
    "                'percentages': {tag: (facet_tag_counts[tag] / facet_size) * 100 \n",
    "                               for tag in comparison_tags}\n",
    "            }\n",
    "        \n",
    "        # Print comparison table\n",
    "        print(f\"\\n{'Facet':<30} {'Size':<8} \" + \n",
    "              \"\".join(f\"{tag:<15}\" for tag in comparison_tags))\n",
    "        print(\"-\" * (30 + 8 + 15 * len(comparison_tags)))\n",
    "        \n",
    "        for facet_value, data in results.items():\n",
    "            facet_display = facet_value[:28] + \"..\" if len(facet_value) > 30 else facet_value\n",
    "            row = f\"{facet_display:<30} {data['size']:<8} \"\n",
    "            row += \"\".join(f\"{data['percentages'][tag]:<15.1f}\" for tag in comparison_tags)\n",
    "            print(row)\n",
    "        \n",
    "        # Highlight interesting differences\n",
    "        print(f\"\\n NOTABLE DIFFERENCES:\")\n",
    "        for tag in comparison_tags:\n",
    "            percentages = [results[facet]['percentages'][tag] for facet in results.keys()]\n",
    "            if max(percentages) - min(percentages) > 20:  # 20% difference threshold\n",
    "                max_facet = max(results.keys(), key=lambda f: results[f]['percentages'][tag])\n",
    "                min_facet = min(results.keys(), key=lambda f: results[f]['percentages'][tag])\n",
    "                print(f\"    {tag}: {max_facet} ({results[max_facet]['percentages'][tag]:.1f}%) vs \" +\n",
    "                      f\"{min_facet} ({results[min_facet]['percentages'][tag]:.1f}%)\")\n",
    "    \n",
    "    def get_tag_values(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                      tag_name: str, attribute: str) -> List[Any]:\n",
    "        \"\"\"Extract attribute values from tags across conversations.\"\"\"\n",
    "        values = []\n",
    "        for tagged_conv in tagged_conversations:\n",
    "            for tag in tagged_conv['tags']:\n",
    "                if tag.name == tag_name and attribute in tag.attributes:\n",
    "                    values.append(tag.attributes[attribute])\n",
    "        return values\n",
    "    \n",
    "    def debug_code_detection(self, tagged_conversations: List[Dict[str, Any]], \n",
    "                            facet_tag_name: str = None, \n",
    "                            facet_value: str = None,\n",
    "                            max_examples: int = 5):\n",
    "        \"\"\"\n",
    "        Debug why conversations are being tagged as having code patterns.\n",
    "        \n",
    "        Args:\n",
    "            facet_tag_name: Optional facet to filter by (e.g., 'gizmo')\n",
    "            facet_value: Optional facet value to filter by (e.g., 'g-IibMsD7w8')\n",
    "            max_examples: Number of example conversations to analyze\n",
    "        \"\"\"\n",
    "        # Filter conversations if faceting specified\n",
    "        conversations_to_analyze = tagged_conversations\n",
    "        if facet_tag_name and facet_value:\n",
    "            conversations_to_analyze = []\n",
    "            for tagged_conv in tagged_conversations:\n",
    "                facet_val = self.get_facet_value(tagged_conv['tags'], facet_tag_name, \n",
    "                                               'gizmo_id' if facet_tag_name == 'gizmo' else None)\n",
    "                if facet_val == facet_value:\n",
    "                    conversations_to_analyze.append(tagged_conv)\n",
    "        \n",
    "        # Show breakdown of different code indicators\n",
    "        indicator_counts = {\n",
    "            'code_blocks': 0,\n",
    "            'function_definitions': 0,\n",
    "            'import_statements': 0,\n",
    "            'script_headers': 0,\n",
    "            'high_keyword_density': 0,\n",
    "            'code_structure_patterns': 0,\n",
    "            'code_patterns': 0,  # Legacy combined\n",
    "        }\n",
    "        \n",
    "        evidence_counts = {\n",
    "            'strong_code_evidence': 0,\n",
    "            'moderate_code_evidence': 0,\n",
    "            'weak_code_evidence': 0,\n",
    "            'likely_coding_assistance': 0,\n",
    "            'conservative_coding_assistance': 0,\n",
    "        }\n",
    "        \n",
    "        for tagged_conv in conversations_to_analyze:\n",
    "            for tag in tagged_conv['tags']:\n",
    "                if tag.name in indicator_counts:\n",
    "                    indicator_counts[tag.name] += 1\n",
    "                if tag.name in evidence_counts:\n",
    "                    evidence_counts[tag.name] += 1\n",
    "        \n",
    "        total = len(conversations_to_analyze)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CODE DETECTION DEBUG\")\n",
    "        if facet_tag_name and facet_value:\n",
    "            print(f\"Filtering by: {facet_tag_name} = {facet_value}\")\n",
    "        print(f\"Analyzing {total} conversations\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(f\"\\n INDIVIDUAL INDICATOR BREAKDOWN:\")\n",
    "        for indicator, count in indicator_counts.items():\n",
    "            percentage = (count / total) * 100 if total > 0 else 0\n",
    "            print(f\"    {indicator}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\n EVIDENCE LEVEL BREAKDOWN:\")\n",
    "        for evidence, count in evidence_counts.items():\n",
    "            percentage = (count / total) * 100 if total > 0 else 0\n",
    "            print(f\"    {evidence}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Find conversations with any code indicators\n",
    "        code_indicator_conversations = []\n",
    "        for tagged_conv in conversations_to_analyze:\n",
    "            has_any_indicator = any(tag.name in indicator_counts for tag in tagged_conv['tags'])\n",
    "            if has_any_indicator:\n",
    "                code_indicator_conversations.append(tagged_conv)\n",
    "        \n",
    "        print(f\"\\n DETAILED EXAMPLES:\")\n",
    "        print(f\"Found {len(code_indicator_conversations)} conversations with code indicators\")\n",
    "        \n",
    "        # Analyze sample conversations\n",
    "        for i, tagged_conv in enumerate(code_indicator_conversations[:max_examples]):\n",
    "            print(f\"\\n--- Example {i+1}: {tagged_conv['title'][:60]}... ---\")\n",
    "            \n",
    "            # Show which indicators triggered\n",
    "            triggered_indicators = [tag.name for tag in tagged_conv['tags'] if tag.name in indicator_counts]\n",
    "            triggered_evidence = [tag.name for tag in tagged_conv['tags'] if tag.name in evidence_counts]\n",
    "            \n",
    "            print(f\"    Triggered indicators: {', '.join(triggered_indicators) if triggered_indicators else 'None'}\")\n",
    "            print(f\"    Evidence levels: {', '.join(triggered_evidence) if triggered_evidence else 'None'}\")\n",
    "            \n",
    "            # Get the raw conversation data\n",
    "            conversation = tagged_conv['conversation']\n",
    "            \n",
    "            # Analyze what triggered each detection\n",
    "            mapping = conversation.get('mapping', {})\n",
    "            \n",
    "            detailed_findings = []\n",
    "            total_text_length = 0\n",
    "            \n",
    "            for node_id, node in mapping.items():\n",
    "                message = node.get('message')\n",
    "                if not message:\n",
    "                    continue\n",
    "                \n",
    "                content = message.get('content', {})\n",
    "                text = content.get('text', '')\n",
    "                parts = content.get('parts', [])\n",
    "                all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "                total_text_length += len(all_text)\n",
    "                \n",
    "                # Check each indicator type\n",
    "                if '```' in all_text:\n",
    "                    detailed_findings.append(f\"code_blocks: found {all_text.count('```')} occurrences\")\n",
    "                \n",
    "                def_keywords = ['def ', 'function ', 'class ']\n",
    "                def_count = sum(all_text.count(kw) for kw in def_keywords)\n",
    "                if def_count > 0:\n",
    "                    detailed_findings.append(f\"function_definitions: {def_count} keywords\")\n",
    "                \n",
    "                import_keywords = ['import ', 'from ', 'require(']\n",
    "                import_count = sum(all_text.count(kw) for kw in import_keywords)\n",
    "                if import_count > 0:\n",
    "                    detailed_findings.append(f\"import_statements: {import_count} keywords\")\n",
    "                \n",
    "                script_indicators = ['#!/bin/', '#include', 'using namespace']\n",
    "                script_count = sum(all_text.count(ind) for ind in script_indicators)\n",
    "                if script_count > 0:\n",
    "                    detailed_findings.append(f\"script_headers: {script_count} indicators\")\n",
    "                \n",
    "                # Check keyword density\n",
    "                if len(all_text) > 1000:\n",
    "                    coding_keywords = ['function', 'class', 'import', 'def ', 'const ', 'let ', 'var ', 'return', 'if ', 'for ', 'while ']\n",
    "                    keyword_count = sum(1 for keyword in coding_keywords if keyword in all_text.lower())\n",
    "                    if keyword_count >= 5:\n",
    "                        detailed_findings.append(f\"high_keyword_density: {keyword_count} keywords in {len(all_text)} chars\")\n",
    "                \n",
    "                # Check structure patterns\n",
    "                patterns = [\n",
    "                    ('def ' in all_text and '(' in all_text and ':' in all_text and 'return' in all_text, 'Python function'),\n",
    "                    ('class ' in all_text and '(' in all_text and ':' in all_text and 'def ' in all_text, 'Python class'),\n",
    "                    (('function(' in all_text or 'function ' in all_text) and '{' in all_text and '}' in all_text, 'JavaScript function'),\n",
    "                    (all_text.count('=') >= 3 and ('let ' in all_text or 'const ' in all_text or 'var ' in all_text), 'Variable assignments'),\n",
    "                ]\n",
    "                \n",
    "                for pattern_check, description in patterns:\n",
    "                    if pattern_check:\n",
    "                        detailed_findings.append(f\"code_structure_patterns: {description}\")\n",
    "            \n",
    "            print(f\"    Total text length: {total_text_length}\")\n",
    "            print(f\"    Detailed findings:\")\n",
    "            if detailed_findings:\n",
    "                for finding in detailed_findings:\n",
    "                    print(f\"         {finding}\")\n",
    "            else:\n",
    "                print(f\"         No specific indicators found (potential bug)\")\n",
    "            \n",
    "            # Show a sample of the content\n",
    "            print(f\"    Content sample:\")\n",
    "            sample_texts = []\n",
    "            for node_id, node in mapping.items():\n",
    "                message = node.get('message')\n",
    "                if message:\n",
    "                    content = message.get('content', {})\n",
    "                    text = content.get('text', '')\n",
    "                    if text and len(text) > 50:\n",
    "                        sample_texts.append(text[:200] + \"...\" if len(text) > 200 else text)\n",
    "                        break\n",
    "            \n",
    "            if sample_texts:\n",
    "                print(f\"        \\\"{sample_texts[0]}\\\"\")\n",
    "            else:\n",
    "                print(f\"        No substantial text content found\")\n",
    "    \n",
    "    def suggest_improved_code_detection(self):\n",
    "        \"\"\"Suggest improvements to code detection logic.\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SUGGESTED IMPROVED CODE DETECTION\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(\"\"\"\n",
    "Current code detection is too broad. Here are suggestions for improvement:\n",
    "\n",
    "1. STRICTER INDICATORS:\n",
    "   - Require multiple strong indicators, not just one\n",
    "   - Weight indicators differently (``` is stronger than 'def ')\n",
    "   - Ignore common false positives in natural language\n",
    "\n",
    "2. CONTEXT AWARENESS:\n",
    "   - Check if code-like text is actually in code blocks (```)\n",
    "   - Look for file extensions in attachments\n",
    "   - Consider the role (user vs assistant) when evaluating code\n",
    "\n",
    "3. BETTER KEYWORD ANALYSIS:\n",
    "   - Increase threshold for keyword density detection\n",
    "   - Use more specific programming keywords\n",
    "   - Exclude common English words that happen to be programming keywords\n",
    "\n",
    "4. GIZMO-SPECIFIC RULES:\n",
    "   - Different detection logic for different gizmo types\n",
    "   - Wiki gizmos should have much stricter code detection\n",
    "   - Art/creative gizmos might mention \"class\" or \"function\" in non-coding contexts\n",
    "\n",
    "5. PROPOSED NEW LOGIC:\n",
    "   ```python\n",
    "   def improved_has_code_patterns(conversation, gizmo_context=None):\n",
    "       # Be much stricter for wiki/writing gizmos\n",
    "       if gizmo_context and 'wiki' in gizmo_context.lower():\n",
    "           return has_very_strong_code_indicators(conversation)\n",
    "       else:\n",
    "           return has_moderate_code_indicators(conversation)\n",
    "   ```\n",
    "        \"\"\")\n",
    "\n",
    "    def print_summary(self, tagged_conversations: List[Dict[str, Any]], show_details: bool = True):\n",
    "        \"\"\"Print comprehensive summary with all tag types and optional details.\"\"\"\n",
    "        total = len(tagged_conversations)\n",
    "        tag_counts = defaultdict(int)\n",
    "        tag_attributes = defaultdict(lambda: defaultdict(list))\n",
    "        unique_structured_tags = defaultdict(set)\n",
    "        \n",
    "        # Collect all tag information\n",
    "        for tagged_conv in tagged_conversations:\n",
    "            for tag in tagged_conv['tags']:\n",
    "                tag_counts[tag.name] += 1\n",
    "                \n",
    "                # Collect attribute information\n",
    "                for attr_name, attr_value in tag.attributes.items():\n",
    "                    if isinstance(attr_value, (int, float)):\n",
    "                        tag_attributes[tag.name][attr_name].append(attr_value)\n",
    "                    else:\n",
    "                        # For non-numeric attributes, track unique values\n",
    "                        unique_structured_tags[tag.name].add(f\"{attr_name}={attr_value}\")\n",
    "        \n",
    "        print(f\"Tagged {total} conversations\")\n",
    "        print(f\"\\n=== TAG SUMMARY ===\")\n",
    "        \n",
    "        # Sort tags by frequency for better readability\n",
    "        sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for tag_name, count in sorted_tags:\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"{tag_name}: {count} ({percentage:.1f}%)\")\n",
    "            \n",
    "            if show_details:\n",
    "                # Show numeric attribute statistics\n",
    "                if tag_name in tag_attributes:\n",
    "                    for attr_name, values in tag_attributes[tag_name].items():\n",
    "                        if values:\n",
    "                            avg_val = sum(values) / len(values)\n",
    "                            min_val = min(values)\n",
    "                            max_val = max(values)\n",
    "                            print(f\"    {attr_name}: avg={avg_val:.1f}, range=[{min_val}, {max_val}]\")\n",
    "                \n",
    "                # Show unique structured values for non-numeric attributes\n",
    "                if tag_name in unique_structured_tags:\n",
    "                    unique_vals = sorted(unique_structured_tags[tag_name])\n",
    "                    if len(unique_vals) <= 10:  # Show all if not too many\n",
    "                        print(f\"    values: {', '.join(unique_vals)}\")\n",
    "                    else:  # Show top 10 most common\n",
    "                        print(f\"    values: {', '.join(unique_vals[:10])} ... (+{len(unique_vals)-10} more)\")\n",
    "\n",
    "\n",
    "# Example usage functions to demonstrate the faceting capabilities\n",
    "\n",
    "def demo_faceting_usage():\n",
    "    \"\"\"\n",
    "    Demonstrate how to use the new faceting capabilities.\n",
    "    \"\"\"\n",
    "    print(\"\"\"\n",
    "# FACETING USAGE EXAMPLES\n",
    "\n",
    "# 1. Facet by gizmo presence (with/without gizmos)\n",
    "tagger.print_faceted_summary(tagged_results, 'gizmo')\n",
    "\n",
    "# 2. Facet by specific gizmo IDs  \n",
    "tagger.print_faceted_summary(tagged_results, 'gizmo', 'gizmo_id')\n",
    "\n",
    "# 3. Facet by conversation length categories\n",
    "tagger.print_faceted_summary(tagged_results, 'conversation_length', 'category')\n",
    "\n",
    "# 4. Facet by prompt length categories\n",
    "tagger.print_faceted_summary(tagged_results, 'prompt_stats', 'length_category')\n",
    "\n",
    "# 5. Facet by prompt consistency patterns\n",
    "tagger.print_faceted_summary(tagged_results, 'prompt_stats', 'consistency')\n",
    "\n",
    "# 6. Compare coding patterns across gizmos\n",
    "tagger.compare_facets(tagged_results, 'gizmo', 'gizmo_id', \n",
    "                     comparison_tags=['coding_assistance', 'code_patterns', 'web_search'])\n",
    "\n",
    "# 7. Compare conversation patterns across length categories\n",
    "tagger.compare_facets(tagged_results, 'conversation_length', 'category',\n",
    "                     comparison_tags=['coding_assistance', 'web_search', 'large_content'])\n",
    "\n",
    "# 8. Show detailed faceted summary (now shows all tags)\n",
    "tagger.print_faceted_summary(tagged_results, 'gizmo', 'gizmo_id', \n",
    "                            show_details=True, max_facets=10)\n",
    "\n",
    "# 9. Focus on specific gizmo usage patterns\n",
    "gizmo_conversations = tagger.filter_by_tags(tagged_results, include_tags=['gizmo'])\n",
    "tagger.print_faceted_summary(gizmo_conversations, 'gizmo', 'gizmo_id')\n",
    "\n",
    "# 10. Analyze how conversation patterns differ by whether they start with code\n",
    "tagger.print_faceted_summary(tagged_results, 'starts_code_patterns')\n",
    "\n",
    "# 11. Debug code detection issues (especially for wiki gizmos)\n",
    "tagger.debug_code_detection(tagged_results, 'gizmo', 'g-IibMsD7w8', max_examples=3)\n",
    "tagger.suggest_improved_code_detection()\n",
    "\n",
    "# 12. Multi-level analysis: first facet by conversation length, then by gizmo within each\n",
    "# 12. Multi-level analysis: first facet by conversation length, then by gizmo within each\n",
    "for length_cat in ['single', 'short', 'medium', 'long', 'very_long']:\n",
    "    length_conversations = tagger.filter_by_tags(\n",
    "        tagged_results, \n",
    "        include_tags=[{'name': 'conversation_length', 'category': length_cat}]\n",
    "    )\n",
    "    if length_conversations:\n",
    "        print(f\"\\\\n=== GIZMO USAGE WITHIN {length_cat.upper()} CONVERSATIONS ===\")\n",
    "        tagger.print_faceted_summary(length_conversations, 'gizmo', 'gizmo_id', max_facets=5)\n",
    "    \"\"\")\n",
    "\n",
    "# Enhanced rule functions for all previously discussed tagging rules\n",
    "\n",
    "def get_all_user_messages(conversation: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get all user messages in chronological order.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    user_messages = []\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        author = message.get('author', {})\n",
    "        if author.get('role') == 'user':\n",
    "            create_time = message.get('create_time') or 0\n",
    "            user_messages.append((create_time, message))\n",
    "    \n",
    "    user_messages.sort(key=lambda x: x[0])\n",
    "    return [msg for _, msg in user_messages]\n",
    "\n",
    "\n",
    "def get_first_user_message(conversation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Find the first user message in the conversation.\"\"\"\n",
    "    user_messages = get_all_user_messages(conversation)\n",
    "    return user_messages[0] if user_messages else None\n",
    "\n",
    "\n",
    "def create_conversation_length_tag(conversation: Dict[str, Any]) -> Tag:\n",
    "    \"\"\"Create structured tag for conversation length.\"\"\"\n",
    "    user_count = len(get_all_user_messages(conversation))\n",
    "    \n",
    "    # Determine category\n",
    "    if user_count == 1:\n",
    "        category = 'single'\n",
    "    elif user_count <= 3:\n",
    "        category = 'short'\n",
    "    elif user_count <= 10:\n",
    "        category = 'medium'\n",
    "    elif user_count <= 25:\n",
    "        category = 'long'\n",
    "    else:\n",
    "        category = 'very_long'\n",
    "    \n",
    "    return Tag('conversation_length', count=user_count, category=category)\n",
    "\n",
    "\n",
    "def create_prompt_stats_tag(conversation: Dict[str, Any]) -> Tag:\n",
    "    \"\"\"Create structured tag for prompt statistics.\"\"\"\n",
    "    user_messages = get_all_user_messages(conversation)\n",
    "    \n",
    "    if not user_messages:\n",
    "        return Tag('prompt_stats', count=0, mean=0, median=0, variance=0, \n",
    "                  length_category='none', consistency='none')\n",
    "    \n",
    "    # Calculate message lengths\n",
    "    lengths = []\n",
    "    for message in user_messages:\n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        lengths.append(len(all_text))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_length = sum(lengths) / len(lengths)\n",
    "    sorted_lengths = sorted(lengths)\n",
    "    n = len(sorted_lengths)\n",
    "    median_length = (sorted_lengths[n//2] if n % 2 == 1 \n",
    "                    else (sorted_lengths[n//2-1] + sorted_lengths[n//2]) / 2)\n",
    "    variance = sum((x - mean_length) ** 2 for x in lengths) / len(lengths) if len(lengths) > 1 else 0\n",
    "    \n",
    "    # Determine categories\n",
    "    if mean_length < 50:\n",
    "        length_category = 'very_short'\n",
    "    elif mean_length < 200:\n",
    "        length_category = 'short'\n",
    "    elif mean_length < 1000:\n",
    "        length_category = 'medium'\n",
    "    elif mean_length < 3000:\n",
    "        length_category = 'long'\n",
    "    else:\n",
    "        length_category = 'very_long'\n",
    "    \n",
    "    if variance < 1000:\n",
    "        consistency = 'consistent'\n",
    "    elif variance < 10000:\n",
    "        consistency = 'mixed'\n",
    "    else:\n",
    "        consistency = 'variable'\n",
    "    \n",
    "    return Tag('prompt_stats', \n",
    "               count=len(lengths),\n",
    "               mean=round(mean_length, 1),\n",
    "               median=round(median_length, 1),\n",
    "               variance=round(variance, 1),\n",
    "               length_category=length_category,\n",
    "               consistency=consistency)\n",
    "\n",
    "\n",
    "def create_gizmo_plugin_tags(conversation: Dict[str, Any]) -> List[Tag]:\n",
    "    \"\"\"Create structured tags for gizmos and plugins.\"\"\"\n",
    "    tags = []\n",
    "    gizmos = set()\n",
    "    plugins = set()\n",
    "    \n",
    "    # Check conversation-level\n",
    "    if conversation.get('gizmo_id'):\n",
    "        gizmos.add(conversation['gizmo_id'])\n",
    "    \n",
    "    plugin_ids = conversation.get('plugin_ids', [])\n",
    "    if plugin_ids:\n",
    "        plugins.update(plugin_ids)\n",
    "    \n",
    "    # Check message-level\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        \n",
    "        # Invoked plugins\n",
    "        invoked_plugin = metadata.get('invoked_plugin', {})\n",
    "        if invoked_plugin:\n",
    "            if invoked_plugin.get('plugin_id'):\n",
    "                plugins.add(invoked_plugin['plugin_id'])\n",
    "            if invoked_plugin.get('namespace'):\n",
    "                plugins.add(invoked_plugin['namespace'])\n",
    "        \n",
    "        # Gizmo usage\n",
    "        if metadata.get('gizmo_id'):\n",
    "            gizmos.add(metadata['gizmo_id'])\n",
    "    \n",
    "    # Create tags\n",
    "    for gizmo in gizmos:\n",
    "        tags.append(Tag('gizmo', gizmo_id=gizmo))\n",
    "    \n",
    "    for plugin in plugins:\n",
    "        tags.append(Tag('plugin', plugin_id=plugin))\n",
    "    \n",
    "    return tags\n",
    "\n",
    "\n",
    "# Individual code indicator detection functions\n",
    "def has_code_blocks(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for explicit code blocks (``` markdown syntax).\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "        \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        \n",
    "        if '```' in all_text:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_function_definitions(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for function/class definition keywords.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "        \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        \n",
    "        definition_keywords = ['def ', 'function ', 'class ']\n",
    "        if any(keyword in all_text for keyword in definition_keywords):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_import_statements(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for import/require statements.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "        \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        \n",
    "        import_keywords = ['import ', 'from ', 'require(']\n",
    "        if any(keyword in all_text for keyword in import_keywords):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_script_headers(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for script headers and system includes.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "        \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        \n",
    "        script_indicators = ['#!/bin/', '#include', 'using namespace']\n",
    "        if any(indicator in all_text for indicator in script_indicators):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_high_keyword_density(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for high density of programming keywords in large text.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "        \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        \n",
    "        # Only check substantial text\n",
    "        if len(all_text) <= 1000:\n",
    "            continue\n",
    "        \n",
    "        coding_keywords = ['function', 'class', 'import', 'def ', 'const ', 'let ', 'var ', 'return', 'if ', 'for ', 'while ']\n",
    "        keyword_count = sum(1 for keyword in coding_keywords if keyword in all_text.lower())\n",
    "        \n",
    "        # High threshold to avoid false positives in articles\n",
    "        if keyword_count >= 5:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_code_structure_patterns(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for actual code structure patterns (syntax combinations that suggest real code).\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "        \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        \n",
    "        # Look for combinations that strongly suggest actual code\n",
    "        patterns = [\n",
    "            # Function definition pattern\n",
    "            ('def ' in all_text and '(' in all_text and ':' in all_text and 'return' in all_text),\n",
    "            # Class definition pattern  \n",
    "            ('class ' in all_text and '(' in all_text and ':' in all_text and 'def ' in all_text),\n",
    "            # JavaScript function pattern\n",
    "            ('function(' in all_text or 'function ' in all_text) and '{' in all_text and '}' in all_text,\n",
    "            # Multiple assignment pattern\n",
    "            all_text.count('=') >= 3 and ('let ' in all_text or 'const ' in all_text or 'var ' in all_text),\n",
    "        ]\n",
    "        \n",
    "        if any(pattern for pattern in patterns):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "# Combined code pattern detection (uses individual indicators)\n",
    "def has_code_patterns(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for any code patterns (combines individual indicators).\"\"\"\n",
    "    return (has_code_blocks(conversation) or \n",
    "            has_function_definitions(conversation) or \n",
    "            has_import_statements(conversation) or \n",
    "            has_script_headers(conversation) or\n",
    "            has_code_structure_patterns(conversation) or\n",
    "            has_high_keyword_density(conversation))\n",
    "\n",
    "\n",
    "# Boolean rule functions for basic content analysis\n",
    "def has_large_content(conversation: Dict[str, Any], min_length: int = 2000) -> bool:\n",
    "    \"\"\"Check if conversation has unusually large content anywhere.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        if len(text) > min_length:\n",
    "            return True\n",
    "            \n",
    "        parts = content.get('parts', [])\n",
    "        for part in parts:\n",
    "            if isinstance(part, str) and len(part) > min_length:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_code_patterns(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for clear code patterns anywhere in conversation.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    # Get gizmo context for smarter detection\n",
    "    gizmo_ids = set()\n",
    "    if conversation.get('gizmo_id'):\n",
    "        gizmo_ids.add(conversation['gizmo_id'])\n",
    "    \n",
    "    # Known gizmos that should have stricter code detection\n",
    "    wiki_writing_gizmos = {\n",
    "        'g-IibMsD7w8',  # Your wiki gizmo\n",
    "        'g-pYtHuQdGh',  # Potentially another writing gizmo\n",
    "        # Add other known writing/article gizmos here\n",
    "    }\n",
    "    \n",
    "    is_writing_gizmo = bool(gizmo_ids & wiki_writing_gizmos)\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "        \n",
    "        content = message.get('content', {})\n",
    "        text = content.get('text', '')\n",
    "        parts = content.get('parts', [])\n",
    "        all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "        \n",
    "        # Very strong code indicators (reliable across all gizmos)\n",
    "        very_strong_indicators = [\n",
    "            '```',  # Code blocks\n",
    "            '#!/bin/',  # Script headers\n",
    "            '#include',  # C includes\n",
    "            'using namespace',  # C++ using\n",
    "        ]\n",
    "        \n",
    "        if any(indicator in all_text for indicator in very_strong_indicators):\n",
    "            return True\n",
    "        \n",
    "        # Medium strength indicators (require more context)\n",
    "        medium_indicators = [\n",
    "            'def ', 'function ', 'class ',  # Function/class definitions\n",
    "            'import ', 'from ', 'require(',  # Import statements\n",
    "        ]\n",
    "        \n",
    "        if is_writing_gizmo:\n",
    "            # For writing gizmos, require multiple strong indicators or very specific patterns\n",
    "            strong_indicator_count = sum(1 for indicator in medium_indicators if indicator in all_text)\n",
    "            if strong_indicator_count >= 3:  # Multiple function definitions, etc.\n",
    "                return True\n",
    "            \n",
    "            # Look for actual code structure patterns, not just keywords\n",
    "            if ('def ' in all_text and '(' in all_text and ':' in all_text and \n",
    "                'return' in all_text):  # Function definition pattern\n",
    "                return True\n",
    "                \n",
    "        else:\n",
    "            # For non-writing gizmos, use original logic\n",
    "            if any(indicator in all_text for indicator in medium_indicators):\n",
    "                return True\n",
    "            \n",
    "            # Check for high density of coding keywords (but stricter for writing gizmos)\n",
    "            coding_keywords = ['function', 'class', 'import', 'def ', 'const ', 'let ', 'var ', 'return', 'if ', 'for ', 'while ']\n",
    "            keyword_count = sum(1 for keyword in coding_keywords if keyword in all_text.lower())\n",
    "            \n",
    "            # Higher threshold for writing gizmos\n",
    "            min_keywords = 5 if is_writing_gizmo else 3\n",
    "            if len(all_text) > 1000 and keyword_count >= min_keywords:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_github_repos(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if GitHub repositories were selected for context.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        repos = metadata.get('selected_github_repos', [])\n",
    "        if repos:  # Non-empty list\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_canvas_operations(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for canvas/document operations.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if metadata.get('canvas'):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_web_search(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for web search operations.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if (metadata.get('search_queries') or \n",
    "            metadata.get('search_result_groups') or\n",
    "            metadata.get('content_references')):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_reasoning_thoughts(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for reasoning/thinking patterns.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        content = message.get('content', {})\n",
    "        if content.get('thoughts'):  # Reasoning thoughts\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def has_code_execution(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check for code execution artifacts.\"\"\"\n",
    "    mapping = conversation.get('mapping', {})\n",
    "    \n",
    "    for node_id, node in mapping.items():\n",
    "        message = node.get('message')\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        metadata = message.get('metadata', {})\n",
    "        if (metadata.get('aggregate_result') or \n",
    "            metadata.get('jupyter_messages')):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "# First user message specific rules\n",
    "def first_user_has_large_content(conversation: Dict[str, Any], min_length: int = 2000) -> bool:\n",
    "    \"\"\"Check if the first user message has large content.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    content = first_message.get('content', {})\n",
    "    text = content.get('text', '')\n",
    "    if len(text) > min_length:\n",
    "        return True\n",
    "        \n",
    "    parts = content.get('parts', [])\n",
    "    for part in parts:\n",
    "        if isinstance(part, str) and len(part) > min_length:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def first_user_has_code_patterns(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message contains code patterns.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    content = first_message.get('content', {})\n",
    "    text = content.get('text', '')\n",
    "    parts = content.get('parts', [])\n",
    "    all_text = text + ' ' + ' '.join(str(p) for p in parts if isinstance(p, str))\n",
    "    \n",
    "    # Strong code indicators\n",
    "    code_indicators = [\n",
    "        '```',  # Code blocks\n",
    "        'def ', 'function ', 'class ',  # Definitions\n",
    "        'import ', 'from ', 'require(',  # Imports\n",
    "        '#!/bin/', '#include',  # Script headers\n",
    "    ]\n",
    "    \n",
    "    return any(indicator in all_text for indicator in code_indicators)\n",
    "\n",
    "\n",
    "def first_user_has_attachments(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message has attachments.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    metadata = first_message.get('metadata', {})\n",
    "    attachments = metadata.get('attachments', [])\n",
    "    return len(attachments) > 0\n",
    "\n",
    "\n",
    "def first_user_has_code_attachments(conversation: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if the first user message has code-related attachments.\"\"\"\n",
    "    first_message = get_first_user_message(conversation)\n",
    "    if not first_message:\n",
    "        return False\n",
    "    \n",
    "    metadata = first_message.get('metadata', {})\n",
    "    attachments = metadata.get('attachments', [])\n",
    "    \n",
    "    for attachment in attachments:\n",
    "        mime_type = attachment.get('mime_type', '').lower()\n",
    "        name = attachment.get('name', '').lower()\n",
    "        \n",
    "        # Check for code file extensions\n",
    "        code_extensions = ['.py', '.js', '.java', '.cpp', '.c', '.go', '.rs', '.ts', '.jsx', '.tsx', '.sql', '.sh', '.rb', '.php']\n",
    "        if any(ext in name for ext in code_extensions):\n",
    "            return True\n",
    "            \n",
    "        # Check for code-related MIME types\n",
    "        code_mimes = ['text/x-python', 'text/x-java', 'application/javascript', 'text/x-script']\n",
    "        if any(mime in mime_type for mime in code_mimes):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def create_default_tagger() -> ConversationTagger:\n",
    "    \"\"\"Create a tagger with all previously discussed rules in the enhanced structured framework.\"\"\"\n",
    "    tagger = ConversationTagger()\n",
    "    \n",
    "    # ===== BASIC CONTENT ANALYSIS RULES (Boolean) =====\n",
    "    tagger.add_base_rule(\n",
    "        'large_content', \n",
    "        lambda conv: has_large_content(conv, 2000),\n",
    "        'Content longer than 2000 characters anywhere in conversation'\n",
    "    )\n",
    "    \n",
    "    # ===== INDIVIDUAL CODE INDICATOR RULES =====\n",
    "    tagger.add_base_rule(\n",
    "        'code_blocks',\n",
    "        has_code_blocks,\n",
    "        'Contains explicit code blocks (``` syntax)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'function_definitions',\n",
    "        has_function_definitions,\n",
    "        'Contains function/class definition keywords (def, function, class)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'import_statements',\n",
    "        has_import_statements,\n",
    "        'Contains import/require statements'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'script_headers',\n",
    "        has_script_headers,\n",
    "        'Contains script headers or system includes (#!/bin/, #include, using namespace)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'high_keyword_density',\n",
    "        has_high_keyword_density,\n",
    "        'High density of programming keywords in substantial text (5+ keywords in 1000+ chars)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'code_structure_patterns',\n",
    "        has_code_structure_patterns,\n",
    "        'Contains actual code structure patterns (function definitions with syntax)'\n",
    "    )\n",
    "    \n",
    "    # ===== LEGACY COMBINED CODE RULE =====\n",
    "    tagger.add_base_rule(\n",
    "        'code_patterns', \n",
    "        has_code_patterns,\n",
    "        'Contains any code patterns (combines individual indicators)'\n",
    "    )\n",
    "    \n",
    "    # ===== OTHER BASIC RULES =====\n",
    "    tagger.add_base_rule(\n",
    "        'github_context',\n",
    "        has_github_repos,\n",
    "        'GitHub repositories selected for context'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'canvas_operations',\n",
    "        has_canvas_operations,\n",
    "        'Uses canvas/document features'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'web_search',\n",
    "        has_web_search,\n",
    "        'Includes web search functionality'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'reasoning',\n",
    "        has_reasoning_thoughts,\n",
    "        'Contains reasoning/thinking content'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'code_execution',\n",
    "        has_code_execution,\n",
    "        'Contains code execution (Jupyter, aggregate results)'\n",
    "    )\n",
    "    \n",
    "    # ===== FIRST USER MESSAGE RULES (Boolean) =====\n",
    "    tagger.add_base_rule(\n",
    "        'starts_large_content',\n",
    "        lambda conv: first_user_has_large_content(conv, 2000),\n",
    "        'First user message has large content (>2000 chars)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_code_patterns',\n",
    "        first_user_has_code_patterns,\n",
    "        'First user message contains code patterns'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_with_attachments',\n",
    "        first_user_has_attachments,\n",
    "        'First user message has any attachments'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'starts_code_attachments',\n",
    "        first_user_has_code_attachments,\n",
    "        'First user message has code-related attachments'\n",
    "    )\n",
    "    \n",
    "    # ===== STRUCTURED TAG RULES =====\n",
    "    tagger.add_base_rule(\n",
    "        'conversation_length',\n",
    "        create_conversation_length_tag,\n",
    "        'Conversation length with count and category'\n",
    "    )\n",
    "    \n",
    "    tagger.add_base_rule(\n",
    "        'prompt_stats',\n",
    "        create_prompt_stats_tag,\n",
    "        'User message statistics (length, variance, etc.)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_multi_tag_rule(\n",
    "        'gizmo_plugin_usage',\n",
    "        create_gizmo_plugin_tags,\n",
    "        'Specific gizmos and plugins used in conversation'\n",
    "    )\n",
    "    \n",
    "    # ===== SUPPLEMENTAL RULES (Based on existing tags) =====\n",
    "    \n",
    "    # Strong code evidence (very reliable indicators)\n",
    "    tagger.add_supplemental_rule(\n",
    "        'strong_code_evidence',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['code_blocks', 'script_headers', 'code_structure_patterns'] for tag in tags)\n",
    "        ),\n",
    "        'Strong evidence of actual code (blocks, headers, or structure patterns)'\n",
    "    )\n",
    "    \n",
    "    # Moderate code evidence (could be false positives in articles)\n",
    "    tagger.add_supplemental_rule(\n",
    "        'moderate_code_evidence',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['function_definitions', 'import_statements'] for tag in tags)\n",
    "        ),\n",
    "        'Moderate evidence of code (keywords that could appear in articles)'\n",
    "    )\n",
    "    \n",
    "    # Weak code evidence (likely false positive in articles)\n",
    "    tagger.add_supplemental_rule(\n",
    "        'weak_code_evidence',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'high_keyword_density' for tag in tags) and\n",
    "            not any(tag.name in ['strong_code_evidence', 'moderate_code_evidence'] for tag in tags)\n",
    "        ),\n",
    "        'Weak evidence of code (keyword density only, no other indicators)'\n",
    "    )\n",
    "    \n",
    "    # Intelligent coding assistance detection\n",
    "    tagger.add_supplemental_rule(\n",
    "        'likely_coding_assistance',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'strong_code_evidence' for tag in tags) or\n",
    "            (any(tag.name == 'moderate_code_evidence' for tag in tags) and\n",
    "             any(tag.name in ['github_context', 'code_execution'] for tag in tags)) or\n",
    "            (len([tag for tag in tags if tag.name in ['function_definitions', 'import_statements', 'code_structure_patterns']]) >= 2)\n",
    "        ),\n",
    "        'Likely actual coding assistance (strong evidence OR moderate + context OR multiple indicators)'\n",
    "    )\n",
    "    \n",
    "    # Conservative coding assistance (stricter threshold)\n",
    "    tagger.add_supplemental_rule(\n",
    "        'conservative_coding_assistance',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'strong_code_evidence' for tag in tags) or\n",
    "            any(tag.name in ['github_context', 'code_execution'] for tag in tags)\n",
    "        ),\n",
    "        'Conservative coding assistance detection (only strongest indicators)'\n",
    "    )\n",
    "    \n",
    "    # Legacy coding assistance (original broad logic)\n",
    "    tagger.add_supplemental_rule(\n",
    "        'coding_assistance',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['code_patterns', 'github_context', 'code_execution'] for tag in tags)\n",
    "        ),\n",
    "        'Legacy coding assistance (broad detection for comparison)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'coding_assistance_start',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['starts_code_patterns', 'starts_large_content', \n",
    "                           'starts_with_attachments', 'starts_code_attachments'] for tag in tags)\n",
    "        ),\n",
    "        'Likely coding assistance based on how conversation starts'\n",
    "    )\n",
    "    \n",
    "    # Research and analysis patterns  \n",
    "    tagger.add_supplemental_rule(\n",
    "        'research_session',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'web_search' for tag in tags) and \n",
    "            any(tag.name == 'large_content' for tag in tags)\n",
    "        ),\n",
    "        'Research session (web search + substantial content)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'complex_analysis',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'reasoning' for tag in tags) and \n",
    "            len([tag for tag in tags if tag.name in ['web_search', 'large_content', 'canvas_operations']]) >= 2\n",
    "        ),\n",
    "        'Complex analysis (reasoning + multiple advanced features)'\n",
    "    )\n",
    "    \n",
    "    # Context and interaction patterns\n",
    "    tagger.add_supplemental_rule(\n",
    "        'context_heavy_start',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['starts_large_content', 'starts_with_attachments'] for tag in tags)\n",
    "        ),\n",
    "        'Conversation starts with substantial context'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'enhanced_conversation',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name in ['gizmo', 'plugin'] for tag in tags)\n",
    "        ),\n",
    "        'Uses enhanced features (gizmos or plugins)'\n",
    "    )\n",
    "    \n",
    "    # Length-based classifications using structured tags\n",
    "    tagger.add_supplemental_rule(\n",
    "        'brief_interaction',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('category') in ['single', 'short'] for tag in tags)\n",
    "        ),\n",
    "        'Brief interaction (1-3 user messages)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'extended_conversation',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('category') in ['long', 'very_long'] for tag in tags)\n",
    "        ),\n",
    "        'Extended conversation (11+ user messages)'\n",
    "    )\n",
    "    \n",
    "    # Prompt pattern classifications using structured tags\n",
    "    tagger.add_supplemental_rule(\n",
    "        'long_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('length_category') in ['long', 'very_long'] for tag in tags)\n",
    "        ),\n",
    "        'Conversation has consistently long prompts'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'short_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('length_category') in ['very_short', 'short'] for tag in tags)\n",
    "        ),\n",
    "        'Conversation has consistently short prompts'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'consistent_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'consistent' for tag in tags)\n",
    "        ),\n",
    "        'User prompts are consistent in length'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'variable_prompts',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'variable' for tag in tags)\n",
    "        ),\n",
    "        'User prompts vary significantly in length'\n",
    "    )\n",
    "    \n",
    "    # Combined patterns for specific use cases\n",
    "    tagger.add_supplemental_rule(\n",
    "        'context_dump',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('category') in ['single', 'short'] for tag in tags) and\n",
    "            any(tag.name in ['starts_large_content', 'starts_with_attachments'] for tag in tags)\n",
    "        ),\n",
    "        'Short conversation starting with large context (likely context dump)'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'interactive_session',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('count', 0) >= 5 for tag in tags) and\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'consistent' for tag in tags)\n",
    "        ),\n",
    "        'Extended back-and-forth conversation with consistent prompt style'\n",
    "    )\n",
    "    \n",
    "    tagger.add_supplemental_rule(\n",
    "        'evolving_discussion',\n",
    "        lambda conv, tags: (\n",
    "            any(tag.name == 'conversation_length' and \n",
    "                tag.attributes.get('count', 0) >= 5 for tag in tags) and\n",
    "            any(tag.name == 'prompt_stats' and \n",
    "                tag.attributes.get('consistency') == 'variable' for tag in tags)\n",
    "        ),\n",
    "        'Extended conversation where prompt style evolves'\n",
    "    )\n",
    "    \n",
    "    return tagger\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03d86c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged 1673 conversations\n",
      "\n",
      "=== TAG SUMMARY ===\n",
      "prompt_stats: 1673 (100.0%)\n",
      "    count: avg=7.6, range=[1, 160]\n",
      "    mean: avg=1142.4, range=[1.0, 184662.5]\n",
      "    median: avg=814.2, range=[1.0, 184662.5]\n",
      "    variance: avg=51563065.2, range=[0, 34065162056.2]\n",
      "    values: consistency=consistent, consistency=mixed, consistency=variable, length_category=long, length_category=medium, length_category=short, length_category=very_long, length_category=very_short\n",
      "conversation_length: 1673 (100.0%)\n",
      "    count: avg=7.6, range=[1, 160]\n",
      "    values: category=long, category=medium, category=short, category=single, category=very_long\n",
      "moderate_code_evidence: 1485 (88.8%)\n",
      "import_statements: 1457 (87.1%)\n",
      "large_content: 1362 (81.4%)\n",
      "short_prompts: 1261 (75.4%)\n",
      "coding_assistance: 1005 (60.1%)\n",
      "code_patterns: 1000 (59.8%)\n",
      "likely_coding_assistance: 914 (54.6%)\n",
      "brief_interaction: 906 (54.2%)\n",
      "consistent_prompts: 896 (53.6%)\n",
      "function_definitions: 809 (48.4%)\n",
      "conservative_coding_assistance: 754 (45.1%)\n",
      "strong_code_evidence: 743 (44.4%)\n",
      "enhanced_conversation: 681 (40.7%)\n",
      "gizmo: 676 (40.4%)\n",
      "    values: gizmo_id=g-IibMsD7w8, gizmo_id=g-KpF6lTka3, gizmo_id=g-QsUj0Smzg, gizmo_id=g-WiEAUBGzb, gizmo_id=g-bWPVPw7oK, gizmo_id=g-pYtHuQdGh\n",
      "code_structure_patterns: 621 (37.1%)\n",
      "high_keyword_density: 539 (32.2%)\n",
      "variable_prompts: 405 (24.2%)\n",
      "code_blocks: 317 (18.9%)\n",
      "extended_conversation: 304 (18.2%)\n",
      "coding_assistance_start: 260 (15.5%)\n",
      "evolving_discussion: 247 (14.8%)\n",
      "starts_code_patterns: 240 (14.3%)\n",
      "interactive_session: 184 (11.0%)\n",
      "long_prompts: 151 (9.0%)\n",
      "context_heavy_start: 127 (7.6%)\n",
      "starts_large_content: 119 (7.1%)\n",
      "web_search: 66 (3.9%)\n",
      "context_dump: 64 (3.8%)\n",
      "research_session: 53 (3.2%)\n",
      "code_execution: 26 (1.6%)\n",
      "script_headers: 17 (1.0%)\n",
      "reasoning: 12 (0.7%)\n",
      "plugin: 11 (0.7%)\n",
      "    values: plugin_id=AskTheCode, plugin_id=chatgpt_production_alltrails_com__jit_plugin, plugin_id=chatwithvideo, plugin_id=code_repo_interaction, plugin_id=g-eaf0e78bb71845ae7b86a9268d1374c168f5bd85, plugin_id=plugin-25ce675b-c2c4-4460-ad33-8e641653498c, plugin_id=plugin-3cf29a7b-2fcb-42aa-b762-4d418b543a8b, plugin_id=plugin-b82d4d0b-79b8-4548-b1b4-65b218376d6f, plugin_id=plugin-e5009c08-c6c8-4195-977f-16f39a7d3b7b, plugin_id=scholar_assist\n",
      "starts_with_attachments: 8 (0.5%)\n",
      "complex_analysis: 5 (0.3%)\n",
      "canvas_operations: 3 (0.2%)\n",
      "weak_code_evidence: 1 (0.1%)\n",
      "Tagged 1673 conversations\n",
      "Faceted by: gizmo.gizmo_id\n",
      "Found 7 facet values\n",
      "\n",
      "================================================================================\n",
      "FACETED TAG SUMMARY\n",
      "================================================================================\n",
      "\n",
      " FACET: <none>\n",
      "    Conversations: 997 (59.6% of total)\n",
      "    ------------------------------------------------------------\n",
      "    prompt_stats: 997 (100.0%)\n",
      "    conversation_length: 997 (100.0%)\n",
      "    coding_assistance: 891 (89.4%)\n",
      "    code_patterns: 886 (88.9%)\n",
      "    moderate_code_evidence: 840 (84.3%)\n",
      "    import_statements: 831 (83.4%)\n",
      "    large_content: 696 (69.8%)\n",
      "    short_prompts: 627 (62.9%)\n",
      "    brief_interaction: 606 (60.8%)\n",
      "    likely_coding_assistance: 433 (43.4%)\n",
      "    consistent_prompts: 427 (42.8%)\n",
      "    conservative_coding_assistance: 340 (34.1%)\n",
      "    function_definitions: 337 (33.8%)\n",
      "    variable_prompts: 330 (33.1%)\n",
      "    strong_code_evidence: 329 (33.0%)\n",
      "    code_blocks: 264 (26.5%)\n",
      "    coding_assistance_start: 236 (23.7%)\n",
      "    high_keyword_density: 229 (23.0%)\n",
      "    starts_code_patterns: 216 (21.7%)\n",
      "    code_structure_patterns: 211 (21.2%)\n",
      "    evolving_discussion: 191 (19.2%)\n",
      "    long_prompts: 139 (13.9%)\n",
      "    context_heavy_start: 119 (11.9%)\n",
      "    extended_conversation: 112 (11.2%)\n",
      "    starts_large_content: 111 (11.1%)\n",
      "    context_dump: 60 (6.0%)\n",
      "    web_search: 54 (5.4%)\n",
      "    research_session: 41 (4.1%)\n",
      "    code_execution: 26 (2.6%)\n",
      "    interactive_session: 18 (1.8%)\n",
      "    script_headers: 17 (1.7%)\n",
      "    reasoning: 12 (1.2%)\n",
      "    plugin: 9 (0.9%)\n",
      "    starts_with_attachments: 8 (0.8%)\n",
      "    complex_analysis: 5 (0.5%)\n",
      "    enhanced_conversation: 5 (0.5%)\n",
      "    canvas_operations: 3 (0.3%)\n",
      "    weak_code_evidence: 1 (0.1%)\n",
      "\n",
      " FACET: g-IibMsD7w8\n",
      "    Conversations: 668 (39.9% of total)\n",
      "    ------------------------------------------------------------\n",
      "    gizmo: 668 (100.0%)\n",
      "    conversation_length: 668 (100.0%)\n",
      "    prompt_stats: 668 (100.0%)\n",
      "    enhanced_conversation: 668 (100.0%)\n",
      "    large_content: 664 (99.4%)\n",
      "    moderate_code_evidence: 641 (96.0%)\n",
      "    short_prompts: 628 (94.0%)\n",
      "    import_statements: 622 (93.1%)\n",
      "    likely_coding_assistance: 479 (71.7%)\n",
      "    function_definitions: 470 (70.4%)\n",
      "    consistent_prompts: 463 (69.3%)\n",
      "    conservative_coding_assistance: 412 (61.7%)\n",
      "    strong_code_evidence: 412 (61.7%)\n",
      "    code_structure_patterns: 408 (61.1%)\n",
      "    high_keyword_density: 308 (46.1%)\n",
      "    brief_interaction: 294 (44.0%)\n",
      "    extended_conversation: 191 (28.6%)\n",
      "    interactive_session: 166 (24.9%)\n",
      "    code_patterns: 110 (16.5%)\n",
      "    coding_assistance: 110 (16.5%)\n",
      "    variable_prompts: 74 (11.1%)\n",
      "    evolving_discussion: 55 (8.2%)\n",
      "    code_blocks: 52 (7.8%)\n",
      "    coding_assistance_start: 23 (3.4%)\n",
      "    starts_code_patterns: 23 (3.4%)\n",
      "    long_prompts: 12 (1.8%)\n",
      "    research_session: 11 (1.6%)\n",
      "    web_search: 11 (1.6%)\n",
      "    starts_large_content: 8 (1.2%)\n",
      "    context_heavy_start: 8 (1.2%)\n",
      "    context_dump: 4 (0.6%)\n",
      "\n",
      " FACET: g-bWPVPw7oK\n",
      "    Conversations: 4 (0.2% of total)\n",
      "    ------------------------------------------------------------\n",
      "    gizmo: 4 (100.0%)\n",
      "    short_prompts: 4 (100.0%)\n",
      "    prompt_stats: 4 (100.0%)\n",
      "    enhanced_conversation: 4 (100.0%)\n",
      "    conversation_length: 4 (100.0%)\n",
      "    brief_interaction: 3 (75.0%)\n",
      "    consistent_prompts: 3 (75.0%)\n",
      "    code_patterns: 2 (50.0%)\n",
      "    coding_assistance: 2 (50.0%)\n",
      "    import_statements: 2 (50.0%)\n",
      "    moderate_code_evidence: 2 (50.0%)\n",
      "    extended_conversation: 1 (25.0%)\n",
      "\n",
      " FACET: g-WiEAUBGzb\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    likely_coding_assistance: 1 (100.0%)\n",
      "    code_patterns: 1 (100.0%)\n",
      "    coding_assistance: 1 (100.0%)\n",
      "    brief_interaction: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    conservative_coding_assistance: 1 (100.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "    import_statements: 1 (100.0%)\n",
      "    function_definitions: 1 (100.0%)\n",
      "    strong_code_evidence: 1 (100.0%)\n",
      "    short_prompts: 1 (100.0%)\n",
      "    high_keyword_density: 1 (100.0%)\n",
      "    code_structure_patterns: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "    consistent_prompts: 1 (100.0%)\n",
      "    moderate_code_evidence: 1 (100.0%)\n",
      "    large_content: 1 (100.0%)\n",
      "\n",
      " FACET: g-pYtHuQdGh\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    coding_assistance_start: 1 (100.0%)\n",
      "    function_definitions: 1 (100.0%)\n",
      "    strong_code_evidence: 1 (100.0%)\n",
      "    variable_prompts: 1 (100.0%)\n",
      "    coding_assistance: 1 (100.0%)\n",
      "    research_session: 1 (100.0%)\n",
      "    starts_code_patterns: 1 (100.0%)\n",
      "    import_statements: 1 (100.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "    high_keyword_density: 1 (100.0%)\n",
      "    code_structure_patterns: 1 (100.0%)\n",
      "    moderate_code_evidence: 1 (100.0%)\n",
      "    likely_coding_assistance: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    web_search: 1 (100.0%)\n",
      "    large_content: 1 (100.0%)\n",
      "    code_patterns: 1 (100.0%)\n",
      "    evolving_discussion: 1 (100.0%)\n",
      "    conservative_coding_assistance: 1 (100.0%)\n",
      "    code_blocks: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "\n",
      " FACET: g-QsUj0Smzg\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    brief_interaction: 1 (100.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "    short_prompts: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "    consistent_prompts: 1 (100.0%)\n",
      "\n",
      " FACET: g-KpF6lTka3\n",
      "    Conversations: 1 (0.1% of total)\n",
      "    ------------------------------------------------------------\n",
      "    plugin: 2 (200.0%)\n",
      "    gizmo: 1 (100.0%)\n",
      "    brief_interaction: 1 (100.0%)\n",
      "    conversation_length: 1 (100.0%)\n",
      "    prompt_stats: 1 (100.0%)\n",
      "    enhanced_conversation: 1 (100.0%)\n",
      "    consistent_prompts: 1 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "tagger = create_default_tagger()\n",
    "tagged_results = tagger.tag_conversations(convs)\n",
    "tagger.print_summary(tagged_results)\n",
    "tagger.print_faceted_summary(tagged_results, 'gizmo', 'gizmo_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0032c591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dmarx/proj/chat2obs/src\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "import json\n",
    "PATH=str((Path().cwd().parent /'src').absolute())\n",
    "print(PATH)\n",
    "if PATH not in sys.path:\n",
    "    sys.path.append(PATH)\n",
    "\n",
    "root = \"../data/ingestion/chatgpt/a40ff5f79c1b3edd3c366f0f628fb79170bae83ecf3a1758b5b258c71f843f53-2025-06-05-03-28-15-df2ed357a4e64443bf464446686c9692/\"\n",
    "fpath = Path(root) / \"conversations.json\"\n",
    "convs = json.load(fpath.open())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1bf4890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversation_tagger import create_default_tagger\n",
    "from conversation_tagger.core.exchange_tagger import ExchangeTagger, DEFAULT_EXCHANGE_RULES\n",
    "tagger = create_default_tagger()\n",
    "#[tagger.add_exchange_rule(k, v) for k, v in DEFAULT_EXCHANGE_RULES.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9481e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'has_wiki_links': <function conversation_tagger.core.exchange_tagger.has_wiki_links(exchange: conversation_tagger.core.exchange.Exchange) -> bool>,\n",
       " 'has_latex_math': <function conversation_tagger.core.exchange_tagger.has_latex_math(exchange: conversation_tagger.core.exchange.Exchange) -> bool>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_EXCHANGE_RULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76783a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_results = [tagger.tag_conversation(c) for c in convs]\n",
    "\n",
    "#tagger.print_summary(tagged_results)\n",
    "#tagger.print_faceted_summary(tagged_results, 'gizmo', 'gizmo_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93de7303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tag('has_multiple_turns', {})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_results[0].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5048a9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged 1673 conversations\n",
      "  has_latex_math: 643 (38.4%)\n",
      "  has_wiki_links: 547 (32.7%)\n",
      "  has_multiple_turns: 432 (25.8%)\n"
     ]
    }
   ],
   "source": [
    "def simple_summary(tagged_results):\n",
    "    from collections import Counter\n",
    "    all_tags = []\n",
    "    for result in tagged_results:\n",
    "        all_tags.extend([tag.name for tag in result.tags])\n",
    "    \n",
    "    tag_counts = Counter(all_tags)\n",
    "    print(f\"Tagged {len(tagged_results)} conversations\")\n",
    "    for tag, count in tag_counts.most_common():\n",
    "        pct = count / len(tagged_results) * 100\n",
    "        print(f\"  {tag}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "simple_summary(tagged_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24612900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged 1673 conversations\n",
      "Faceted by: gizmo\n",
      "Found 2 facet values\n",
      "\n",
      "================================================================================\n",
      "FACETED TAG SUMMARY\n",
      "================================================================================\n",
      "\n",
      " FACET: <none>\n",
      "    Conversations: 997 (59.6% of total)\n",
      "    ------------------------------------------------------------\n",
      "    exchange_count: 997 (100.0%)\n",
      "    has_assistant_has_code_blocks: 261 (26.2%)\n",
      "    has_user_has_code_blocks: 29 (2.9%)\n",
      "    plugin: 5 (0.5%)\n",
      "\n",
      " FACET: has_gizmo\n",
      "    Conversations: 676 (40.4% of total)\n",
      "    ------------------------------------------------------------\n",
      "    exchange_count: 676 (100.0%)\n",
      "    gizmo: 676 (100.0%)\n",
      "    has_assistant_has_code_blocks: 53 (7.8%)\n",
      "    has_user_has_code_blocks: 1 (0.1%)\n"
     ]
    }
   ],
   "source": [
    "from conversation_tagger.analysis.faceting import print_faceted_summary #, facet_conversations\n",
    "\n",
    "#print_faceted_summary(tagged_results, 'gizmo', 'gizmo_id')\n",
    "print_faceted_summary(tagged_results, 'gizmo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc8d5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
