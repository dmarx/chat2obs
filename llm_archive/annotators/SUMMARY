---
File: llm_archive/annotators/__init__.py
---
# llm_archive/annotators/__init__.py
"""Annotation infrastructure for entities.

Annotators analyze entities and produce annotations stored in derived.annotations.

Architecture Overview:
---------------------

**Annotation Keys vs Annotators (Strategy Pattern)**

An ANNOTATION_KEY identifies what we're trying to detect (e.g., 'code', 'latex').
Multiple annotators can target the same key using different strategies.
Higher PRIORITY annotators run first; lower-priority ones can be skipped
if the key is already satisfied.

Example: Detecting code in an exchange
  - ChatGPTCodeExecutionAnnotator (priority=100): Platform ground truth
  - CodeBlockAnnotator (priority=90): Explicit ``` blocks  
  - CodeStructureAnnotator (priority=70): Function/class patterns
  - CodeKeywordDensityAnnotator (priority=30): Keyword density

If code execution is detected (priority 100), lower-priority heuristics
can check has_annotation_key() to skip redundant work.

**Annotation Types**
- tag: For filtering (topic:physics, quality:high)
- feature: Detected features (has_code_blocks, has_latex)
- metadata: Structural data (dialogue_length, prompt_stats)
- title: Generated titles
- summary: Brief descriptions

**Entity Types**
- message: Individual messages
- exchange: User prompt + assistant response pair
- dialogue: Entire conversation

Creating Custom Annotators:
--------------------------

For MESSAGE annotations based on text content:

    class MyMessageAnnotator(MessageTextAnnotator):
        ANNOTATION_TYPE = 'feature'
        ANNOTATION_KEY = 'my_feature'  # What we're detecting
        PRIORITY = 50                   # When to run (higher = first)
        VERSION = '1.0'
        ROLE_FILTER = 'assistant'       # or 'user' or None for all
        
        def annotate(self, data: MessageTextData) -> list[AnnotationResult]:
            if 'keyword' in data.text:
                return [AnnotationResult(value='has_keyword', confidence=0.9)]
            return []

For EXCHANGE annotations based on content:

    class MyExchangeAnnotator(ExchangeAnnotator):
        ANNOTATION_TYPE = 'tag'
        ANNOTATION_KEY = 'my_tag'
        PRIORITY = 50
        VERSION = '1.0'
        
        def annotate(self, data: ExchangeData) -> list[AnnotationResult]:
            if (data.assistant_word_count or 0) > 1000:
                return [AnnotationResult(value='long_response', key='length')]
            return []

For EXCHANGE annotations based on platform features (e.g., ChatGPT):

    class MyChatGPTAnnotator(ExchangePlatformAnnotator):
        ANNOTATION_TYPE = 'feature'
        ANNOTATION_KEY = 'platform_feature'
        PRIORITY = 100  # Platform = ground truth
        VERSION = '1.0'
        
        def annotate(self, data: ExchangePlatformData) -> list[AnnotationResult]:
            # Query platform tables using data.message_ids
            ...

For DIALOGUE annotations with aggregate statistics:

    class MyDialogueAnnotator(DialogueAnnotator):
        ANNOTATION_TYPE = 'metadata'
        ANNOTATION_KEY = 'my_stats'
        PRIORITY = 50
        VERSION = '1.0'
        
        def annotate(self, data: DialogueData) -> list[AnnotationResult]:
            if data.exchange_count > 10:
                return [AnnotationResult(value='extended', key='length')]
            return []

Priority Guidelines:
- 100: Platform features (ground truth from database)
- 90: Explicit syntax (```, shebangs)
- 70: Structural patterns (function definitions)
- 50: Keyword detection (default)
- 30: Density/heuristic analysis

Bump VERSION to reprocess all entities with new logic.
"""

from llm_archive.annotators.prompt_response import (
    PromptResponseAnnotator,
    PromptResponseData,
    WikiCandidateAnnotator,
    NaiveTitleAnnotator,
)

__all__ = [

    # Prompt-response annotators
    "PromptResponseAnnotator",
    "PromptResponseData",
    "WikiCandidateAnnotator",
    "NaiveTitleAnnotator",
]



---
File: llm_archive/annotators/content_part.py
---
# llm_archive/annotators/content_part.py
"""
Content-part level annotators.

These annotators work on individual content_parts within messages,
detecting features like code blocks, LaTeX, and other content types.
"""

import re
from abc import abstractmethod
from dataclasses import dataclass
from datetime import datetime
from typing import Iterator
from uuid import UUID

from sqlalchemy import text
from sqlalchemy.orm import Session

from llm_archive.annotations.core import (
    AnnotationWriter,
    AnnotationReader,
    AnnotationResult,
    EntityType,
    ValueType,
)


@dataclass
class ContentPartData:
    """Data passed to content-part annotation logic."""
    content_part_id: UUID
    message_id: UUID
    dialogue_id: UUID
    sequence: int
    part_type: str
    text_content: str | None
    language: str | None
    role: str
    created_at: datetime | None


class ContentPartAnnotator:
    """
    Base class for annotating content parts.
    
    Iterates over raw.content_parts joined with raw.messages.
    Supports annotation prerequisites and skip conditions.
    """
    
    ENTITY_TYPE = EntityType.CONTENT_PART
    ANNOTATION_KEY: str = ''  # Subclass must define
    VALUE_TYPE: ValueType = ValueType.FLAG
    PRIORITY: int = 50  # Higher = runs first
    
    # Annotation filters
    REQUIRES_FLAGS: list[str] = []
    REQUIRES_STRINGS: list[tuple[str, str]] = []
    SKIP_IF_FLAGS: list[str] = []
    SKIP_IF_STRINGS: list[tuple[str, ...]] = []
    
    # Content filters
    PART_TYPE_FILTER: str | None = None  # Limit to specific part_type
    ROLE_FILTER: str | None = None  # Limit to 'user' or 'assistant'
    
    def __init__(self, session: Session):
        self.session = session
        self.writer = AnnotationWriter(session)
        self.reader = AnnotationReader(session)
    
    def compute(self) -> int:
        """Run annotation over content parts."""
        count = 0
        for data in self._iter_content_parts():
            results = self.annotate(data)
            for result in results:
                if self._write_result(data.content_part_id, result):
                    count += 1
        return count
    
    def _write_result(self, entity_id: UUID, result: AnnotationResult) -> bool:
        """Write an annotation result to the appropriate table."""
        return self.writer.write(EntityType.CONTENT_PART, entity_id, result)
    
    def _iter_content_parts(self) -> Iterator[ContentPartData]:
        """Iterate over content parts, respecting filters."""
        # Build base query
        query = """
            SELECT 
                cp.id as content_part_id,
                cp.message_id,
                m.dialogue_id,
                cp.sequence,
                cp.part_type,
                cp.text_content,
                cp.language,
                m.role,
                m.created_at
            FROM raw.content_parts cp
            JOIN raw.messages m ON m.id = cp.message_id
            WHERE m.deleted_at IS NULL
        """
        
        params = {}
        
        # Add part_type filter
        if self.PART_TYPE_FILTER:
            query += " AND cp.part_type = :part_type"
            params['part_type'] = self.PART_TYPE_FILTER
        
        # Add role filter
        if self.ROLE_FILTER:
            query += " AND m.role = :role"
            params['role'] = self.ROLE_FILTER
        
        query += " ORDER BY m.dialogue_id, m.created_at, cp.sequence"
        
        result = self.session.execute(text(query), params)
        
        for row in result:
            yield ContentPartData(
                content_part_id=row.content_part_id,
                message_id=row.message_id,
                dialogue_id=row.dialogue_id,
                sequence=row.sequence,
                part_type=row.part_type,
                text_content=row.text_content,
                language=row.language,
                role=row.role,
                created_at=row.created_at,
            )
    
    @abstractmethod
    def annotate(self, data: ContentPartData) -> list[AnnotationResult]:
        """
        Analyze content part and return annotations to create.
        
        Args:
            data: ContentPartData with content and metadata
            
        Returns:
            List of AnnotationResult objects (empty list if no match)
        """
        pass


# ============================================================
# Code Detection Annotators
# ============================================================

class CodeBlockAnnotator(ContentPartAnnotator):
    """
    Detect explicit code blocks (```) in text content parts.
    
    Highest priority code detector - explicit markdown code blocks
    are the most reliable signal.
    
    Produces:
    - has_code_block FLAG
    - code_block_count NUMERIC
    - code_languages STRING (multi-value)
    """
    
    ANNOTATION_KEY = 'has_code_block'
    VALUE_TYPE = ValueType.FLAG
    PRIORITY = 90
    PART_TYPE_FILTER = 'text'
    ROLE_FILTER = 'assistant'
    
    # Pattern to match complete code blocks: ```lang\n...content...```
    # Captures the optional language specifier
    CODE_BLOCK_PATTERN = re.compile(r'```(\w*)\n?[\s\S]*?```')
    
    def annotate(self, data: ContentPartData) -> list[AnnotationResult]:
        if not data.text_content:
            return []
        
        # Find all complete code blocks
        matches = list(self.CODE_BLOCK_PATTERN.finditer(data.text_content))
        
        if not matches:
            return []
        
        results = []
        
        # Flag annotation
        results.append(AnnotationResult(
            key='has_code_block',
            value_type=ValueType.FLAG,
            reason='markdown_code_block',
            confidence=1.0,
        ))
        
        # Count annotation
        block_count = len(matches)
        results.append(AnnotationResult(
            key='code_block_count',
            value=block_count,
            value_type=ValueType.NUMERIC,
        ))
        
        # Language annotations (multi-value)
        languages = set()
        for match in matches:
            lang = match.group(1).lower()
            if lang:  # Skip empty language specs
                languages.add(lang)
        
        for lang in languages:
            results.append(AnnotationResult(
                key='code_language',
                value=lang,
                value_type=ValueType.STRING,
                reason='code_block_language_spec',
            ))
        
        return results


class ScriptHeaderAnnotator(ContentPartAnnotator):
    """
    Detect script headers and system includes (strong code evidence).
    
    Shebangs and #include are unambiguous code markers.
    
    Produces:
    - has_script_header FLAG
    - script_type STRING
    """
    
    ANNOTATION_KEY = 'has_script_header'
    VALUE_TYPE = ValueType.FLAG
    PRIORITY = 85
    PART_TYPE_FILTER = 'text'
    
    SHEBANG_PATTERN = re.compile(r'^#!\s*/(?:usr/)?bin/(?:env\s+)?(\w+)', re.MULTILINE)
    INCLUDE_PATTERN = re.compile(r'^#include\s*[<"]', re.MULTILINE)
    PHP_PATTERN = re.compile(r'<\?php', re.IGNORECASE)
    
    def annotate(self, data: ContentPartData) -> list[AnnotationResult]:
        if not data.text_content:
            return []
        
        results = []
        
        # Check for shebang
        shebang_match = self.SHEBANG_PATTERN.search(data.text_content)
        if shebang_match:
            results.append(AnnotationResult(
                key='has_script_header',
                value_type=ValueType.FLAG,
                reason='shebang',
                confidence=1.0,
            ))
            results.append(AnnotationResult(
                key='script_type',
                value=shebang_match.group(1),
                value_type=ValueType.STRING,
                reason='shebang_interpreter',
            ))
            return results
        
        # Check for C/C++ includes
        if self.INCLUDE_PATTERN.search(data.text_content):
            results.append(AnnotationResult(
                key='has_script_header',
                value_type=ValueType.FLAG,
                reason='c_include',
                confidence=1.0,
            ))
            results.append(AnnotationResult(
                key='script_type',
                value='c',
                value_type=ValueType.STRING,
            ))
            return results
        
        # Check for PHP
        if self.PHP_PATTERN.search(data.text_content):
            results.append(AnnotationResult(
                key='has_script_header',
                value_type=ValueType.FLAG,
                reason='php_tag',
                confidence=1.0,
            ))
            results.append(AnnotationResult(
                key='script_type',
                value='php',
                value_type=ValueType.STRING,
            ))
            return results
        
        return results


# ============================================================
# Content Type Annotators
# ============================================================

class LatexContentAnnotator(ContentPartAnnotator):
    """
    Detect LaTeX/MathJax mathematical notation in content parts.
    
    Produces:
    - has_latex FLAG
    - latex_type STRING ('display', 'inline', 'commands')
    """
    
    ANNOTATION_KEY = 'has_latex'
    VALUE_TYPE = ValueType.FLAG
    PRIORITY = 70
    PART_TYPE_FILTER = 'text'
    ROLE_FILTER = 'assistant'
    
    # Display math: $$ ... $$ or \[ ... \]
    DISPLAY_MATH_PATTERN = re.compile(r'\$\$.+?\$\$|\\\[.+?\\\]', re.DOTALL)
    
    # Inline math: $ ... $ (but not $$)
    INLINE_MATH_PATTERN = re.compile(r'(?<!\$)\$(?!\$).+?(?<!\$)\$(?!\$)')
    
    # LaTeX commands: \frac, \sum, \int, etc.
    LATEX_COMMANDS_PATTERN = re.compile(
        r'\\(?:frac|sum|int|prod|lim|sqrt|begin|end|alpha|beta|gamma|'
        r'delta|epsilon|theta|lambda|sigma|omega|pi|infty|partial|nabla|'
        r'mathbb|mathcal|mathbf|mathrm|text|left|right|cdot|times|div)'
    )
    
    def annotate(self, data: ContentPartData) -> list[AnnotationResult]:
        if not data.text_content:
            return []
        
        results = []
        latex_types = set()
        
        if self.DISPLAY_MATH_PATTERN.search(data.text_content):
            latex_types.add('display')
        
        if self.INLINE_MATH_PATTERN.search(data.text_content):
            latex_types.add('inline')
        
        if self.LATEX_COMMANDS_PATTERN.search(data.text_content):
            latex_types.add('commands')
        
        if not latex_types:
            return []
        
        # Main flag
        results.append(AnnotationResult(
            key='has_latex',
            value_type=ValueType.FLAG,
            confidence=0.95 if 'display' in latex_types else 0.8,
            reason='latex_notation_detected',
        ))
        
        # Type annotations
        for latex_type in latex_types:
            results.append(AnnotationResult(
                key='latex_type',
                value=latex_type,
                value_type=ValueType.STRING,
            ))
        
        return results


class WikiLinkContentAnnotator(ContentPartAnnotator):
    """
    Detect Obsidian-style [[wiki links]] in content parts.
    
    This is a content-part level version for granular detection.
    The prompt-response level WikiCandidateAnnotator aggregates this.
    
    Produces:
    - has_wiki_links FLAG
    - wiki_link_count NUMERIC
    """
    
    ANNOTATION_KEY = 'has_wiki_links'
    VALUE_TYPE = ValueType.FLAG
    PRIORITY = 75
    PART_TYPE_FILTER = 'text'
    ROLE_FILTER = 'assistant'
    
    WIKI_LINK_PATTERN = re.compile(r'\[\[([^\]]+)\]\]')
    
    def annotate(self, data: ContentPartData) -> list[AnnotationResult]:
        if not data.text_content:
            return []
        
        matches = self.WIKI_LINK_PATTERN.findall(data.text_content)
        
        if not matches:
            return []
        
        return [
            AnnotationResult(
                key='has_wiki_links',
                value_type=ValueType.FLAG,
                confidence=1.0,
                reason='wiki_links_detected',
            ),
            AnnotationResult(
                key='wiki_link_count',
                value=len(matches),
                value_type=ValueType.NUMERIC,
            ),
        ]


# ============================================================
# Registry for running all annotators
# ============================================================

CONTENT_PART_ANNOTATORS = [
    CodeBlockAnnotator,
    ScriptHeaderAnnotator,
    LatexContentAnnotator,
    WikiLinkContentAnnotator,
]


def run_content_part_annotators(session: Session) -> dict[str, int]:
    """
    Run all content-part annotators in priority order.
    
    Returns dict mapping annotator name to annotation count.
    """
    # Sort by priority (descending)
    sorted_annotators = sorted(
        CONTENT_PART_ANNOTATORS,
        key=lambda cls: cls.PRIORITY,
        reverse=True,
    )
    
    results = {}
    for annotator_cls in sorted_annotators:
        annotator = annotator_cls(session)
        count = annotator.compute()
        results[annotator_cls.__name__] = count
    
    session.commit()
    return results



---
File: llm_archive/annotators/prompt_response.py
---
# llm_archive/annotators/prompt_response.py
"""Prompt-response level annotators.

These annotators work on the prompt_responses table, which is built
without tree dependency. They support:
- Wiki article candidate detection
- Naive title extraction
- Prerequisite/skip annotation filtering

Uses the new typed annotation tables (derived.prompt_response_annotations_*).
"""

from abc import abstractmethod
from dataclasses import dataclass
from datetime import datetime
from typing import Iterator
from uuid import UUID

from sqlalchemy import text
from sqlalchemy.orm import Session

from llm_archive.annotations.core import (
    AnnotationWriter, AnnotationReader, AnnotationResult,
    EntityType, ValueType,
)


# ============================================================
# Data Classes
# ============================================================

@dataclass
class PromptResponseData:
    """Data passed to prompt-response annotation logic."""
    prompt_response_id: UUID
    dialogue_id: UUID
    prompt_message_id: UUID
    response_message_id: UUID
    prompt_text: str | None
    response_text: str | None
    prompt_word_count: int | None
    response_word_count: int | None
    prompt_role: str
    response_role: str
    created_at: datetime | None


# ============================================================
# Base PromptResponse Annotator
# ============================================================

class PromptResponseAnnotator:
    """
    Base class for annotating prompt-response pairs.
    
    Iterates over derived.prompt_response_content_v (view that joins
    content from raw.content_parts - no denormalized storage).
    
    Supports annotation prerequisites and skip conditions using the
    new typed annotation tables:
    - REQUIRES_FLAGS: Only process entities with ALL of these flag annotations
    - REQUIRES_STRINGS: Only process entities with ALL of these (key, value) string annotations
    - SKIP_IF_FLAGS: Skip entities with ANY of these flag annotations
    - SKIP_IF_STRINGS: Skip entities with ANY of these (key,) or (key, value) string annotations
    
    Example:
        REQUIRES_STRINGS = [('exchange_type', 'wiki_article')]
        SKIP_IF_FLAGS = ['has_preamble']
    """
    
    ENTITY_TYPE = EntityType.PROMPT_RESPONSE
    
    # Annotator metadata
    ANNOTATION_KEY: str = None  # Required: the key this annotator produces
    VALUE_TYPE: ValueType = ValueType.STRING  # What type of annotation this produces
    PRIORITY: int = 50  # Higher runs first
    VERSION: str = '1.0'
    SOURCE: str = 'heuristic'
    
    # Filtering - override in subclass
    REQUIRES_FLAGS: list[str] = []
    REQUIRES_STRINGS: list[tuple[str, str]] = []  # (key, value) pairs
    SKIP_IF_FLAGS: list[str] = []
    SKIP_IF_STRINGS: list[tuple[str, ...]] = []  # (key,) or (key, value)
    
    def __init__(self, session: Session):
        self.session = session
        self.writer = AnnotationWriter(session)
        self.reader = AnnotationReader(session)
    
    def compute(self) -> int:
        """Run annotation over prompt-response pairs."""
        count = 0
        
        for data in self._iter_prompt_responses():
            results = self.annotate(data)
            for result in results:
                if self._write_result(data.prompt_response_id, result):
                    count += 1
        
        return count
    
    def _write_result(self, entity_id: UUID, result: AnnotationResult) -> bool:
        """Write an annotation result to the appropriate table."""
        # Use the result's value_type if specified, otherwise use class default
        value_type = result.value_type if result.value_type else self.VALUE_TYPE
        
        if value_type == ValueType.FLAG:
            return self.writer.write_flag(
                entity_type=self.ENTITY_TYPE,
                entity_id=entity_id,
                key=result.key,
                confidence=result.confidence,
                reason=result.reason,
                source=result.source or self.SOURCE,
                source_version=result.source_version or self.VERSION,
            )
        elif value_type == ValueType.STRING:
            return self.writer.write_string(
                entity_type=self.ENTITY_TYPE,
                entity_id=entity_id,
                key=result.key,
                value=str(result.value),
                confidence=result.confidence,
                reason=result.reason,
                source=result.source or self.SOURCE,
                source_version=result.source_version or self.VERSION,
            )
        elif value_type == ValueType.NUMERIC:
            return self.writer.write_numeric(
                entity_type=self.ENTITY_TYPE,
                entity_id=entity_id,
                key=result.key,
                value=float(result.value),
                confidence=result.confidence,
                reason=result.reason,
                source=result.source or self.SOURCE,
                source_version=result.source_version or self.VERSION,
            )
        elif value_type == ValueType.JSON:
            return self.writer.write_json(
                entity_type=self.ENTITY_TYPE,
                entity_id=entity_id,
                key=result.key,
                value=result.value,
                confidence=result.confidence,
                reason=result.reason,
                source=result.source or self.SOURCE,
                source_version=result.source_version or self.VERSION,
            )
        return False
    
    def _iter_prompt_responses(self) -> Iterator[PromptResponseData]:
        """Iterate over prompt-responses with content, respecting annotation filters."""
        # Base query uses the content view
        query_parts = ["""
            SELECT 
                prc.prompt_response_id,
                prc.dialogue_id,
                prc.prompt_message_id,
                prc.response_message_id,
                prc.prompt_text,
                prc.response_text,
                prc.prompt_word_count,
                prc.response_word_count,
                prc.prompt_role,
                prc.response_role,
                prc.created_at
            FROM derived.prompt_response_content_v prc
        """]
        
        params = {}
        join_idx = 0
        
        # Add REQUIRES_FLAGS joins (must have these flags)
        for flag_key in self.REQUIRES_FLAGS:
            alias = f"req_flag_{join_idx}"
            query_parts.append(f"""
                JOIN derived.prompt_response_annotations_flag {alias} ON 
                    {alias}.entity_id = prc.prompt_response_id
                    AND {alias}.annotation_key = :req_flag_key_{join_idx}
            """)
            params[f'req_flag_key_{join_idx}'] = flag_key
            join_idx += 1
        
        # Add REQUIRES_STRINGS joins (must have these key+value pairs)
        for key, value in self.REQUIRES_STRINGS:
            alias = f"req_str_{join_idx}"
            query_parts.append(f"""
                JOIN derived.prompt_response_annotations_string {alias} ON 
                    {alias}.entity_id = prc.prompt_response_id
                    AND {alias}.annotation_key = :req_str_key_{join_idx}
                    AND {alias}.annotation_value = :req_str_val_{join_idx}
            """)
            params[f'req_str_key_{join_idx}'] = key
            params[f'req_str_val_{join_idx}'] = value
            join_idx += 1
        
        # Add SKIP_IF_FLAGS exclusions
        skip_where_clauses = []
        for flag_key in self.SKIP_IF_FLAGS:
            alias = f"skip_flag_{join_idx}"
            query_parts.append(f"""
                LEFT JOIN derived.prompt_response_annotations_flag {alias} ON 
                    {alias}.entity_id = prc.prompt_response_id
                    AND {alias}.annotation_key = :skip_flag_key_{join_idx}
            """)
            params[f'skip_flag_key_{join_idx}'] = flag_key
            skip_where_clauses.append(f"{alias}.id IS NULL")
            join_idx += 1
        
        # Add SKIP_IF_STRINGS exclusions
        for skip_spec in self.SKIP_IF_STRINGS:
            alias = f"skip_str_{join_idx}"
            if len(skip_spec) == 1:
                # Skip if key exists (any value)
                query_parts.append(f"""
                    LEFT JOIN derived.prompt_response_annotations_string {alias} ON 
                        {alias}.entity_id = prc.prompt_response_id
                        AND {alias}.annotation_key = :skip_str_key_{join_idx}
                """)
                params[f'skip_str_key_{join_idx}'] = skip_spec[0]
            else:
                # Skip if key+value match
                query_parts.append(f"""
                    LEFT JOIN derived.prompt_response_annotations_string {alias} ON 
                        {alias}.entity_id = prc.prompt_response_id
                        AND {alias}.annotation_key = :skip_str_key_{join_idx}
                        AND {alias}.annotation_value = :skip_str_val_{join_idx}
                """)
                params[f'skip_str_key_{join_idx}'] = skip_spec[0]
                params[f'skip_str_val_{join_idx}'] = skip_spec[1]
            skip_where_clauses.append(f"{alias}.id IS NULL")
            join_idx += 1
        
        # WHERE clause for exclusions
        if skip_where_clauses:
            query_parts.append("WHERE " + " AND ".join(skip_where_clauses))
        
        query_parts.append("ORDER BY prc.created_at")
        
        query = text("\n".join(query_parts))
        
        for row in self.session.execute(query, params):
            yield PromptResponseData(
                prompt_response_id=row.prompt_response_id,
                dialogue_id=row.dialogue_id,
                prompt_message_id=row.prompt_message_id,
                response_message_id=row.response_message_id,
                prompt_text=row.prompt_text,
                response_text=row.response_text,
                prompt_word_count=row.prompt_word_count,
                response_word_count=row.response_word_count,
                prompt_role=row.prompt_role,
                response_role=row.response_role,
                created_at=row.created_at,
            )
    
    @abstractmethod
    def annotate(self, data: PromptResponseData) -> list[AnnotationResult]:
        """
        Analyze prompt-response pair and return annotations to create.
        
        Args:
            data: PromptResponseData with texts and metadata
            
        Returns:
            List of AnnotationResult objects (empty list if no match)
        """
        pass


# ============================================================
# Wiki Article Detection
# ============================================================

class WikiCandidateAnnotator(PromptResponseAnnotator):
    """
    Detect wiki-style article candidates.
    
    Looks for [[wiki links]] in assistant responses, which indicate
    the response was likely formatted as a wiki article.
    
    Produces a STRING annotation: exchange_type = 'wiki_article'
    """
    
    ANNOTATION_KEY = 'exchange_type'
    VALUE_TYPE = ValueType.STRING
    PRIORITY = 60
    VERSION = '1.0'
    
    def annotate(self, data: PromptResponseData) -> list[AnnotationResult]:
        if data.response_role != 'assistant':
            return []
        
        response = data.response_text or ''
        
        # Count wiki links
        wiki_link_count = response.count('[[')
        
        if wiki_link_count >= 1:
            # High confidence if multiple links
            confidence = 0.95 if wiki_link_count >= 3 else 0.8
            
            results = [
                # String annotation for exchange type
                AnnotationResult(
                    key='exchange_type',
                    value='wiki_article',
                    value_type=ValueType.STRING,
                    confidence=confidence,
                    reason='wiki_links_detected',
                ),
                # Numeric annotation for link count
                AnnotationResult(
                    key='wiki_link_count',
                    value=wiki_link_count,
                    value_type=ValueType.NUMERIC,
                    confidence=1.0,
                    reason='counted',
                ),
            ]
            return results
        
        return []


# ============================================================
# Title Extraction
# ============================================================

class NaiveTitleAnnotator(PromptResponseAnnotator):
    """
    Extract title from first line of response.
    
    Looks for:
    - Markdown headers: # Title
    - Bold headers: **Title**
    
    Should run AFTER wiki candidate detection.
    Only runs on wiki_article candidates.
    
    Produces a STRING annotation: proposed_title = '<extracted title>'
    """
    
    ANNOTATION_KEY = 'proposed_title'
    VALUE_TYPE = ValueType.STRING
    PRIORITY = 50
    VERSION = '1.0'
    
    # Only process wiki article candidates
    REQUIRES_STRINGS = [('exchange_type', 'wiki_article')]
    
    def annotate(self, data: PromptResponseData) -> list[AnnotationResult]:
        if data.response_role != 'assistant':
            return []
        
        response = data.response_text or ''
        title, reason = self._extract_title(response)
        
        if title:
            return [AnnotationResult(
                key='proposed_title',
                value=title,
                value_type=ValueType.STRING,
                confidence=0.8,
                reason=reason,
            )]
        
        return []
    
    def _extract_title(self, text: str) -> tuple[str | None, str | None]:
        """Extract title from first line of text. Returns (title, reason)."""
        lines = text.strip().split('\n')
        if not lines:
            return None, None
        
        first_line = lines[0].strip()
        
        # Markdown header: # Title or ## Title
        if first_line.startswith('#'):
            title = first_line.lstrip('#').strip()
            if title:
                return title, 'markdown_header'
        
        # Bold header: **Title**
        if first_line.startswith('**') and first_line.endswith('**'):
            title = first_line.strip('*').strip()
            if title:
                return title, 'bold_header'
        
        # Bold header with trailing content: **Title** - some subtitle
        if first_line.startswith('**') and '**' in first_line[2:]:
            end_idx = first_line.index('**', 2)
            title = first_line[2:end_idx].strip()
            if title:
                return title, 'bold_header_with_suffix'
        
        return None, None


# ============================================================
# Code Detection
# ============================================================

import re


class HasCodeAnnotator(PromptResponseAnnotator):
    """
    Detect if prompt-response pair involves code.
    
    Aggregates evidence from multiple sources:
    - Code blocks (```)
    - Script headers (shebang, #include)
    - Function definitions
    - Import statements
    
    Produces:
    - has_code FLAG
    - code_evidence STRING (multi-value)
    """
    
    ANNOTATION_KEY = 'has_code'
    VALUE_TYPE = ValueType.FLAG
    PRIORITY = 55
    VERSION = '1.0'
    
    SKIP_IF_FLAGS = ['has_code']  # Skip if already annotated
    
    def annotate(self, data: PromptResponseData) -> list[AnnotationResult]:
        if data.response_role != 'assistant':
            return []
        
        if not data.response_text:
            return []
        
        results = []
        evidence_types = set()
        
        # Check for code blocks
        if '```' in data.response_text:
            evidence_types.add('code_block')
        
        # Check for script headers
        if re.search(r'^#!\s*/(?:usr/)?bin/', data.response_text, re.MULTILINE):
            evidence_types.add('shebang')
        if re.search(r'^#include\s*[<"]', data.response_text, re.MULTILINE):
            evidence_types.add('c_include')
        
        # Check for function definitions
        if re.search(r'\bdef\s+\w+\s*\(', data.response_text):
            evidence_types.add('python_function')
        if re.search(r'function\s+\w+\s*\(', data.response_text):
            evidence_types.add('js_function')
        if re.search(r'const\s+\w+\s*=\s*\([^)]*\)\s*=>', data.response_text):
            evidence_types.add('arrow_function')
        
        # Check for import statements
        if re.search(r'^(?:import|from)\s+\w+', data.response_text, re.MULTILINE):
            evidence_types.add('python_import')
        if re.search(r'^(?:const|let|var)\s+.*=\s*require\s*\(', data.response_text, re.MULTILINE):
            evidence_types.add('js_require')
        
        if not evidence_types:
            return []
        
        # Main flag with confidence based on evidence strength
        strong_evidence = {'code_block', 'shebang', 'c_include'}
        is_strong = bool(evidence_types & strong_evidence)
        
        results.append(AnnotationResult(
            key='has_code',
            value_type=ValueType.FLAG,
            confidence=0.95 if is_strong else 0.75,
            reason=','.join(sorted(evidence_types)),
        ))
        
        # Evidence type annotations (multi-value)
        for evidence in evidence_types:
            results.append(AnnotationResult(
                key='code_evidence',
                value=evidence,
                value_type=ValueType.STRING,
            ))
        
        return results


# ============================================================
# LaTeX Detection
# ============================================================

class HasLatexAnnotator(PromptResponseAnnotator):
    """
    Detect if prompt-response pair contains LaTeX/math notation.
    
    Produces:
    - has_latex FLAG
    - latex_type STRING (multi-value: 'display', 'inline', 'commands')
    """
    
    ANNOTATION_KEY = 'has_latex'
    VALUE_TYPE = ValueType.FLAG
    PRIORITY = 54
    VERSION = '1.0'
    
    SKIP_IF_FLAGS = ['has_latex']
    
    # Patterns
    DISPLAY_MATH = re.compile(r'\$\$.+?\$\$|\\\[.+?\\\]', re.DOTALL)
    INLINE_MATH = re.compile(r'(?<!\$)\$(?!\$).+?(?<!\$)\$(?!\$)')
    LATEX_COMMANDS = re.compile(
        r'\\(?:frac|sum|int|prod|lim|sqrt|begin|end|alpha|beta|gamma|'
        r'delta|epsilon|theta|lambda|sigma|omega|pi|infty|partial|nabla|'
        r'mathbb|mathcal|mathbf|mathrm|text|left|right|cdot|times|div)'
    )
    
    def annotate(self, data: PromptResponseData) -> list[AnnotationResult]:
        if data.response_role != 'assistant':
            return []
        
        if not data.response_text:
            return []
        
        results = []
        latex_types = set()
        
        if self.DISPLAY_MATH.search(data.response_text):
            latex_types.add('display')
        
        if self.INLINE_MATH.search(data.response_text):
            latex_types.add('inline')
        
        if self.LATEX_COMMANDS.search(data.response_text):
            latex_types.add('commands')
        
        if not latex_types:
            return []
        
        # Main flag
        results.append(AnnotationResult(
            key='has_latex',
            value_type=ValueType.FLAG,
            confidence=0.95 if 'display' in latex_types else 0.8,
            reason='latex_detected',
        ))
        
        # Type annotations
        for latex_type in latex_types:
            results.append(AnnotationResult(
                key='latex_type',
                value=latex_type,
                value_type=ValueType.STRING,
            ))
        
        return results


# ============================================================
# Annotator Registry
# ============================================================

PROMPT_RESPONSE_ANNOTATORS = [
    WikiCandidateAnnotator,
    NaiveTitleAnnotator,
    HasCodeAnnotator,
    HasLatexAnnotator,
]


def run_prompt_response_annotators(session: Session) -> dict[str, int]:
    """
    Run all prompt-response annotators in priority order.
    
    Returns dict mapping annotator name to annotation count.
    """
    # Sort by priority (descending)
    sorted_annotators = sorted(
        PROMPT_RESPONSE_ANNOTATORS,
        key=lambda cls: cls.PRIORITY,
        reverse=True,
    )
    
    results = {}
    for annotator_cls in sorted_annotators:
        annotator = annotator_cls(session)
        count = annotator.compute()
        results[annotator_cls.__name__] = count
    
    session.commit()
    return results


