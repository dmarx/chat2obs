---
File: .github/workflows/ci.yml
---
# .github/workflows/ci.yml
# Continuous Integration workflow for llm_archive

name: CI

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master, sql]
  workflow_dispatch: 

env:
  TEST_DATABASE_URL: "postgresql://postgres:postgres@localhost:5432/llm_archive_test"

jobs:
  lint:
    name: Lint
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: "Set up Python"
        uses: actions/setup-python@v6
        with:
          python-version-file: "pyproject.toml"

      - name: Install uv
        uses: astral-sh/setup-uv@v7

      - name: Install dependencies
        # run: uv sync --locked --all-extras --dev
        run: uv sync --all-extras --dev

      - name: Check formatting with ruff
        run: uv run ruff format --check .
        continue-on-error: true

      - name: Lint with ruff
        run: uv run ruff check .
        continue-on-error: true

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
      
      - name: "Set up Python"
        uses: actions/setup-python@v6
        with:
          python-version-file: "pyproject.toml"
      
      - name: Install uv
        uses: astral-sh/setup-uv@v7

      - name: Install dependencies
        # run: uv sync --locked --all-extras --dev
        run: uv sync --all-extras --dev

      - name: Run unit tests
        run: uv run pytest tests/unit -v --cov=llm_archive --cov-report=xml

      - name: Upload coverage reports
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          fail_ci_if_error: false
        continue-on-error: true


  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
      
      - name: "Set up Python"
        uses: actions/setup-python@v6
        with:
          python-version-file: "pyproject.toml"
      
      - name: Install uv
        uses: astral-sh/setup-uv@v7

      - name: Install dependencies
        # run: uv sync --locked --all-extras --dev
        run: uv sync --all-extras --dev

      - name: Create .env file
        run: |
          cat > .env << EOF
          POSTGRES_HOST=localhost
          POSTGRES_PORT=5432
          POSTGRES_DB=llm_archive_test
          POSTGRES_USER=postgres
          POSTGRES_PASSWORD=postgres
          EOF

      - name: Start database with docker-compose
        run: |
          docker compose up -d
          # Wait for database to be ready
          for i in {1..30}; do
            if docker compose exec -T db pg_isready -U postgres; then
              echo "Database is ready"
              break
            fi
            echo "Waiting for database... ($i)"
            sleep 2
          done

      - name: Create test database
        run: |
          docker compose exec -T db psql -U postgres -c "CREATE DATABASE llm_archive_test" || true

      - name: Run integration tests
        run: uv run pytest tests/integration -v --cov=llm_archive --cov-report=xml
        env:
          TEST_DATABASE_URL: postgresql://postgres:postgres@localhost:5432/llm_archive_test

      - name: Upload coverage reports
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          fail_ci_if_error: false
        continue-on-error: true


  build:
    name: Build Package
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]

    steps:
      - uses: actions/checkout@v4
      
      - name: "Set up Python"
        uses: actions/setup-python@v6
        with:
          python-version-file: "pyproject.toml"
          
      - name: Install uv
        uses: astral-sh/setup-uv@v7

      - name: Build package
        run: uv build

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist
          path: dist/



---
File: .github/workflows/summary-for-llm.yml
---
name: Llamero Summarization

on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  generate-summaries:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - name: Install llamero
      run: touch requirements.txt

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        #cache: 'pip'

    - name: Install llamero
      run: pip install llamero

    - name: Generate summaries
      run: |
        llamero summarize all
        #llamero tree --output summaries/tree.md



---
File: README.md
---
# LLM Archive

Ingest, normalize, and analyze LLM conversation exports for downstream processing.

## Features

- **Multi-source ingestion**: ChatGPT and Claude exports
- **Prompt-response pairing**: Direct userâ†’assistant associations
- **Typed annotation system**: Flag, string, numeric, and JSON annotations
- **Content hashing**: Deduplication preparation
- **Incremental processing**: Efficient re-annotation with cursor tracking

## Project Structure

```
llm_archive/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ raw.py          # Raw schema models
â”‚   â””â”€â”€ derived.py      # Derived schema models
â”œâ”€â”€ extractors/
â”‚   â”œâ”€â”€ base.py         # Base extractor class
â”‚   â”œâ”€â”€ chatgpt.py      # ChatGPT extractor
â”‚   â””â”€â”€ claude.py       # Claude extractor
â”œâ”€â”€ builders/
â”‚   â””â”€â”€ prompt_response.py  # Prompt-response builder
â”œâ”€â”€ annotations/
â”‚   â””â”€â”€ core.py         # AnnotationWriter/Reader
â”œâ”€â”€ annotators/
â”‚   â””â”€â”€ prompt_response.py  # Prompt-response annotators
â”œâ”€â”€ cli.py              # Command-line interface
â””â”€â”€ config.py           # Environment configuration
```

## Additional Documentation

See the `docs/` folder for detailed documentation:

- [Architecture](docs/architecture.md) - System design and data flow
- [Schema](docs/schema.md) - Database schema details
- [Models](docs/models.md) - SQLAlchemy ORM models
- [Extractors](docs/extractors.md) - Platform-specific extraction
- [Builders](docs/builders.md) - Derived data construction
- [Annotators](docs/annotators.md) - Annotation system
- [CLI Reference](docs/cli.md) - Command-line interface
- [Testing](docs/testing.md) - Testing strategy


## Requirements

- Python 3.11+
- Docker (for PostgreSQL with pgvector)

## Installation

```bash
# Install uv if not already installed
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create virtual environment and install dependencies
uv venv
source .venv/bin/activate  # or `.venv\Scripts\activate` on Windows
uv pip install -e .

# Or install directly without explicit venv creation
uv pip install -e . --system

# Install with dev dependencies
uv pip install -e ".[dev]"
```

## Database Setup

### Configure Environment

```bash
# Copy example env file
cp .env.example .env

# Edit if needed (defaults work out of the box)
# vim .env
```

### Start PostgreSQL (pgvector)

```bash
# Start the database (data persists in ./data/postgres/)
docker compose up -d

# Check it's running
docker compose ps

# View logs if needed
docker compose logs -f db
```

The database configuration is read from `.env`:
- **Host**: `POSTGRES_HOST` (default: `localhost`)
- **Port**: `POSTGRES_PORT` (default: `5432`)
- **Database**: `POSTGRES_DB` (default: `llm_archive`)
- **User**: `POSTGRES_USER` (default: `postgres`)
- **Password**: `POSTGRES_PASSWORD` (default: `postgres`)

### Initialize Schema

```bash
uv run llm-archive init --schema_dir=schema
```

### Stop Database

```bash
# Stop but keep data
docker compose down

# Stop and DELETE all data
docker compose down -v
rm -rf data/postgres
```

## Usage

### Import

```bash
# Import ChatGPT export
uv run llm-archive import_chatgpt /path/to/conversations.json

# Import Claude export
uv run llm-archive import_claude /path/to/claude.json

# Import both
uv run llm-archive import_all \
    --chatgpt_path=/path/to/chatgpt.json \
    --claude_path=/path/to/claude.json
```

### Build Derived Structures

```bash
# Build prompt-response pairs
uv run llm-archive build_prompt_responses

# Build for specific dialogue
uv run llm-archive build_prompt_responses --dialogue_id=<uuid>
```

### Annotations

```bash
# Run all registered annotators
uv run llm-archive annotate

# Run specific annotator
uv run llm-archive annotate WikiCandidateAnnotator

# Clear and re-run
uv run llm-archive annotate --clear
```

### Analysis

```bash
# Show statistics
uv run llm-archive stats

# Query annotated content
uv run llm-archive query_annotations \
    --entity_type=prompt_response \
    --annotation_key=exchange_type \
    --annotation_value=wiki_article
```

### Full Pipeline

```bash
# Run everything
uv run llm-archive pipeline \
    --chatgpt_path=/path/to/chatgpt.json \
    --claude_path=/path/to/claude.json \
    --init_db
```

## Idempotent Import

Re-importing the same export files is safe:
- Dialogues are identified by their source ID
- If `updated_at` timestamp is newer, dialogue is updated
- If unchanged, dialogue is skipped
- Messages use content hashing for change detection

## Schema

### Raw Layer (`raw.*`)

Source of truth from imports:

- `raw.sources` - Registry of dialogue sources
- `raw.dialogues` - Conversations/chats
- `raw.messages` - Individual messages (with tree structure)
- `raw.content_parts` - Message content segments
- `raw.citations` - Source citations
- `raw.attachments` - File attachments
- `raw.chatgpt_*` - ChatGPT-specific extensions
- `raw.claude_*` - Claude-specific extensions

### Derived Layer (`derived.*`)

Computed structures that can be rebuilt:

- `derived.prompt_responses` - User prompt â†’ assistant response pairs
- `derived.prompt_response_content` - Denormalized text content
- `derived.{entity}_annotations_{type}` - Typed annotation tables
  - Entity types: `content_part`, `message`, `prompt_response`, `dialogue`
  - Value types: `flag`, `string`, `numeric`, `json`
- `derived.annotator_cursors` - Incremental processing state

## Annotation System

The annotation system supports multiple detection strategies for the same concept:

```python
from llm_archive.annotators.prompt_response import PromptResponseAnnotator
from llm_archive.annotations.core import AnnotationResult, ValueType

class MyAnnotator(PromptResponseAnnotator):
    ANNOTATION_KEY = 'my_classification'
    VALUE_TYPE = ValueType.STRING
    PRIORITY = 50  # Higher runs first
    VERSION = '1.0'
    SOURCE = 'heuristic'
    
    def annotate(self, data: PromptResponseData) -> list[AnnotationResult]:
        if self._matches_criteria(data):
            return [AnnotationResult(
                key=self.ANNOTATION_KEY,
                value='my_tag_value',
                confidence=0.9,
            )]
        return []
```

Register and run:

```python
from llm_archive.cli import AnnotationManager

manager = AnnotationManager(session)
manager.register(MyAnnotator)
results = manager.run_all()
```

## Pipeline

```
Import â†’ Prompt-Response Building â†’ Annotation â†’ Analysis
                                         â†“
                                  [Future: Export,
                                   Knowledge graph]
```

## Testing

### Setup Test Database

```bash
# Create test database (separate from main)
docker exec -it llm_archive_db psql -U postgres -c "CREATE DATABASE llm_archive_test"

# Or set custom test database URL
export TEST_DATABASE_URL="postgresql://postgres:postgres@localhost:5432/llm_archive_test"
```

### Run Tests

```bash
# Install dev dependencies
uv pip install -e ".[dev]"

# Run all tests
pytest

# Run only unit tests (no database required)
pytest tests/unit/

# Run integration tests
pytest tests/integration/

# Run with coverage
pytest --cov=llm_archive --cov-report=html

# View coverage report
open htmlcov/index.html
```

### Test Organization

```
tests/
â”œâ”€â”€ unit/               # Fast tests, no database
â”‚   â”œâ”€â”€ test_annotations.py
â”‚   â””â”€â”€ test_utils.py
â”œâ”€â”€ integration/        # Tests requiring database
â”‚   â”œâ”€â”€ conftest.py     # Shared fixtures
â”‚   â”œâ”€â”€ test_extractors.py
â”‚   â”œâ”€â”€ test_prompt_response_builder.py
â”‚   â””â”€â”€ test_annotations.py
â””â”€â”€ conftest.py         # Root configuration
```



---
File: attic/requirements.txt
---
loguru
networkx
numpy
pandas
pytest
python-frontmatter
jinja2



---
File: attic/setup.py
---
# setup.py
"""Minimal setup for the conversation tagger."""

from setuptools import setup, find_packages

setup(
    name="conversation_tagger",
    version="0.1.0",
    description="Exchange-based conversation analysis system",
    package_dir={"": "src"},
    packages=find_packages(where="src"),
    python_requires=">=3.8",
    install_requires=[],
    extras_require={
        "dev": ["pytest>=6.0"]
    }
)



---
File: attic/src/conversation_tagger/__init__.py
---
# conversation_tagger/__init__.py
"""
Minimal conversation tagging system with exchange-based analysis.
"""

from .core.tag import Tag
from .core.tagger import ConversationTagger
from .factory import create_default_tagger
from .core.conversation import Conversation
from .core.exchange import Exchange
from .core.message import Message
from .core.exchange_parser import ExchangeParser, ExchangeParserOAI, ExchangeParserClaude
from .core.exchange_tagger import ExchangeTagger
from .core.detection import EXCHANGE_RULES, CONVERSATION_RULES
from .core.generate import generate_notes   

__all__ = ['Tag', 'ConversationTagger', 'create_default_tagger', 'Conversation', 'Exchange', 'Message', 'ExchangeParser', 'ExchangeParserOAI', 'ExchangeParserClaude', 'ExchangeTagger', 'EXCHANGE_RULES', 'CONVERSATION_RULES' ]


---
File: attic/src/conversation_tagger/analysis/__init__.py
---



---
File: attic/src/conversation_tagger/analysis/faceting.py
---
# conversation_tagger/analysis/faceting.py
"""
Faceting functionality for analyzing conversations by different dimensions.
Updated to use dictionary-based annotations.
"""

from typing import Dict, Any, List, Optional
from collections import defaultdict

from ..core.tag import Tag


def get_facet_value(annotations: Dict[str, Any], facet_annotation_name: str, 
                   facet_attribute: Optional[str] = None) -> str:
    """Extract facet value from a conversation's annotations."""
    if facet_annotation_name not in annotations:
        return "<none>"
    
    annotation_value = annotations[facet_annotation_name]
    
    if facet_attribute is None:
        # Just check for presence of the annotation
        if annotation_value is True:
            return f"has_{facet_annotation_name}"
        else:
            return str(annotation_value)
    
    # Extract specific attribute values from structured annotation
    if isinstance(annotation_value, dict) and facet_attribute in annotation_value:
        return str(annotation_value[facet_attribute])
    
    return f"<{facet_annotation_name}_no_{facet_attribute}>"


def do_facet_conversations(tagged_conversations: List[Dict[str, Any]], 
                       facet_annotation_name: str, 
                       facet_attribute: Optional[str] = None,
                       max_facets: int = 50) -> Dict[str, List[Dict[str, Any]]]:
    """Group conversations by facet values."""
    facets = defaultdict(list)
    
    for tagged_conv in tagged_conversations:
        # Handle both old Tag-based format and new annotation format
        if 'annotations' in tagged_conv:
            annotations = tagged_conv['annotations']
        else:
            # Legacy: convert tags to annotations for compatibility
            annotations = {}
            for tag in tagged_conv.get('tags', []):
                if isinstance(tag, Tag):
                    annotations.update(tag.to_dict())
                else:
                    annotations[str(tag)] = True
        
        facet_value = get_facet_value(annotations, facet_annotation_name, facet_attribute)
        facets[facet_value].append(tagged_conv)
    
    # Sort by facet size (largest first) and limit
    sorted_facets = dict(sorted(facets.items(), key=lambda x: len(x[1]), reverse=True))
    
    if len(sorted_facets) > max_facets:
        # Keep top facets and group rest into "others"
        items = list(sorted_facets.items())
        top_facets = dict(items[:max_facets-1])
        
        other_conversations = []
        for _, conversations in items[max_facets-1:]:
            other_conversations.extend(conversations)
        
        if other_conversations:
            top_facets["<other>"] = other_conversations
        
        return top_facets
    
    return sorted_facets


def print_faceted_summary(tagged_conversations: List[Dict[str, Any]], 
                         facet_annotation_name: str, 
                         facet_attribute: Optional[str] = None,
                         show_details: bool = False,
                         max_facets: int = 20):
    """Print annotation summary broken down by facets."""
    total = len(tagged_conversations)
    facets = do_facet_conversations(tagged_conversations, facet_annotation_name, facet_attribute, max_facets)
    
    print(f"Tagged {total} conversations")
    print(f"Faceted by: {facet_annotation_name}" + 
          (f".{facet_attribute}" if facet_attribute else ""))
    print(f"Found {len(facets)} facet values")
    
    print(f"\n{'='*80}")
    print(f"FACETED ANNOTATION SUMMARY")
    print(f"{'='*80}")
    
    for facet_value, facet_conversations in facets.items():
        facet_size = len(facet_conversations)
        facet_percentage = (facet_size / total) * 100
        
        print(f"\nðŸ“Š FACET: {facet_value}")
        print(f"    Conversations: {facet_size} ({facet_percentage:.1f}% of total)")
        print(f"    {'-' * 60}")
        
        # Calculate annotation statistics for this facet
        annotation_counts = defaultdict(int)
        annotation_attributes = defaultdict(lambda: defaultdict(list))
        unique_structured_annotations = defaultdict(set)
        
        for tagged_conv in facet_conversations:
            # Handle both new annotation format and legacy tag format
            if 'annotations' in tagged_conv:
                annotations = tagged_conv['annotations']
            else:
                # Legacy: convert tags to annotations
                annotations = {}
                for tag in tagged_conv.get('tags', []):
                    if isinstance(tag, Tag):
                        annotations.update(tag.to_dict())
                    else:
                        annotations[str(tag)] = True
            
            for annotation_name, annotation_value in annotations.items():
                annotation_counts[annotation_name] += 1
                
                # Collect attribute information
                if isinstance(annotation_value, dict):
                    for attr_name, attr_value in annotation_value.items():
                        if isinstance(attr_value, (int, float)):
                            annotation_attributes[annotation_name][attr_name].append(attr_value)
                        else:
                            unique_structured_annotations[annotation_name].add(f"{attr_name}={attr_value}")
                elif isinstance(annotation_value, (int, float)):
                    annotation_attributes[annotation_name]['value'].append(annotation_value)
        
        # Sort annotations for this facet (show all annotations)
        sorted_annotations = sorted(annotation_counts.items(), key=lambda x: x[1], reverse=True)
        
        for annotation_name, count in sorted_annotations:
            percentage = (count / facet_size) * 100
            print(f"    {annotation_name}: {count} ({percentage:.1f}%)")
            
            if show_details:
                # Show numeric attribute statistics
                if annotation_name in annotation_attributes:
                    for attr_name, values in annotation_attributes[annotation_name].items():
                        if values:
                            avg_val = sum(values) / len(values)
                            min_val = min(values)
                            max_val = max(values)
                            print(f"        {attr_name}: avg={avg_val:.1f}, range=[{min_val}, {max_val}]")
                
                # Show unique structured values
                if annotation_name in unique_structured_annotations:
                    unique_vals = sorted(unique_structured_annotations[annotation_name])
                    if len(unique_vals) <= 5:
                        print(f"        values: {', '.join(unique_vals)}")
                    else:
                        print(f"        values: {', '.join(unique_vals[:5])} ... (+{len(unique_vals)-5} more)")



---
File: attic/src/conversation_tagger/core/conversation.py
---
# conversation_tagger/core/conversation.py
"""
Conversation class updated to use dictionary-based annotations.
"""

from typing import List, Dict, Any, TYPE_CHECKING
from dataclasses import dataclass, field

if TYPE_CHECKING:
    from .exchange import Exchange
    from .exchange_parser import ExchangeParser

from .tag import Tag

@dataclass 
class Conversation:
    """A conversation consisting of sequential exchanges with annotations."""
    
    conversation_id: str
    title: str
    exchanges: List['Exchange'] = field(default_factory=list)
    annotations: Dict[str, Any] = field(default_factory=dict)  # Dictionary-based annotations
    raw: Dict[str, Any] | None = field(default=None)  
    
    def __post_init__(self):
        """Post-initialization to ensure annotations are set."""
        if not self.annotations:
            self._add_exchange_annotations()

    def _add_exchange_annotations(self):
        """Aggregate annotations from all exchanges."""
        if not self.annotations:
            # Collect all unique annotations from exchanges
            for exchange in self.exchanges:
                for name, value in exchange.annotations.items():
                    if name not in self.annotations:
                        self.annotations[name] = value

    def add_annotation(self, name: str, value: Any = True) -> None:
        """Add an annotation to this conversation."""
        self.annotations[name] = value
    
    def has_annotation(self, name: str) -> bool:
        """Check if annotation exists."""
        return name in self.annotations
    
    def get_annotation(self, name: str, default: Any = None) -> Any:
        """Get annotation value."""
        return self.annotations.get(name, default)
    
    # Legacy compatibility
    @property
    def tags(self) -> List[Tag]:
        """Convert annotations back to Tag objects for backward compatibility."""
        tags = []
        for name, value in self.annotations.items():
            if value is True:
                tags.append(Tag(name))
            elif isinstance(value, dict):
                tags.append(Tag(name, **value))
            else:
                tags.append(Tag(name, value=value))
        return tags
    
    @tags.setter
    def tags(self, tag_list: List[Tag]) -> None:
        """Convert Tag objects to annotations for backward compatibility."""
        self.annotations = {}
        for tag in tag_list:
            self.annotations.update(tag.to_dict())
    
    @property
    def exchange_count(self) -> int:
        return len(self.exchanges)
    
    @property 
    def total_message_count(self) -> int:
        return sum(len(exchange.messages) for exchange in self.exchanges)
    
    @property
    def total_user_messages(self) -> int:
        return sum(len(exchange.get_user_messages()) for exchange in self.exchanges)
        
    @property
    def total_assistant_messages(self) -> int:
        return sum(len(exchange.get_assistant_messages()) for exchange in self.exchanges)
    
    @property
    def has_continuations(self) -> bool:
        return any(exchange.has_continuations() for exchange in self.exchanges)
    
    def get_all_user_text(self) -> str:
        return ' '.join(' '.join(exchange.get_user_texts()) for exchange in self.exchanges)
    
    def get_all_assistant_text(self) -> str:
        return ' '.join(' '.join(exchange.get_assistant_texts()) for exchange in self.exchanges)



---
File: attic/src/conversation_tagger/core/detection.py
---
# src/conversation_tagger/core/detection.py
"""
High-value detection rules for conversations and exchanges.
Updated to use dictionary-based annotations.
"""

import re
from typing import Dict, Any, List
from .exchange import Exchange
from .conversation import Conversation
from .tag import Tag, create_annotation
from .message import Message, MessageOpenAI

######################
#  Conversation Rules #
######################
# These should only do aggregation/summarization, not detection

def create_conversation_length_annotation(conversation: Conversation) -> Dict[str, Any]:
    """Create annotation for conversation length."""
    exchange_count = conversation.exchange_count
    
    # Determine category based on number of exchanges
    if exchange_count == 1:
        category = 'single'
    elif exchange_count <= 3:
        category = 'short'
    elif exchange_count <= 10:
        category = 'medium'
    elif exchange_count <= 25:
        category = 'long'
    else:
        category = 'very_long'
    
    return create_annotation('conversation_length', {
        'count': exchange_count,
        'category': category
    })


def conversation_feature_summary(conversation: Conversation) -> Dict[str, Any]:
    """Aggregate feature usage across all exchanges."""
    feature_counts = {}
    total_exchanges = conversation.exchange_count
    
    # Count exchanges with each feature
    for exchange in conversation.exchanges:
        exchange_features = set()
        for annotation_name in exchange.annotations:
            if annotation_name in ['has_github_repos', 'has_canvas_operations', 'has_web_search', 
                                 'has_reasoning_thoughts', 'has_code_execution', 'has_code_blocks',
                                 'has_script_headers', 'has_code_structure_patterns', 'has_wiki_links',
                                 'has_latex_math', 'user_has_attachments']:
                exchange_features.add(annotation_name)
            elif annotation_name.startswith('gizmo_'):
                exchange_features.add('has_gizmo_usage')
            elif annotation_name.startswith('plugin_'):
                exchange_features.add('has_plugin_usage')
        
        # Count each feature once per exchange
        for feature in exchange_features:
            feature_counts[feature] = feature_counts.get(feature, 0) + 1
    
    annotations = {}
    for feature, count in feature_counts.items():
        percentage = (count / total_exchanges) * 100 if total_exchanges > 0 else 0
        annotations[f'conversation_{feature}'] = {
            'exchange_count': count,
            'total_exchanges': total_exchanges,
            'percentage': round(percentage, 1)
        }
    
    return annotations


def conversation_gizmo_plugin_summary(conversation: Conversation) -> Dict[str, Any]:
    """Aggregate gizmo/plugin usage across all exchanges."""
    all_gizmos = set()
    all_plugins = set()
    gizmo_count = 0
    plugin_count = 0
    
    # Collect from all exchange annotations
    for exchange in conversation.exchanges:
        for name, value in exchange.annotations.items():
            if name.startswith('gizmo_'):
                if isinstance(value, dict) and 'gizmo_id' in value:
                    all_gizmos.add(value['gizmo_id'])
                gizmo_count += 1
            elif name.startswith('plugin_'):
                if isinstance(value, dict) and 'plugin_id' in value:
                    all_plugins.add(value['plugin_id'])
                plugin_count += 1
    
    annotations = {}
    
    # Summary annotations
    if all_gizmos:
        annotations['conversation_gizmo_usage'] = {
            'unique_gizmos': len(all_gizmos),
            'total_usage': gizmo_count,
            'gizmo_list': list(all_gizmos)
        }
    
    if all_plugins:
        annotations['conversation_plugin_usage'] = {
            'unique_plugins': len(all_plugins),
            'total_usage': plugin_count,
            'plugin_list': list(all_plugins)
        }
    
    return annotations


######################
#   Exchange Rules   #
######################
# These do actual detection on individual exchanges

# Feature detection (moved from conversation-level)
def has_github_repos(exchange: Exchange) -> bool:
    if isinstance(exchange.messages[0], MessageOpenAI):
        return has_github_repos_oai(exchange)

def has_github_repos_oai(exchange: Exchange) -> bool:
    """Check if GitHub repositories were selected for context in this exchange."""
    repos = None
    for message in exchange.messages:
        metadata = message.data.get('metadata', {})
        repos = metadata.get('selected_github_repos', [])
        if repos:
            return True
    return False

def has_canvas_operations(exchange: Exchange) -> bool:
    if isinstance(exchange.messages[0], MessageOpenAI):
        return has_canvas_operations_oai(exchange)
    
def has_canvas_operations_oai(exchange: Exchange) -> bool:
    """Check for canvas/document operations in this exchange."""
    for message in exchange.messages:
        metadata = message.data.get('metadata', {})
        if metadata.data.get('canvas'):
            return True
    return False


def has_web_search(exchange: Exchange) -> bool:
    if isinstance(exchange.messages[0], MessageOpenAI):
        return has_web_search_oai(exchange)
    
def has_web_search_oai(exchange: Exchange) -> bool:
    """Check for web search operations in this exchange."""
    for message in exchange.messages:
        metadata = message.data.get('metadata', {})
        if (metadata.data.get('search_queries') or 
            metadata.data.get('search_result_groups') or
            metadata.data.get('content_references')):
            return True
    return False


def has_reasoning_thoughts(exchange: Exchange) -> bool:
    if isinstance(exchange.messages[0], MessageOpenAI):
        return has_reasoning_thoughts_oai(exchange)
    
def has_reasoning_thoughts_oai(exchange: Exchange) -> bool:
    """Check for reasoning/thinking patterns in this exchange."""
    for message in exchange.messages:
        content = message.data.get('content', {})
        if content.get('thoughts'):  # Reasoning thoughts
            return True
    return False

def has_code_execution(exchange: Exchange) -> bool:
    if isinstance(exchange.messages[0], MessageOpenAI):
        return has_code_execution_oai(exchange)
    
def has_code_execution_oai(exchange: Exchange) -> bool:
    """Check for code execution artifacts in this exchange."""
    for message in exchange.messages:
        metadata = message.data.get('metadata', {})
        if (metadata.get('aggregate_result') or 
            metadata.get('jupyter_messages')):
            return True
    return False


def has_code_blocks(exchange: Exchange) -> bool:
    if isinstance(exchange.messages[0], MessageOpenAI):
        return has_code_blocks_oai(exchange)

# Code detection
def has_code_blocks_oai(exchange: Exchange) -> bool:
    """Check for explicit code blocks (``` markdown syntax)."""
    all_texts = exchange.get_user_texts() + exchange.get_assistant_texts()
    return any('```' in text for text in all_texts)


def has_script_headers(exchange: Exchange) -> bool:
    """Check for script headers and system includes."""
    all_texts = exchange.get_user_texts() + exchange.get_assistant_texts()
    script_indicators = ['#!/bin/', '#include', 'using namespace']
    
    for text in all_texts:
        if any(indicator in text for indicator in script_indicators):
            return True
    return False


def has_code_structure_patterns(exchange: Exchange) -> bool:
    """Check for actual code structure patterns (syntax combinations that suggest real code)."""
    all_texts = exchange.get_user_texts() + exchange.get_assistant_texts()
    
    for text in all_texts:
        # Look for combinations that strongly suggest actual code
        patterns = [
            # Function definition pattern
            ('def ' in text and '(' in text and ':' in text and 'return' in text),
            # Class definition pattern  
            ('class ' in text and '(' in text and ':' in text and 'def ' in text),
            # JavaScript function pattern
            ('function(' in text or 'function ' in text) and '{' in text and '}' in text,
            # Multiple assignment pattern
            text.count('=') >= 3 and ('let ' in text or 'const ' in text or 'var ' in text),
        ]
        
        if any(pattern for pattern in patterns):
            return True
    
    return False


# User behavior detection
def user_has_quote_elaborate(exchange: Exchange) -> bool:
    """Check if user messages contain quote+elaborate continuation pattern."""
    for message in exchange.get_user_messages():
        text = message.content
        if not text.startswith('>'):
            continue
        
        lines = text.split('\n')
        if len(lines) >= 2 and lines[-1].strip().lower() == 'elaborate':
            return True
    
    return False


def user_has_attachments(exchange: Exchange) -> bool:
    if isinstance(exchange.messages[0], MessageOpenAI):
        return user_has_attachments_oai(exchange)
    
def user_has_attachments_oai(exchange: Exchange) -> bool:
    """Check if user messages have attachments."""
    for message in exchange.get_user_messages():
        metadata = message.data.get('metadata', {})
        if metadata.get('attachments'):
            return True
    return False


def user_is_continuation(exchange: Exchange) -> bool:
    """Check if this exchange started with a continuation prompt."""
    return exchange.has_continuations()


# Assistant behavior detection
def assistant_has_reasoning(exchange: Exchange) -> bool:
    """Check if assistant messages contain reasoning/thinking content."""
    for message in exchange.get_assistant_messages():
        content = message.get('content', {})
        if content.get('thoughts'):
            return True
    return False


def has_wiki_links(exchange: Exchange) -> bool:
    """Check for Obsidian-style wiki links [[link text]]."""
    assistant_texts = exchange.get_assistant_texts()
    return any(bool(re.search(r'\[\[.+?\]\]', text)) for text in assistant_texts)


def has_latex_math(exchange: Exchange) -> bool:
    """Check for LaTeX/MathJax mathematical formulas."""
    assistant_texts = exchange.get_assistant_texts()
    
    for text in assistant_texts:
        math_indicators = [
            re.search(r'\$\$.+?\$\$', text) is not None,
            re.search(r'\\\((.+?)\\\)', text) is not None,
            re.search(r'\\\[(.+?)\\\]', text) is not None,
            # Common LaTeX commands
            any(cmd in text for cmd in ['\\frac', '\\sum', '\\int', '\\sqrt', '\\alpha', 
                                       '\\beta', '\\gamma', '\\theta', '\\pi', '\\sigma', 
                                       '\\infty', '\\partial', '\\nabla']),
        ]
        
        if any(math_indicators):
            return True
    
    return False

def first_user_has_large_content(exchange: Exchange, min_length: int = 2000) -> bool:
    """Check if the first user message has large content."""
    user_messages = exchange.get_user_messages()
    if not user_messages:
        return False
    
    first_message = user_messages[0]
    text = first_message.content
    
    return len(text.strip()) > min_length


def first_user_has_code_patterns(exchange: Exchange) -> bool:
    """Check if the first user message contains code patterns."""
    user_messages = exchange.get_user_messages()
    if not user_messages:
        return False
    
    first_message = user_messages[0]
    content = first_message.get('content', {})
    text = content.get('text', '')
    parts = content.get('parts', [])
    joined = ' '.join(str(p) for p in parts if isinstance(p, str)).strip()
    if joined:
        text = f"{text} {joined}"
    
    # Strong code indicators
    code_indicators = [
        '```',  # Code blocks
        'def ', 'function ', 'class ',  # Definitions
        'import ', 'from ', 'require(',  # Imports
        '#!/bin/', '#include',  # Script headers
    ]
    
    return any(indicator in text for indicator in code_indicators)


def first_user_has_attachments(exchange: Exchange) -> bool:
    """Check if the first user message has attachments."""
    user_messages = exchange.get_user_messages()
    if not user_messages:
        return False
    
    first_message = user_messages[0]
    metadata = first_message.data.get('metadata', {})
    attachments = metadata.get('attachments', [])
    return len(attachments) > 0


def first_user_has_code_attachments(exchange: Exchange) -> bool:
    """Check if the first user message has code-related attachments."""
    user_messages = exchange.get_user_messages()
    if not user_messages:
        return False
    
    first_message = user_messages[0]
    metadata = first_message.data.get('metadata', {})
    attachments = metadata.get('attachments', [])
    
    for attachment in attachments:
        mime_type = attachment.get('mime_type', '').lower()
        name = attachment.get('name', '').lower()
        
        # Check for code file extensions
        code_extensions = ['.py', '.js', '.java', '.cpp', '.c', '.go', '.rs', 
                          '.ts', '.jsx', '.tsx', '.sql', '.sh', '.rb', '.php']
        if any(ext in name for ext in code_extensions):
            return True
            
        # Check for code-related MIME types
        code_mimes = ['text/x-python', 'text/x-java', 'application/javascript', 'text/x-script']
        if any(mime in mime_type for mime in code_mimes):
            return True
    
    return False


def get_gizmo_annotations(exchange: Exchange) -> dict[str, Any]:
    if isinstance(exchange.messages[0], MessageOpenAI):
        return get_gizmo_annotations_oai(exchange)
    
def get_gizmo_annotations_oai(exchange: Exchange) -> dict[str, Any]:
    """Get annotations for specific gizmos used in this exchange."""
    annotations = {}
    gizmos = set()
    
    for message in exchange.messages:
        metadata = message.data.get('metadata', {})
        if metadata.get('gizmo_id'):
            gizmos.add(metadata['gizmo_id'])
    
    for i, gizmo in enumerate(gizmos):
        annotations[f'gizmo_{i+1}'] = {'gizmo_id': gizmo}
    
    return annotations


def get_plugin_annotations(exchange: Exchange) -> dict[str, Any]:
    if isinstance(exchange.messages[0], MessageOpenAI):
        return get_plugin_annotations_oai(exchange)
    
def get_plugin_annotations_oai(exchange: Exchange) -> dict[str, Any]:
    """Get annotations for specific plugins used in this exchange."""
    annotations = {}
    plugins = set()
    
    for message in exchange.messages:
        metadata = message.data.get('metadata', {})
        invoked_plugin = metadata.get('invoked_plugin', {})
        if invoked_plugin:
            if invoked_plugin.get('plugin_id'):
                plugins.add(invoked_plugin['plugin_id'])
            if invoked_plugin.get('namespace'):
                plugins.add(invoked_plugin['namespace'])
    
    for i, plugin in enumerate(plugins):
        annotations[f'plugin_{i+1}'] = {'plugin_id': plugin}
    
    return annotations


##############################
# Template content inference #
##############################

def naive_title_extraction(text):
    """
    Attempts to detect presence of title in first line of a message.
    """
    # get first line
    top = text.strip().split("\n")[0]

    # title/section header detected
    outv = None
    if top.startswith("#"):
        outv = top.replace("#","").strip()
    elif top.startswith("**") and top.endswith("**"):
        outv = top.replace("**","")
    if outv is not None:
        outv = outv.strip()
    return outv

def extract_proposed_title(exchange: Exchange) -> str:
    """
    Extracts proposed content title from the assistant's response.
    Assumes that an article was generated with a proposed title.
    """
    try:
       text = exchange.get_assistant_texts()[0]
    except IndexError:
        return None
    return naive_title_extraction(text)


######################
#   Rule Registry    #
######################

# High-value conversation-level rules (aggregation only)
CONVERSATION_RULES = {
    'conversation_length': create_conversation_length_annotation,
    'conversation_feature_summary': conversation_feature_summary,
    'conversation_gizmo_plugin_summary': conversation_gizmo_plugin_summary,
}

# High-value exchange-level rules (actual detection)
EXCHANGE_RULES = {
    # Feature detection (moved from conversation-level)
    'has_github_repos': has_github_repos,
    'has_canvas_operations': has_canvas_operations,
    'has_web_search': has_web_search,
    'has_reasoning_thoughts': has_reasoning_thoughts,
    'has_code_execution': has_code_execution,
    
    # Code detection
    'has_code_blocks': has_code_blocks,
    'has_script_headers': has_script_headers,
    'has_code_structure_patterns': has_code_structure_patterns,
    
    # User behavior
    'user_has_quote_elaborate': user_has_quote_elaborate,
    'user_has_attachments': user_has_attachments,
    'user_is_continuation': user_is_continuation,
    
    # Assistant behavior
    'assistant_has_reasoning': assistant_has_reasoning,
    'has_wiki_links': has_wiki_links,
    'has_latex_math': has_latex_math,
    
    # First user message analysis
    'first_user_has_large_content': first_user_has_large_content,
    'first_user_has_code_patterns': first_user_has_code_patterns,
    'first_user_has_attachments': first_user_has_attachments,
    'first_user_has_code_attachments': first_user_has_code_attachments,
    
    # Gizmo/plugin detection
    'get_gizmo_annotations': get_gizmo_annotations,
    'get_plugin_annotations': get_plugin_annotations,

    # Template content inference
    'proposed_title': extract_proposed_title,
}



---
File: attic/src/conversation_tagger/core/detection_old.py
---
"""
NB: The intention is to collect rule functions in this file for organizaitonal purposes, but at present
probably none of these will work out of the box. They were ported from an older brainstorming version of the
codebase and at minimum need to be updated to account for the new Exchange/Conversation objects, and several
probably need to have their logic re-implemented to just be better classifiers.

In the future, would be great if we could compress documents into vectors which could be used for these classifications.
"""
from typing import Dict, Any

from .exchange import Exchange


### Already implemented elsewhere, need to be moved here
# - wiki markdown
#   - TODO: add a "see also:{bullleted list}" detector
# - user first message?




### Content ###

def has_large_content(conversation: Dict[str, Any], min_length: int = 2000) -> bool:
    """Check if conversation has unusually large content anywhere."""
    mapping = conversation.get('mapping', {})
    
    for node_id, node in mapping.items():
        message = node.get('message')
        if not message:
            continue
            
        content = message.get('content', {})
        text = content.get('text', '')
        if len(text) > min_length:
            return True
            
        parts = content.get('parts', [])
        for part in parts:
            if isinstance(part, str) and len(part) > min_length:
                return True
    
    return False


def has_github_repos(conversation: Dict[str, Any]) -> bool:
    """Check if GitHub repositories were selected for context."""
    mapping = conversation.get('mapping', {})
    
    for node_id, node in mapping.items():
        message = node.get('message')
        if not message:
            continue
            
        metadata = message.get('metadata', {})
        repos = metadata.get('selected_github_repos', [])
        if repos:  # Non-empty list
            return True
    
    return False


def has_canvas_operations(conversation: Dict[str, Any]) -> bool:
    """Check for canvas/document operations."""
    mapping = conversation.get('mapping', {})
    
    for node_id, node in mapping.items():
        message = node.get('message')
        if not message:
            continue
            
        metadata = message.get('metadata', {})
        if metadata.get('canvas'):
            return True
    
    return False


def has_web_search(conversation: Dict[str, Any]) -> bool:
    """Check for web search operations."""
    mapping = conversation.get('mapping', {})
    
    for node_id, node in mapping.items():
        message = node.get('message')
        if not message:
            continue
            
        metadata = message.get('metadata', {})
        if (metadata.get('search_queries') or 
            metadata.get('search_result_groups') or
            metadata.get('content_references')):
            return True
    
    return False


def has_reasoning_thoughts(conversation: Dict[str, Any]) -> bool:
    """Check for reasoning/thinking patterns."""
    mapping = conversation.get('mapping', {})
    
    for node_id, node in mapping.items():
        message = node.get('message')
        if not message:
            continue
            
        content = message.get('content', {})
        if content.get('thoughts'):  # Reasoning thoughts
            return True
    
    return False


def has_code_execution(conversation: Dict[str, Any]) -> bool:
    """Check for code execution artifacts."""
    mapping = conversation.get('mapping', {})
    
    for node_id, node in mapping.items():
        message = node.get('message')
        if not message:
            continue
            
        metadata = message.get('metadata', {})
        if (metadata.get('aggregate_result') or 
            metadata.get('jupyter_messages')):
            return True
    
    return False


### Code Indicators ###

# conversation_tagger/detection/code_indicators.py
"""
Individual code detection functions - each detects a specific type of code evidence.
"""

from typing import Dict, Any

from .helpers import get_all_text_from_message


def has_code_blocks(conversation: Dict[str, Any]) -> bool:
    """Check for explicit code blocks (``` markdown syntax)."""
    mapping = conversation.get('mapping', {})
    
    for node_id, node in mapping.items():
        message = node.get('message')
        if not message:
            continue
        
        all_text = get_all_text_from_message(message)
        if '```' in all_text:
            return True
    
    return False


def has_function_definitions(conversation: Dict[str, Any]) -> bool:
    """Check for function/class definition keywords."""
    mapping = conversation.get('mapping', {})
    
    for node_id, node in mapping.items():
        message = node.get('message')
        if not message:
            continue
        
        all_text = get_all_text_from_message(message)
        definition_keywords = ['def ', 'function ', 'class ']
        if any(keyword in all_text for keyword in definition_keywords):
            return True
    
    return False


def has_import_statements(conversation: Dict[str, Any]) -> bool:
    """Check for import/require statements."""
    mapping = conversation.get('mapping', {})
    
    for node_id, node in mapping.items():
        message = node.get('message')
        if not message:
            continue
        
        all_text = get_all_text_from_message(message)
        import_keywords = ['import ', 'from ', 'require(']
        if any(keyword in all_text for keyword in import_keywords):
            return True
    
    return False


def has_script_headers(conversation: Dict[str, Any]) -> bool:
    """Check for script headers and system includes."""
    mapping = conversation.get('mapping', {})
    
    for node_id, node in mapping.items():
        message = node.get('message')
        if not message:
            continue
        
        all_text = get_all_text_from_message(message)
        script_indicators = ['#!/bin/', '#include', 'using namespace']
        if any(indicator in all_text for indicator in script_indicators):
            return True
    
    return False


def has_high_keyword_density(conversation: Dict[str, Any]) -> bool:
    """Check for high density of programming keywords in large text."""
    mapping = conversation.get('mapping', {})
    
    for node_id, node in mapping.items():
        message = node.get('message')
        if not message:
            continue
        
        all_text = get_all_text_from_message(message)
        
        # Only check substantial text
        if len(all_text) <= 1000:
            continue
        
        coding_keywords = ['function', 'class', 'import', 'def ', 'const ', 'let ', 'var ', 'return', 'if ', 'for ', 'while ']
        keyword_count = sum(1 for keyword in coding_keywords if keyword in all_text.lower())
        
        # High threshold to avoid false positives in articles
        if keyword_count >= 5:
            return True
    
    return False


def has_code_structure_patterns(conversation: Dict[str, Any]) -> bool:
    """Check for actual code structure patterns (syntax combinations that suggest real code)."""
    mapping = conversation.get('mapping', {})
    
    for node_id, node in mapping.items():
        message = node.get('message')
        if not message:
            continue
        
        all_text = get_all_text_from_message(message)
        
        # Look for combinations that strongly suggest actual code
        patterns = [
            # Function definition pattern
            ('def ' in all_text and '(' in all_text and ':' in all_text and 'return' in all_text),
            # Class definition pattern  
            ('class ' in all_text and '(' in all_text and ':' in all_text and 'def ' in all_text),
            # JavaScript function pattern
            ('function(' in all_text or 'function ' in all_text) and '{' in all_text and '}' in all_text,
            # Multiple assignment pattern
            all_text.count('=') >= 3 and ('let ' in all_text or 'const ' in all_text or 'var ' in all_text),
        ]
        
        if any(pattern for pattern in patterns):
            return True
    
    return False


def has_code_patterns(conversation: Dict[str, Any]) -> bool:
    """Check for any code patterns (combines individual indicators)."""
    return (has_code_blocks(conversation) or 
            has_function_definitions(conversation) or 
            has_import_statements(conversation) or 
            has_script_headers(conversation) or
            has_code_structure_patterns(conversation) or
            has_high_keyword_density(conversation))

  
### Continuation Rules ###

def user_has_quote_elaborate(exchange: Exchange) -> bool:
    """Check if user messages contain quote+elaborate continuation pattern."""
    for message in exchange.user_messages:
        content = message.get('content', {})
        text = content.get('text', '').strip()
        
        if not text.startswith('>'):
            continue
        
        lines = text.split('\n')
        if len(lines) >= 2 and lines[-1].strip().lower() == 'elaborate':
            return True
    
    return False

### Exchange Rules

# conversation_tagger/detection/exchange_rules.py
"""
Detection rules specifically designed for exchange-level analysis.
"""

from typing import Dict, Any
from ..core.exchange import Exchange
from ..core.tag import Tag


# User message detection rules
def user_has_code_blocks(exchange: Exchange) -> bool:
    """Check if user messages contain code blocks."""
    user_text = exchange.get_user_text()
    return '```' in user_text


def user_has_attachments(exchange: Exchange) -> bool:
    """Check if user messages have attachments."""
    for message in exchange.user_messages:
        metadata = message.get('metadata', {})
        if metadata.get('attachments'):
            return True
    return False


def user_has_error_messages(exchange: Exchange) -> bool:
    """Check if user messages contain error patterns."""
    user_text = exchange.get_user_text().lower()
    error_patterns = [
        'error:', 'traceback', 'exception:', 'failed:', 'cannot', 'not working',
        'broken', 'issue', 'problem', 'bug', 'crash', 'threw an error'
    ]
    return any(pattern in user_text for pattern in error_patterns)


def user_prompt_length_category(exchange: Exchange) -> Tag:
    """Categorize user prompt length."""
    user_text = exchange.get_user_text()
    length = len(user_text)
    
    if length < 50:
        category = 'very_short'
    elif length < 200:
        category = 'short'
    elif length < 1000:
        category = 'medium'
    elif length < 3000:
        category = 'long'
    else:
        category = 'very_long'
    
    return Tag('user_prompt_length', length=length, category=category)


def user_is_continuation(exchange: Exchange) -> bool:
    """Check if this exchange started with a continuation prompt."""
    return exchange.has_continuations()


# Assistant message detection rules
def assistant_has_code_blocks(exchange: Exchange) -> bool:
    """Check if assistant messages contain code blocks."""
    assistant_text = exchange.get_assistant_text()
    return '```' in assistant_text


def assistant_has_wiki_links(exchange: Exchange) -> bool:
    """Check if assistant messages contain wiki-style links."""
    assistant_text = exchange.get_assistant_text()
    return '[[' in assistant_text and ']]' in assistant_text


def assistant_has_latex_math(exchange: Exchange) -> bool:
    """Check if assistant messages contain mathematical formulas."""
    assistant_text = exchange.get_assistant_text()
    
    math_indicators = [
        ('$' in assistant_text and assistant_text.count('$') >= 2),
        '$$' in assistant_text,
        ('\\(' in assistant_text and '\\)' in assistant_text),
        any(cmd in assistant_text for cmd in ['\\frac', '\\sum', '\\int', '\\sqrt'])
    ]
    
    return any(math_indicators)


def assistant_response_length_category(exchange: Exchange) -> Tag:
    """Categorize assistant response length."""
    assistant_text = exchange.get_assistant_text()
    length = len(assistant_text)
    
    if length < 100:
        category = 'very_short'
    elif length < 500:
        category = 'short'
    elif length < 2000:
        category = 'medium'
    elif length < 5000:
        category = 'long'
    else:
        category = 'very_long'
    
    return Tag('assistant_response_length', length=length, category=category)


def assistant_has_reasoning(exchange: Exchange) -> bool:
    """Check if assistant messages contain reasoning/thinking content."""
    for message in exchange.assistant_messages:
        content = message.get('content', {})
        if content.get('thoughts'):
            return True
    return False


# Exchange-level detection rules
def exchange_is_coding_focused(exchange: Exchange) -> bool:
    """Check if the entire exchange is focused on coding."""
    return (user_has_code_blocks(exchange) or 
            assistant_has_code_blocks(exchange) or
            exchange.is_code_focused())


def exchange_is_wiki_article_focused(exchange: Exchange) -> bool:
    """Check if exchange is focused on wiki/documentation content."""
    user_text = exchange.get_user_text()
    assistant_text = exchange.get_assistant_text()
    
    wiki_indicators = [
        '[[' in user_text or '[[' in assistant_text,
        'write an article' in user_text.lower(),
        'create a wiki' in user_text.lower(),
        len(assistant_text) > 1000 and ('# ' in assistant_text or '## ' in assistant_text)
    ]
    
    return any(wiki_indicators)


def exchange_has_error_resolution(exchange: Exchange) -> bool:
    """Check if exchange involves error troubleshooting."""
    return (user_has_error_messages(exchange) and 
            len(exchange.assistant_messages) > 0)


def exchange_interaction_pattern(exchange: Exchange) -> Tag:
    """Determine the interaction pattern of this exchange."""
    user_stats = exchange.get_user_prompt_stats()
    assistant_stats = exchange.get_assistant_response_stats()
    
    if user_stats['message_count'] > 1:
        pattern = 'multi_turn'
    elif user_stats['length'] > 2000:
        pattern = 'context_heavy'
    elif assistant_stats['length'] > 3000:
        pattern = 'detailed_response'
    elif user_stats['length'] < 50 and assistant_stats['length'] < 200:
        pattern = 'quick_qa'
    else:
        pattern = 'standard'
    
    return Tag('interaction_pattern', 
               pattern=pattern,
               user_messages=user_stats['message_count'],
               assistant_messages=assistant_stats['message_count'])


# #  For exchange no, but something like this could be interesting for Conversation level analysis.
# def exchange_timing_stats(exchange: Exchange) -> Tag:
#     """Calculate timing statistics for the exchange."""
#     if exchange.start_time and exchange.end_time:
#         duration = exchange.end_time - exchange.start_time
        
#         if duration < 30:
#             speed = 'very_fast'
#         elif duration < 120:
#             speed = 'fast'
#         elif duration < 300:
#             speed = 'medium'
#         elif duration < 600:
#             speed = 'slow'
#         else:
#             speed = 'very_slow'
        
#         return Tag('exchange_timing', 
#                    duration_seconds=duration,
#                    speed_category=speed)
    
#     return Tag('exchange_timing', duration_seconds=0, speed_category='unknown')

### User 1st message
# possibly already implemented some or all of this elsewhere?

def first_user_has_large_content(conversation: Dict[str, Any], min_length: int = 2000) -> bool:
    """Check if the first user message has large content."""
    first_message = get_first_user_message(conversation)
    if not first_message:
        return False
    
    all_text = get_all_text_from_message(first_message)
    return len(all_text) > min_length


def first_user_has_code_patterns(conversation: Dict[str, Any]) -> bool:
    """Check if the first user message contains code patterns."""
    first_message = get_first_user_message(conversation)
    if not first_message:
        return False
    
    all_text = get_all_text_from_message(first_message)
    
    # Strong code indicators
    code_indicators = [
        '```',  # Code blocks
        'def ', 'function ', 'class ',  # Definitions
        'import ', 'from ', 'require(',  # Imports
        '#!/bin/', '#include',  # Script headers
    ]
    
    return any(indicator in all_text for indicator in code_indicators)


def first_user_has_attachments(conversation: Dict[str, Any]) -> bool:
    """Check if the first user message has attachments."""
    first_message = get_first_user_message(conversation)
    if not first_message:
        return False
    
    metadata = first_message.get('metadata', {})
    attachments = metadata.get('attachments', [])
    return len(attachments) > 0


def first_user_has_code_attachments(conversation: Dict[str, Any]) -> bool:
    """Check if the first user message has code-related attachments."""
    first_message = get_first_user_message(conversation)
    if not first_message:
        return False
    
    metadata = first_message.get('metadata', {})
    attachments = metadata.get('attachments', [])
    
    for attachment in attachments:
        mime_type = attachment.get('mime_type', '').lower()
        name = attachment.get('name', '').lower()
        
        # Check for code file extensions
        code_extensions = ['.py', '.js', '.java', '.cpp', '.c', '.go', '.rs', '.ts', '.jsx', '.tsx', '.sql', '.sh', '.rb', '.php']
        if any(ext in name for ext in code_extensions):
            return True
            
        # Check for code-related MIME types
        code_mimes = ['text/x-python', 'text/x-java', 'application/javascript', 'text/x-script']
        if any(mime in mime_type for mime in code_mimes):
            return True
    
    return False


### Structured Tags

# conversation_tagger/detection/structured_tags.py
"""
Functions that create structured tags with attributes.
"""

from typing import Dict, Any, List

from ..core.tag import Tag
from .helpers import get_all_user_messages


def create_conversation_length_tag(conversation: Dict[str, Any]) -> Tag:
    """Create structured tag for conversation length."""
    user_count = len(get_all_user_messages(conversation))
    
    # Determine category
    if user_count == 1:
        category = 'single'
    elif user_count <= 3:
        category = 'short'
    elif user_count <= 10:
        category = 'medium'
    elif user_count <= 25:
        category = 'long'
    else:
        category = 'very_long'
    
    return Tag('conversation_length', count=user_count, category=category)


def create_prompt_stats_tag(conversation: Dict[str, Any]) -> Tag:
    """Create structured tag for prompt statistics."""
    from .helpers import get_all_text_from_message
    
    user_messages = get_all_user_messages(conversation)
    
    if not user_messages:
        return Tag('prompt_stats', count=0, mean=0, median=0, variance=0, 
                  length_category='none', consistency='none')
    
    # Calculate message lengths
    lengths = []
    for message in user_messages:
        all_text = get_all_text_from_message(message)
        lengths.append(len(all_text))
    
    # Calculate statistics
    mean_length = sum(lengths) / len(lengths)
    sorted_lengths = sorted(lengths)
    n = len(sorted_lengths)
    median_length = (sorted_lengths[n//2] if n % 2 == 1 
                    else (sorted_lengths[n//2-1] + sorted_lengths[n//2]) / 2)
    variance = sum((x - mean_length) ** 2 for x in lengths) / len(lengths) if len(lengths) > 1 else 0
    
    # Determine categories
    if mean_length < 50:
        length_category = 'very_short'
    elif mean_length < 200:
        length_category = 'short'
    elif mean_length < 1000:
        length_category = 'medium'
    elif mean_length < 3000:
        length_category = 'long'
    else:
        length_category = 'very_long'
    
    if variance < 1000:
        consistency = 'consistent'
    elif variance < 10000:
        consistency = 'mixed'
    else:
        consistency = 'variable'
    
    return Tag('prompt_stats', 
               count=len(lengths),
               mean=round(mean_length, 1),
               median=round(median_length, 1),
               variance=round(variance, 1),
               length_category=length_category,
               consistency=consistency)


def create_gizmo_plugin_tags(conversation: Dict[str, Any]) -> List[Tag]:
    """Create structured tags for gizmos and plugins."""
    tags = []
    gizmos = set()
    plugins = set()
    
    # Check conversation-level
    if conversation.get('gizmo_id'):
        gizmos.add(conversation['gizmo_id'])
    
    plugin_ids = conversation.get('plugin_ids', [])
    if plugin_ids:
        plugins.update(plugin_ids)
    
    # Check message-level
    mapping = conversation.get('mapping', {})
    for node_id, node in mapping.items():
        message = node.get('message')
        if not message:
            continue
            
        metadata = message.get('metadata', {})
        
        # Invoked plugins
        invoked_plugin = metadata.get('invoked_plugin', {})
        if invoked_plugin:
            if invoked_plugin.get('plugin_id'):
                plugins.add(invoked_plugin['plugin_id'])
            if invoked_plugin.get('namespace'):
                plugins.add(invoked_plugin['namespace'])
        
        # Gizmo usage
        if metadata.get('gizmo_id'):
            gizmos.add(metadata['gizmo_id'])
    
    # Create tags
    for gizmo in gizmos:
        tags.append(Tag('gizmo', gizmo_id=gizmo))
    
    for plugin in plugins:
        tags.append(Tag('plugin', plugin_id=plugin))
    
    return tags



---
File: attic/src/conversation_tagger/core/exchange.py
---
# conversation_tagger/core/exchange.py
"""
Exchange abstraction with sequential message handling and merge capabilities.
Updated to use dictionary-based annotations.
"""

from typing import Dict, Any, List, Optional, TYPE_CHECKING
from dataclasses import dataclass, field
import uuid


if TYPE_CHECKING:
    from .tag import Tag

from .message import Message



@dataclass
class Exchange:
    """A sequential conversation exchange with merge capabilities."""
    conversation_id: str
    messages: list[Message]
    annotations: Dict[str, Any] = field(default_factory=dict)  # Dictionary-based annotations
    exchange_id: str|None = '' # this should just be the message id of the last assistant response, otherwise won't properly handle forks/leaves
    
    def __post_init__(self):
        _id = None
        if self.exchange_id:
            return
        #print(self.messages)
        if self.messages:
            _id = self.messages[-1].id
        if _id is None:
            _id = str(uuid.uuid4())
        self.exchange_id = _id

    @classmethod
    def create(cls, conversation_id: str, messages: List[Message]) -> 'Exchange':
        """Create a new exchange with a random UUID."""
        return cls(
            #exchange_id=str(uuid.uuid4()),
            conversation_id=conversation_id,
            messages=messages,
            annotations={}
        )
    
    @property
    def last_message_time(self) -> float:
        """Get the create_time of the last message for ordering."""
        if not self.messages:
            return 0.0
        return self.messages[-1].created_date
    
    @property
    def first_message_time(self) -> float:
        """Get the create_time of the first message for ordering."""
        if not self.messages:
            return 0.0
        return self.messages[0].created_date
    
    def has_continuations(self) -> bool:
        """Check if this exchange has continuation prompts (multiple user messages)."""
        return len(self.get_user_messages()) > 1
    
    def get_message_ids(self) -> List[str]:
        """Get the IDs of all messages in this exchange."""
        return [msg.id for msg in self.messages if msg.id]

    def get_user_messages(self) -> List[Dict[str, Any]]:
        """Get just the user messages."""
        return [msg for msg in self.messages if msg.author_role == 'user']
    
    def get_assistant_messages(self) -> List[Dict[str, Any]]:
        """Get just the assistant messages."""
        return [msg for msg in self.messages if msg.author_role == 'assistant']
    
    def get_user_texts(self) -> List[str]:
        """Get text from all user messages."""
        return [msg.content for msg in self.get_user_messages()]
    
    def get_assistant_texts(self) -> List[str]:
        """Get text from all assistant messages."""
        return [msg.content for msg in self.get_assistant_messages()]

    def add_annotation(self, name: str, value: Any = True) -> None:
        """Add an annotation to this exchange."""
        self.annotations[name] = value
    
    def has_annotation(self, name: str) -> bool:
        """Check if annotation exists."""
        return name in self.annotations
    
    def get_annotation(self, name: str, default: Any = None) -> Any:
        """Get annotation value."""
        return self.annotations.get(name, default)

    # # Legacy compatibility
    # @property 
    # def tags(self) -> List['Tag']:
    #     """Convert annotations back to Tag objects for backward compatibility."""
    #     from .tag import Tag
    #     tags = []
    #     for name, value in self.annotations.items():
    #         if value is True:
    #             tags.append(Tag(name))
    #         elif isinstance(value, dict):
    #             tags.append(Tag(name, **value))
    #         else:
    #             tags.append(Tag(name, value=value))
    #     return tags
    
    # @tags.setter
    # def tags(self, tag_list: List['Tag']) -> None:
    #     """Convert Tag objects to annotations for backward compatibility."""
    #     self.annotations = {}
    #     for tag in tag_list:
    #         self.annotations.update(tag.to_dict())

    def __add__(self, other: 'Exchange') -> 'Exchange':
        """Merge two exchanges by combining and time-ordering their messages."""
        if not isinstance(other, Exchange):
            raise TypeError("Can only add Exchange objects")
        
        if self.conversation_id != other.conversation_id:
            raise ValueError("Cannot merge exchanges from different conversations")
        
        # Combine and sort messages by create_time to ensure proper chronological order
        combined_messages = self.messages + other.messages
        combined_messages.sort(key=lambda msg: msg.created_date)
        
        # Merge annotations from both exchanges
        combined_annotations = {}
        combined_annotations.update(self.annotations)
        combined_annotations.update(other.annotations)
        
        # Create new exchange with combined content
        merged_exchange = Exchange(
            exchange_id=str(uuid.uuid4()),  # New UUID for merged exchange
            conversation_id=self.conversation_id,
            messages=combined_messages,
            annotations=combined_annotations
        )
        
        return merged_exchange
    
    def __len__(self) -> int:
        """Return number of messages in exchange."""
        return len(self.messages)
    
    @property
    def content(self) -> str:
        """Get concatenated content of all messages in this exchange."""
        return '\n'.join(str(msg) for msg in self.messages if msg.content).strip()
    
    # def __str__(self) -> str:
    #     """String representation showing message sequence."""
    #     roles = [msg.get('author', {}).get('role', 'unknown') for msg in self.messages]
    #     return f"Exchange({self.exchange_id[:8]}...: {' â†’ '.join(roles)})"



---
File: attic/src/conversation_tagger/core/exchange_parser.py
---
# src/conversation_tagger/core/exchange_parser.py
"""
Parse conversations into exchanges using a two-step approach:
1. Segment into dyadic USER-ASSISTANT chunks
2. Merge chunks when continuations are detected
"""

from typing import Dict, Any, List, Callable
from .exchange import Exchange
from .conversation import Conversation

from .message import Message, MessageOpenAI, MessageClaude
from .exchange_tagger import ExchangeTagger


def quote_elaborate_rule(previous_exchange: Exchange, current_exchange: Exchange) -> bool:
    """Check for quote + elaborate continuation pattern."""
    user_messages = current_exchange.get_user_messages()
    if not user_messages:
        return False
    
    first_user_message = user_messages[0]
    #content = first_user_message.get('content', {})
    #text = content.get('text', '').strip()
    text = first_user_message.content
    
    return (text.startswith('>') and 
            len(text.split('\n')) >= 2 and 
            text.split('\n')[-1].strip().lower() == 'elaborate')


def simple_continuation_rule(previous_exchange: Exchange, current_exchange: Exchange) -> bool:
    """Check for simple continuation keywords."""
    continuation_patterns = [
        'continue', 'more', 'keep going', 'go on', 'next', 
        'tell me more', 'expand', 'keep writing', 'finish'
    ]
    
    user_messages = current_exchange.get_user_messages()
    if not user_messages:
        return False
    
    first_user_message = user_messages[0]
    text = first_user_message.content
    
    return text in continuation_patterns


def short_continuation_rule(previous_exchange: Exchange, current_exchange: Exchange) -> bool:
    """Check for short prompts starting with continuation words."""
    continuation_starters = [
        'continue', 'more', 'keep going', 'go on', 'next', 
        'tell me more', 'expand', 'keep writing', 'finish', 'elaborate','do go on', 'make it so', 'yes', 'please', 'do it'
    ]
    
    user_messages = current_exchange.get_user_messages()
    if not user_messages:
        return False
    
    first_user_message = user_messages[0]
    text = first_user_message.content
    
    if len(text.split()) <= 3:
        for pattern in continuation_starters:
            if text.startswith(pattern):
                return True
    
    return False


class ExchangeParser:
    """Parses conversations into tagged exchanges."""
    SOURCE="LLM"
    def __init__(self, exchange_tagger: ExchangeTagger | None = None):
        self.continuation_rules: List[Callable[[Exchange, Exchange], bool]] = [
            quote_elaborate_rule,
            simple_continuation_rule,
            short_continuation_rule
        ]
        if exchange_tagger is None:
            exchange_tagger = ExchangeTagger()
        exchange_tagger.add_rule('source', lambda x: self.SOURCE)
        self.exchange_tagger = exchange_tagger

    def add_continuation_rule(self, rule_function: Callable[[Exchange, Exchange], bool]):
        """Add a new continuation detection rule."""
        self.continuation_rules.append(rule_function)

    def get_messages(self, conversation: dict):
        raise NotImplementedError
    
    def get_conversation_id(self, conversation: dict) -> str:
        raise NotImplementedError
    
    def get_title(self, conversation: dict) -> str:
        raise NotImplementedError

    def parse_conversation(self, conversation: Dict[str, Any]) -> Conversation:
        """Parse a conversation into a Conversation object with fully-tagged exchanges."""
        messages = self.get_messages(conversation)
        
        conversation_id = self.get_conversation_id(conversation)
        title = self.get_title(conversation)
        
        dyadic_exchanges = self._create_dyadic_exchanges(messages, conversation_id)
        merged_exchanges = self._merge_continuations(dyadic_exchanges)
        
        # Tag exchanges as they're finalized
        if self.exchange_tagger:
            tagged_exchanges = []
            for exchange in merged_exchanges:
                tagged_exchange = self.exchange_tagger.tag_exchange(exchange)
                tagged_exchanges.append(tagged_exchange)
        else:
            tagged_exchanges = merged_exchanges
        
        # Create and return Conversation object
        conv = Conversation(
            conversation_id=conversation_id,
            title=title,
            exchanges=tagged_exchanges,
            raw=conversation,
        )
        
        return conv
    
    def _create_dyadic_exchanges(self, messages: list[Message|dict], 
                                conversation_id: str) -> List[Exchange]:
        """Step 1: Create simple USER-ASSISTANT dyadic exchanges."""
        dyadic_exchanges = []
        current_pair = []
        
        for message in messages:
            # if not isinstance(message, Message):
            #     message=Message(**message)
            if message.author_role in ['user', 'assistant']:
                current_pair.append(message)
                
                # If we have a user->assistant pair, create exchange
                if (len(current_pair) == 2 and 
                    current_pair[0].author_role == 'user' and
                    current_pair[1].author_role == 'assistant'):
                    
                    exchange = Exchange.create(conversation_id, current_pair.copy())
                    dyadic_exchanges.append(exchange)
                    current_pair = []
                
                # Handle cases where we have multiple user messages or assistant messages
                elif len(current_pair) > 2:
                    # Create exchange with what we have so far
                    exchange = Exchange.create(conversation_id, current_pair.copy())
                    dyadic_exchanges.append(exchange)
                    current_pair = []
        
        # Handle any remaining messages
        if current_pair:
            exchange = Exchange.create(conversation_id, current_pair)
            dyadic_exchanges.append(exchange)
        
        return dyadic_exchanges
    
    def _merge_continuations(self, dyadic_exchanges: List[Exchange]) -> List[Exchange]:
        """Step 2: Merge exchanges when continuation patterns are detected."""
        if not dyadic_exchanges:
            return []
        
        merged_exchanges = []
        current_exchange = dyadic_exchanges[0]
        
        for i in range(1, len(dyadic_exchanges)):
            next_exchange = dyadic_exchanges[i]
            
            # Check if next exchange is a continuation using any rule
            should_merge = any(rule(current_exchange, next_exchange) 
                             for rule in self.continuation_rules)
            
            if should_merge:
                # Merge with current exchange (time-ordering handled by __add__)
                current_exchange = current_exchange + next_exchange
            else:
                # Finalize current exchange and start new one
                merged_exchanges.append(current_exchange)
                current_exchange = next_exchange
        
        # Add the final exchange
        merged_exchanges.append(current_exchange)
        
        return merged_exchanges


# TODO: 
# * Attach appropriate Message type to parser
#   - currently, determination of source delegated to
#     `message.msg_factory`, which is invoked in Exchange.create
# * Rename to ConversationParser?
class ExchangeParserOAI(ExchangeParser):
    SOURCE = "oai"
    def get_messages(self, conversation: dict):
        mapping = conversation.get('mapping', {})
        all_messages = []
        for node_id, node in mapping.items():
            message = node.get('message')
            if message and message.get('author'):
                create_time = message.get('create_time') or 0
                all_messages.append((create_time, message))
        all_messages.sort(key=lambda x: x[0])
        return [MessageOpenAI(data=msg) for _, msg in all_messages]
    
    def get_conversation_id(self, conversation: dict) -> str:
        return  conversation.get('conversation_id')

    def get_title(self, conversation: dict) -> str:
        return conversation.get('title')

class ExchangeParserClaude(ExchangeParser):
    SOURCE = "claude"
    def get_messages(self, conversation: dict):
        # Parse Claude conversation format
        chat_messages = conversation.get('chat_messages', [])
        all_messages = [MessageClaude(data=msg) for msg in chat_messages if msg]
        all_messages.sort(key=lambda x: x.created_date)
        return all_messages
    
    def get_conversation_id(self, conversation: dict) -> str:
        return conversation.get('uuid')
    def get_title(self, conversation: dict) -> str:
        return conversation.get('name')


---
File: attic/src/conversation_tagger/core/exchange_tagger.py
---
# src/conversation_tagger/core/exchange_tagger.py
"""
Tag individual exchanges using the improved exchange structure.
Updated to use dictionary-based annotations.
"""
from typing import Dict, Callable, Any
from .tag import Tag
from .exchange import Exchange


class ExchangeTagger:
    """Tags exchanges with configurable rules using annotations."""
    
    def __init__(self):
        self.rules: Dict[str, Callable] = {}
    
    def add_rule(self, annotation_name: str, rule_function: Callable):
        """Add rule for exchanges."""
        self.rules[annotation_name] = rule_function
    
    def tag_exchange(self, exchange: Exchange) -> Exchange:
        """Tag a single exchange and return the updated exchange."""
        for annotation_name, rule_func in self.rules.items():
            try:
                result = rule_func(exchange)
                if result:
                    if isinstance(result, bool):
                        # Simple boolean annotation
                        exchange.add_annotation(annotation_name, True)
                    elif isinstance(result, dict):
                        # Multiple annotations returned
                        for name, value in result.items():
                            exchange.add_annotation(name, value)
                    elif isinstance(result, Tag):
                        # Legacy Tag object - convert to annotation
                        exchange.annotations.update(result.to_dict())
                    else:
                        # Other truthy value - store as annotation value
                        exchange.add_annotation(annotation_name, result)
            except Exception as e:
                # Skip failed rules silently for now
                pass
        
        return exchange



---
File: attic/src/conversation_tagger/core/generate.py
---
"""
Generates Obsidian notes from a conversation.
"""
from typing import List, Dict, Any
from .conversation import Conversation
from .exchange import Exchange
from .message import Message

# Generate Obsidian notes from a conversation using jinja template from templates/article.md.jinja
import re
import os
from pathlib import Path
from jinja2 import Environment, FileSystemLoader, select_autoescape
from jinja2 import Template

import frontmatter
from yaml.parser import ParserError
from yaml.scanner import ScannerError


from loguru import logger

def sanitize_filename(title: str, max_length: int = 200) -> str:
    """
    Sanitize a title to be safe for use as a filename.
    
    Args:
        title: The title to sanitize
        max_length: Maximum length of the resulting filename
        
    Returns:
        A sanitized filename string
    """
    if not title:
        return
    # Replace problematic characters with underscores
    sanitized = re.sub(r'[<>:"/\\|?*\[\]]', '', title)
    if sanitized.lower().startswith('the '):
        sanitized = sanitized[4:]  # Remove 'the ' prefix if present
    # Truncate to max length
    return sanitized[:max_length].lower().strip()


def extract_title(exchange: Exchange) -> str:
    title = exchange.annotations.get('title')
    if not title:
        title = exchange.annotations.get('proposed_title')
    if not title:
        # If no title is set, use the first user message as the title
        user_messages = exchange.get_user_messages()
        if user_messages:
            title = user_messages[0].content.split('\n')[0]
    if not title:
        # If still no title, use a default
        title = f"_untitled_{exchange.exchange_id}"
    return sanitize_filename(title)

def load_template(template_name: str) -> Template:
    """Load a Jinja template from the templates directory."""
    templates_dir = Path(__file__).parent.parent / 'templates'
    env = Environment(
        loader=FileSystemLoader(templates_dir),
        autoescape=select_autoescape(['html', 'xml'])
    )
    return env.get_template(template_name)

def make_metadata(page) -> Dict[str, object]:
    """
    Build a dict that python-frontmatter will turn into YAML.
    """
    return {
        "title": extract_title(page),
        "date": page.first_message_time,
        "tags": [
            "autogenerated",
            f"source__{page.annotations['source']}",
            f"conversation_id__{page.conversation_id}",
            *(f"msg_{mid}" for mid in page.get_message_ids()),
        ],
    }


# before generating the notes, we need to infer some attributes, specifically
# - the title for the preceding note
# - the date of the conversation
# - the title of the proceding note
# notes will generally correspond to a single exchange, so we will generate one note per exchange
# thte title will be associated as an annotation on the exchange
# teh date is an attribute on the exchange object, or the first message in the exchange
# output filename will be the title of the exchange, with spaces replaced by underscores and .md extension
def generate_notes(
        conversation: Conversation,
        template_name: str = 'article_body.md.jinja',
        output_dir: str = 'data/staging'
) -> List[str]:
    """Generate Obsidian notes from a conversation."""
    template = load_template(template_name)
    notes = []

    # need to infer the previous and next note titles before we can generate the notes
    # this is done by iterating through the exchanges and using the annotations
    # we will use the first message's created_date as the date of the exchange
    # and the title from the exchange annotations, or a default title if not present    
    # START BY ASSIGNING DEFAULT TITLES AND FILENAMES SO WE CAN REFER TO THEM WHEN WE NEED THE PREVIOUS AND NEXT TITLES
    for exchange in conversation.exchanges:
        date = exchange.messages[0].created_date if exchange.messages else None
        #title = exchange.annotations.get('title', f'Exchange {exchange.exchange_id}')
    
        title = extract_title(exchange)
                
        #output_filename = f"{title.replace(' ', '_')}.md"
        # need to actually sanitize the title to make it a valid filename
        #output_filename = f"{title.replace(' ', '_').replace('/', '_').replace('\\', '_').replace(':', '_')[:200]}.md"
        output_filename = sanitize_filename(title) + '.md'
        #logger.info(f"output_filename: {output_filename}")
        exchange.annotations['output_filename'] = output_filename
        exchange.annotations['date'] = date
        exchange.annotations['title'] = title
        notes.append((exchange, output_filename))       

    # NOW ASSOCIATE PREVIOUS AND NEXT TITLES
    for i, (exchange, output_filename) in enumerate(notes):
        # Set previous title if not the first exchange
        if i > 0:
            previous_exchange = notes[i - 1][0]
            exchange.annotations['previous_title'] = previous_exchange.annotations['title']
            exchange.annotations['previous_filename'] = previous_exchange.annotations['output_filename']
        else:
            exchange.annotations['previous_title'] = None
            exchange.annotations['previous_filename'] = None
        
        # Set next title if not the last exchange
        if i < len(notes) - 1:
            next_exchange = notes[i + 1][0]
            exchange.annotations['next_title'] = next_exchange.annotations['title']
            exchange.annotations['next_filename'] = next_exchange.annotations['output_filename']
        else:
            exchange.annotations['next_title'] = None
            exchange.annotations['next_filename'] = None

    # NOW GENERATE THE (first draft) NOTES
    # in retrospect, we should infer initial title, frontmatter, and content,
    # then we can update wikilinks in notes as needed before writing to file
    articles = {}
    for exchange, output_filename in notes: 
        content = template.render(page=exchange)
        metadata = make_metadata(exchange)
        # try:
        #     metadata, content = frontmatter.parse(content)
        # except (ParserError, ScannerError) as e:
        #     print(f"Error parsing frontmatter for {output_filename}: {e}")
        #     print(content)
        #     raise
        #title = exchange.annotations.get('title')
        title = metadata['title']

        # merge matching titles
        if title not in articles:
            articles[title] = {
                "content": content,
                "metadata": metadata,
                "output_filename": output_filename,
                "exchange":exchange,
            }
        else:
            article = articles[title]
            article['content'] += f"\n---\n{content}"
            for k,v in metadata.items():
                if k in article['metadata']:
                    # ... do stuff
                    if isinstance(metadata[k], list):
                        article['metadata'][k] += metadata[k]
                else:
                    article['metadata'][k] = v

    # find titles that appear in content, convert to wikilinks
    # TODO: match unsanitized (or maybe extra sanitized?) titles
    for title, article in list(articles.items()):
        content = article['content']
        # replace other titles with wikilinks (if not already a wikilink) 
        for other_title, other_article in articles.items():
            if other_title != title:
                #TODO: wrap in a "wikilinkify" function
                if not re.search(r'\[\[' + re.escape(other_title) + r'\]\]', content):
                    # Replace only if not already a wikilink
                    # Use word boundary to avoid partial matches
                    didchange = False
                    content_after = re.sub(rf'\b{re.escape(other_title)}\b', f'[[{other_title}]]', content)
                    if content_after != content:
                        content = content_after
                        didchange = True
                    articles[title]['content'] = content
                    # can I just do `article['content'] = content`?
                
                    # while we're here, let's check if we're directly quoting separately from the inferred title
                    if not didchange:
                        # check if prompt starts with a blockquote
                        exchange: Exchange = article['exchange']
                        #prompt = exchange.get_user_texts()[0] # should probably just make this a propoerty, Exchange.prompt
                        prompt = exchange.messages[0].content
                        # remove USER commentary on quoted text
                        quote = prompt
                        if '\n' in prompt:
                            quote, *_ = prompt.split('\n')
                        # remove blockquote markdown
                        if prompt.startswith('>'):
                            while quote.startswith('>'):
                                quote = quote[1:]
                            #if quote in articles[other_title]['content']:
                            # standardize before matching
                            sanitized_other_content = other_article['content'][:]
                            sanitized_other_content = sanitize_filename(sanitized_other_content)
                            sanitized_other_content = sanitized_other_content.replace('#','')
                            sanitized_quote = sanitize_filename(quote)
                            sanitized_quote = sanitized_quote.replace('#','')
                            if sanitized_quote.lower() in sanitized_other_content.lower():
                                #articles[other_title]['content'] += f"\nsee also: [[{title}]]  "
                                other_article['content'] += f"\nsee also: [[{title}]]  "


                            
                        


    # Ensure output directory exists
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # ok, now we can write the articles to files
    for title, article in articles.items():
        content = article['content']
        metadata = article['metadata']
        output_filename = article['output_filename']

        post = frontmatter.Post(content, **metadata)        
        output_filepath = output_path / output_filename
        with open(output_filepath, 'a') as f:
            #frontmatter.dump(post, f)
            f.write(frontmatter.dumps(post))


---
File: attic/src/conversation_tagger/core/message.py
---
from typing import Any
# from datetime import datetime


class Message:
    def __init__(self, data: dict):
        self.data = data
    
    @property
    def content(self):
        return self._get_content()
    
    @property
    def created_date(self):
        return self._get_created_date()
    
    @property
    def author_role(self):
        return self._get_author_role()

    @property
    def id(self):
        return self._get_id()

    def _get_id(self):
        raise NotImplementedError

    def _get_author_role(self):
        raise NotImplementedError

    def _get_content(self):
        raise NotImplementedError
    
    def _get_created_date(self):
        raise NotImplementedError
    def __repr__(self):
        #return f"Message(author_role={self.author_role}, content={self.content}, created_date={self.created_date})"
        return f"\n{self.created_date} - {self.author_role.upper()}: {self.content[:200].strip()+'...' if len(self.content) > 200 else self.content.strip()}"
    def __str__(self):
        return f"\n{self.created_date} - {self.author_role.upper()}: {self.content.strip()}"


def get_message_text_chatgpt(message: dict[str, Any]) -> str:
    """Extract text content from a message."""
    content = message.get('content', {})
    text = content.get('text', '')
    parts = content.get('parts', [])
    joined = ' '.join(str(p) for p in parts if isinstance(p, str)).strip()
    if joined:
        text = f"{text} {joined}"
    return text.strip()


class MessageOpenAI(Message):
    def _get_id(self):
        return self.data.get('id')
    def _get_content(self):
        return get_message_text_chatgpt(self.data)
    def _get_created_date(self):
        return self.data.get('create_time', 0.0)
    def _get_author_role(self):
        return self.data.get('author', {}).get('role')


class MessageClaude(Message):
    def _get_id(self):
        return self.data.get('uuid')
    def _get_content(self):
        return self.data.get('text', '')
    
    def _get_created_date(self):
        # Claude uses ISO format: "2024-01-01T12:00:00Z"
        created_at = self.data.get('created_at')
        # if created_at:
        #     return datetime.fromisoformat(created_at.replace('Z', '+00:00')).timestamp()
        # return 0.0
        return created_at
    
    def _get_author_role(self):
        sender = self.data.get('sender')
        if sender == 'human':
            sender = 'user'
        return sender

# def is_oai_msg(msg):
#     #return True
#     return isinstance(msg, dict) and 'content' in msg and 'create_time' in msg and 'author' in msg

# def is_anthropic_msg(msg):
#     return isinstance(msg, dict) and 'text' in msg and 'created_at' in msg and 'author' in msg

# def msg_factory(msg):
#     if is_oai_msg(msg):
#         return MessageOpenAI(data=msg)
#     else:
#         raise NotImplementedError


---
File: attic/src/conversation_tagger/core/tag.py
---
# conversation_tagger/core/tag.py
"""
Simplified annotation system using dictionaries instead of custom Tag objects.
"""

from typing import Any, Dict, Union


def create_annotation(name: str, value: Union[bool, int, float, str, Dict[str, Any]] = True) -> Dict[str, Any]:
    """Create a simple annotation as a dictionary entry."""
    return {name: value}


def merge_annotations(*annotation_dicts: Dict[str, Any]) -> Dict[str, Any]:
    """Merge multiple annotation dictionaries."""
    result = {}
    for annotations in annotation_dicts:
        result.update(annotations)
    return result


def has_annotation(annotations: Dict[str, Any], name: str) -> bool:
    """Check if an annotation exists."""
    return name in annotations


def get_annotation_value(annotations: Dict[str, Any], name: str, default: Any = None) -> Any:
    """Get the value of an annotation."""
    return annotations.get(name, default)


# Legacy Tag class for backward compatibility during transition
class Tag:
    """A tag with optional key-value attributes - DEPRECATED: Use dictionaries instead."""
    
    def __init__(self, name: str, **attributes):
        self.name = name
        self.attributes = attributes
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary format."""
        if not self.attributes:
            return {self.name: True}
        elif len(self.attributes) == 1 and 'value' in self.attributes:
            return {self.name: self.attributes['value']}
        else:
            return {self.name: self.attributes}
    
    def __str__(self):
        if self.attributes:
            attrs = ", ".join(f"{k}={v}" for k, v in self.attributes.items())
            return f"{self.name}({attrs})"
        return self.name
    
    def __repr__(self):
        return f"Tag('{self.name}', {self.attributes})"
    
    def __eq__(self, other):
        if isinstance(other, str):
            return self.name == other
        elif isinstance(other, Tag):
            return self.name == other.name and self.attributes == other.attributes
        return False
    
    def __hash__(self):
        return hash((self.name, tuple(sorted(self.attributes.items()))))



---
File: attic/src/conversation_tagger/core/tagger.py
---
# src/conversation_tagger/core/tagger.py
"""
Main ConversationTagger that orchestrates the exchange-based analysis.
Updated to use dictionary-based annotations.
"""

from typing import Dict, Any, List, Callable
from .exchange_parser import ExchangeParser, ExchangeParserOAI
from .exchange_tagger import ExchangeTagger
from .conversation import Conversation
from .exchange import Exchange
from .tag import Tag


class ConversationTagger:
    """Main tagger that uses exchange-based analysis with annotations."""
    
    def __init__(self, exchange_parser: ExchangeParser | None = None):
        if not exchange_parser:
            exchange_parser = ExchangeParserOAI()
        self.exchange_parser = exchange_parser
        self.conversation_rules: Dict[str, Callable] = {}
    
    def add_exchange_rule(self, annotation_name: str, rule_function: Callable):
        """Add rule for analyzing exchanges."""
        self.exchange_parser.exchange_tagger.add_rule(annotation_name, rule_function)

    def add_conversation_rule(self, annotation_name: str, rule_function: Callable):
        """Add rule for analyzing entire conversations."""
        self.conversation_rules[annotation_name] = rule_function
    
    def tag_conversation(self, conversation: Dict[str, Any]) -> Conversation:
        """Tag a conversation using exchange-based analysis."""
        # Parse into tagged exchanges and return Conversation object
        conv = self.exchange_parser.parse_conversation(conversation)
        
        # Apply conversation-level tagging rules
        for annotation_name, rule_func in self.conversation_rules.items():
            try:
                result = rule_func(conv)
                if result:
                    if isinstance(result, bool):
                        conv.add_annotation(annotation_name, True)
                    elif isinstance(result, dict):
                        # Multiple annotations returned
                        for name, value in result.items():
                            conv.add_annotation(name, value)
                    elif isinstance(result, Tag):
                        # Legacy Tag object - convert to annotation
                        conv.annotations.update(result.to_dict())
                    elif isinstance(result, list):
                        # Handle multiple tags returned from one rule
                        for item in result:
                            if isinstance(item, Tag):
                                conv.annotations.update(item.to_dict())
                            elif isinstance(item, dict):
                                conv.annotations.update(item)
                            else:
                                # Treat other items as simple annotations
                                conv.add_annotation(annotation_name, item)
                    else:
                        # Treat other truthy values as simple annotations
                        conv.add_annotation(annotation_name, result)
            except Exception as e:
                # Skip failed rules - could add logging here later
                pass
        
        return conv



---
File: attic/src/conversation_tagger/factory.py
---
# conversation_tagger/factory.py
"""
Factory to create configured tagger with improved exchange handling.
"""
#from ATTIC.conversation_tagger.core import exchange_parser
from .core.tagger import ConversationTagger
from .core.detection import EXCHANGE_RULES, CONVERSATION_RULES

# todo: use enum for source types
def create_default_tagger(source="oai") -> ConversationTagger:
    """Create a basic tagger with example rules for the new exchange design."""

    if source == "oai":
        from .core.exchange_parser import ExchangeParserOAI
        exchange_parser = ExchangeParserOAI()
    elif source == "claude":
        from .core.exchange_parser import ExchangeParserClaude
        exchange_parser = ExchangeParserClaude()
    else:
        raise ValueError(f"Unsupported source: {source}")
    
    tagger = ConversationTagger(exchange_parser=exchange_parser)
    for rule_name, rule_func in EXCHANGE_RULES.items():
        tagger.add_exchange_rule(rule_name, rule_func)   
    for rule_name, rule_func in CONVERSATION_RULES.items():
        tagger.add_conversation_rule(rule_name, rule_func)
    
    return tagger



---
File: attic/tests/conversation_tagger/conftest.py
---
# tests/conftest.py
"""
Shared test fixtures and configuration.
"""
from pathlib import Path
import sys
#PATH=str((Path().cwd().parent /'src').absolute())
PATH=str((Path().cwd() /'src').absolute())
print(PATH)
if PATH not in sys.path:
    sys.path.append(PATH)

import pytest
from conversation_tagger.core.exchange import Exchange
from conversation_tagger.core.message import Message


@pytest.fixture
def simple_user_message():
    """A basic user message."""
    return Message(**{
        'author': {'role': 'user'},
        'create_time': 1000,
        'content': {'text': 'Hello, how are you?'}
    })


@pytest.fixture
def simple_assistant_message():
    """A basic assistant message."""
    return {
        'author': {'role': 'assistant'},
        'create_time': 2000,
        'content': {'text': 'I am doing well, thank you!'}
    }


@pytest.fixture
def basic_exchange(simple_user_message, simple_assistant_message):
    """A simple two-message exchange."""
    return Exchange.create('test_conv', [simple_user_message, simple_assistant_message])


@pytest.fixture
def minimal_conversation_data():
    """Minimal conversation data for parsing tests."""
    return {
        'conversation_id': 'test_conv',
        'title': 'Test Conversation',
        'mapping': {
            'msg1': {
                'message': {
                    'author': {'role': 'user'},
                    'create_time': 1000,
                    'content': {'text': 'Test question'}
                }
            },
            'msg2': {
                'message': {
                    'author': {'role': 'assistant'},
                    'create_time': 2000,
                    'content': {'text': 'Test answer'}
                }
            }
        }
    }



---
File: attic/tests/conversation_tagger/test_basic_working.py
---
# tests/test_basic_working.py
"""
Tests for functionality that we know works in the current implementation.
Updated to test annotation-based system.
"""

import pytest
from conversation_tagger import create_default_tagger
from conversation_tagger.core.exchange import Exchange
from conversation_tagger.core.exchange_tagger import ExchangeTagger
from conversation_tagger.core.tag import Tag, create_annotation, merge_annotations


from conversation_tagger.core.message import MessageOpenAI

def test_annotation_functionality():
    """Test that annotation helpers work correctly."""
    # Simple annotation
    simple = create_annotation('test_annotation', True)
    assert simple == {'test_annotation': True}
    
    # Valued annotation
    valued = create_annotation('count', 42)
    assert valued == {'count': 42}
    
    # Complex annotation
    complex_data = {'type': 'test', 'score': 0.95}
    complex_ann = create_annotation('analysis', complex_data)
    assert complex_ann == {'analysis': complex_data}
    
    # Merge annotations
    merged = merge_annotations(simple, valued, complex_ann)
    assert 'test_annotation' in merged
    assert 'count' in merged
    assert 'analysis' in merged
    assert merged['count'] == 42


# def test_tag_backward_compatibility():
#     """Test that Tag objects still work and convert properly."""
#     tag = Tag('test_tag', value='test_value')
#     assert tag.name == 'test_tag'
#     assert tag.attributes['value'] == 'test_value'
    
#     # Test conversion to annotation format
#     annotation_dict = tag.to_dict()
#     assert annotation_dict == {'test_tag': 'test_value'}
    
#     # Test complex tag
#     complex_tag = Tag('stats', count=5, average=2.5, category='medium')
#     complex_dict = complex_tag.to_dict()
#     assert complex_dict == {'stats': {'count': 5, 'average': 2.5, 'category': 'medium'}}


def test_exchange_creation_and_annotations():
    """Test basic exchange creation and annotation handling."""
    messages = [
        {'author': {'role': 'user'}, 'content': {'text': 'Test'}, 'create_time': 1000},
        {'author': {'role': 'assistant'}, 'content': {'text': 'Response'}, 'create_time': 2000}
    ]
    messages = [MessageOpenAI(data=msg) for msg in messages]
    exchange = Exchange.create('test_conv', messages)
    
    assert exchange.conversation_id == 'test_conv'
    assert len(exchange.messages) == 2
    assert len(exchange.get_user_messages()) == 1
    assert len(exchange.get_assistant_messages()) == 1
    
    # Test adding annotations
    exchange.add_annotation('has_greeting', True)
    exchange.add_annotation('message_count', 2)
    exchange.add_annotation('analysis', {'sentiment': 'positive', 'confidence': 0.8})
    
    assert exchange.has_annotation('has_greeting')
    assert exchange.get_annotation('message_count') == 2
    assert exchange.get_annotation('analysis')['sentiment'] == 'positive'
    assert not exchange.has_annotation('missing')


def test_exchange_text_api_with_annotations():
    """Test the text extraction API and annotation usage."""
    messages = [
        {'author': {'role': 'user'}, 'content': {'text': 'Hello world'}, 'create_time': 1000},
        {'author': {'role': 'assistant'}, 'content': {'text': 'Hi there'}, 'create_time': 2000}
    ]
    messages = [MessageOpenAI(data=msg) for msg in messages]
    
    exchange = Exchange.create('test_conv', messages)
    
    # Test what the exchange actually provides
    user_texts = exchange.get_user_texts()
    assistant_texts = exchange.get_assistant_texts()
    
    # Verify these are lists (based on implementation)
    assert isinstance(user_texts, list)
    assert isinstance(assistant_texts, list)
    assert len(user_texts) == 1
    assert len(assistant_texts) == 1
    assert 'Hello world' in user_texts[0]
    assert 'Hi there' in assistant_texts[0]
    
    # Test annotation based on text analysis
    def analyze_text(texts):
        combined = ' '.join(texts)
        return {
            'word_count': len(combined.split()),
            'char_count': len(combined),
            'has_greeting': any(word in combined.lower() for word in ['hello', 'hi', 'hey'])
        }
    
    user_analysis = analyze_text(user_texts)
    assistant_analysis = analyze_text(assistant_texts)
    
    exchange.add_annotation('user_analysis', user_analysis)
    exchange.add_annotation('assistant_analysis', assistant_analysis)
    
    assert exchange.get_annotation('user_analysis')['word_count'] == 2
    assert exchange.get_annotation('assistant_analysis')['has_greeting'] is True


def test_exchange_tagger_with_annotations():
    """Test exchange tagger using the annotation system."""
    tagger = ExchangeTagger()
    
    def greeting_detector(exchange):
        """A rule that uses the correct API and returns annotation data."""
        user_texts = exchange.get_user_texts()
        if user_texts:
            text = ' '.join(user_texts).lower()
            if any(greeting in text for greeting in ['hello', 'hi', 'hey']):
                return {
                    'has_greeting': True,
                    'greeting_type': 'informal' if 'hi' in text or 'hey' in text else 'formal'
                }
        return False
    
    def message_counter(exchange):
        """Rule that returns simple numeric annotation."""
        return len(exchange.messages)
    
    tagger.add_rule('greeting_analysis', greeting_detector)
    tagger.add_rule('message_count', message_counter)
    
    messages = [
        {'author': {'role': 'user'}, 'content': {'text': 'Hello world'}, 'create_time': 1000},
        {'author': {'role': 'assistant'}, 'content': {'text': 'Hi there!'}, 'create_time': 2000}
    ]
    messages = [MessageOpenAI(data=msg) for msg in messages]
    # Test with exchange that should match
    exchange = Exchange.create('test', messages)
    
    tagged = tagger.tag_exchange(exchange)
    
    # Check annotations
    assert tagged.has_annotation('has_greeting')
    assert tagged.get_annotation('has_greeting') is True
    assert tagged.get_annotation('greeting_type') == 'formal'
    assert tagged.get_annotation('message_count') == 2
    
    # Test backward compatibility - can still access as tags
    # tag_names = [tag.name for tag in tagged.tags]
    # assert 'has_greeting' in tag_names or any('greeting' in name for name in tag_names)
    # assert 'message_count' in tag_names


def test_conversation_parsing_with_annotations():
    """Test basic conversation parsing with annotation support."""
    conversation_data = {
        'conversation_id': 'test_conv',
        'title': 'Test Chat',
        'mapping': {
            'msg1': {
                'message': {
                    'author': {'role': 'user'},
                    'create_time': 1000,
                    'content': {'text': 'Hello'}
                }
            },
            'msg2': {
                'message': {
                    'author': {'role': 'assistant'},
                    'create_time': 2000,
                    'content': {'text': 'Hi there!'}
                }
            }
        }
    }
    
    tagger = create_default_tagger()
    
    # Add a custom annotation rule
    def simple_stats(exchange):
        return {
            'user_messages': len(exchange.get_user_messages()),
            'assistant_messages': len(exchange.get_assistant_messages()),
            'total_messages': len(exchange.messages)
        }
    
    tagger.add_exchange_rule('stats', simple_stats)
    
    result = tagger.tag_conversation(conversation_data)
    
    # Test basic structure
    assert result.conversation_id == 'test_conv'
    assert result.title == 'Test Chat'
    assert result.exchange_count == 1
    
    # Test that we can access the exchange and its annotations
    exchange = result.exchanges[0]
    assert len(exchange.messages) == 2
    
    # Check that our custom annotation rule was applied
    assert exchange.has_annotation('user_messages')
    assert exchange.get_annotation('user_messages') == 1
    assert exchange.get_annotation('assistant_messages') == 1
    assert exchange.get_annotation('total_messages') == 2
    
    # Test text extraction still works
    user_texts = exchange.get_user_texts()
    assert isinstance(user_texts, list)
    assert len(user_texts) == 1
    assert 'Hello' in user_texts[0]


def test_default_tagger_with_annotations():
    """Test that the default tagger works with annotation system."""
    tagger = create_default_tagger()
    assert tagger is not None
    assert hasattr(tagger, 'exchange_parser')
    assert hasattr(tagger.exchange_parser, 'exchange_tagger')
    
    # Test that it has some default rules
    assert len(tagger.exchange_parser.exchange_tagger.rules) > 0


def test_default_rules_produce_annotations():
    """Test that default rules work and produce annotations."""
    conversation_data = {
        'conversation_id': 'code_conv',
        'title': 'Code Chat',
        'mapping': {
            'msg1': {
                'message': {
                    'author': {'role': 'user'},
                    'create_time': 1000,
                    'content': {'text': 'def hello(): print("hello")'},
                    'metadata': {'attachments': []}
                }
            },
            'msg2': {
                'message': {
                    'author': {'role': 'assistant'},
                    'create_time': 2000,
                    'content': {'text': 'Nice Python function!'}
                }
            }
        }
    }
    
    tagger = create_default_tagger()
    result = tagger.tag_conversation(conversation_data)
    
    # Check if any annotations were applied
    all_annotations = {}
    for exchange in result.exchanges:
        all_annotations.update(exchange.annotations)
    
    # Also check conversation-level annotations
    all_annotations.update(result.annotations)
    
    annotation_names = list(all_annotations.keys())
    print(f"Applied annotations: {annotation_names}")
    
    # Some rules should work - at minimum we should have non-empty result
    assert len(result.exchanges) > 0
    
    # Should detect code patterns in the first user message
    first_exchange = result.exchanges[0]
    # We know from testing that first_user_has_code_patterns should work
    if first_exchange.has_annotation('first_user_has_code_patterns'):
        assert first_exchange.get_annotation('first_user_has_code_patterns') is True


def test_annotation_backward_compatibility_workflow():
    """Test complete workflow using both annotations and legacy tags."""
    # Create exchange
    exchange = Exchange.create('test', [
        MessageOpenAI(data={'author': {'role': 'user'}, 'content': {'text': 'Hello'}, 'create_time': 1000})
    ])
    
    # Add annotations directly (new way)
    exchange.add_annotation('modern_flag', True)
    exchange.add_annotation('score', 85)
    exchange.add_annotation('metadata', {'version': '2.0', 'processed': True})
    
    # # Add via legacy tag interface (old way)
    # legacy_tags = [
    #     Tag('legacy_flag'),
    #     Tag('rating', value=4.5),
    #     Tag('details', category='important', priority='high')
    # ]
    
    # # This should merge with existing annotations
    # old_annotations = exchange.annotations.copy()
    # exchange.tags = exchange.tags + legacy_tags  # Append to existing
    
    # Verify all annotations are present
    assert exchange.has_annotation('modern_flag')
    assert exchange.has_annotation('score')
    assert exchange.has_annotation('metadata')
    # assert exchange.has_annotation('legacy_flag')
    # assert exchange.has_annotation('rating')
    # assert exchange.has_annotation('details')
    
    # Verify values are correct
    assert exchange.get_annotation('modern_flag') is True
    assert exchange.get_annotation('score') == 85
    # assert exchange.get_annotation('rating') == 4.5
    # assert exchange.get_annotation('details')['category'] == 'important'
    
    # # Test that we can still get everything as tags
    # all_tags = exchange.tags
    # tag_names = [tag.name for tag in all_tags]
    # assert 'modern_flag' in tag_names
    # assert 'legacy_flag' in tag_names
    # assert 'score' in tag_names
    # assert 'rating' in tag_names


def test_rule_return_value_handling():
    """Test that different rule return value types are handled correctly."""
    tagger = ExchangeTagger()
    
    def bool_rule(exchange):
        return True
    
    def string_rule(exchange):
        return "detected"
    
    def number_rule(exchange):
        return 42
    
    def dict_rule(exchange):
        return {
            'count': 3,
            'type': 'test',
            'valid': True
        }
    
    # def legacy_tag_rule(exchange):
    #     return Tag('legacy', style='old', version=1.0)
    
    def false_rule(exchange):
        return False
    
    def none_rule(exchange):
        return None
    
    tagger.add_rule('bool_test', bool_rule)
    tagger.add_rule('string_test', string_rule)
    tagger.add_rule('number_test', number_rule)
    tagger.add_rule('dict_test', dict_rule)
    # tagger.add_rule('legacy_test', legacy_tag_rule)
    tagger.add_rule('false_test', false_rule)
    tagger.add_rule('none_test', none_rule)
    
    exchange = Exchange.create('test', [
        MessageOpenAI({'author': {'role': 'user'}, 'content': {'text': 'test'}, 'create_time': 1000})
    ])
    
    tagged = tagger.tag_exchange(exchange)
    
    # Check that different return types are handled correctly
    assert tagged.get_annotation('bool_test') is True
    assert tagged.get_annotation('string_test') == "detected"
    assert tagged.get_annotation('number_test') == 42
    
    # Dict rule should create multiple annotations
    assert tagged.get_annotation('count') == 3
    assert tagged.get_annotation('type') == 'test'
    assert tagged.get_annotation('valid') is True
    
    # Legacy tag should be converted
    # assert tagged.has_annotation('legacy')
    # legacy_data = tagged.get_annotation('legacy')
    # assert legacy_data['style'] == 'old'
    # assert legacy_data['version'] == 1.0
    
    # False and None should not create annotations
    assert not tagged.has_annotation('false_test')
    assert not tagged.has_annotation('none_test')



---
File: attic/tests/conversation_tagger/test_core.py
---
# tests/test_core.py
"""
Core functionality tests for conversation tagging system.
Updated to test both annotation system and backward compatibility.
"""

import pytest
from conversation_tagger.core.tag import Tag, create_annotation, merge_annotations
from conversation_tagger.core.exchange import Exchange
from conversation_tagger.core.conversation import Conversation
from conversation_tagger.core.message import MessageOpenAI

def test_annotation_helpers():
    """Test annotation helper functions."""
    # Simple annotation
    simple = create_annotation('has_code', True)
    assert simple == {'has_code': True}
    
    # Annotation with value
    valued = create_annotation('length', 150)
    assert valued == {'length': 150}
    
    # Annotation with structured data
    structured = create_annotation('stats', {'count': 5, 'avg': 2.5})
    assert structured == {'stats': {'count': 5, 'avg': 2.5}}
    
    # Merge annotations
    merged = merge_annotations(simple, valued, structured)
    assert merged == {'has_code': True, 'length': 150, 'stats': {'count': 5, 'avg': 2.5}}


def test_tag_backward_compatibility():
    """Test that Tag objects still work and convert properly."""
    # Simple tag
    simple_tag = Tag('simple')
    assert simple_tag.name == 'simple'
    assert simple_tag.attributes == {}
    assert simple_tag.to_dict() == {'simple': True}
    
    # Tag with single value attribute
    value_tag = Tag('length', value=100)
    assert value_tag.to_dict() == {'length': 100}
    
    # Tag with multiple attributes
    complex_tag = Tag('stats', count=5, avg=2.5)
    assert complex_tag.to_dict() == {'stats': {'count': 5, 'avg': 2.5}}
    
    # Test equality
    assert simple_tag == 'simple'
    assert simple_tag != complex_tag


def test_exchange_annotations():
    """Test exchange annotation functionality."""
    messages = [
        {'author': {'role': 'user'}, 'content': {'text': 'Hello'}, 'create_time': 1000},
        {'author': {'role': 'assistant'}, 'content': {'text': 'Hi!'}, 'create_time': 2000}
    ]
    messages = [MessageOpenAI(data=msg) for msg in messages]
    
    exchange = Exchange.create('conv_1', messages)
    
    # Test adding annotations
    exchange.add_annotation('has_greeting', True)
    exchange.add_annotation('message_count', 2)
    exchange.add_annotation('stats', {'user_msgs': 1, 'assistant_msgs': 1})
    
    assert exchange.has_annotation('has_greeting')
    assert exchange.get_annotation('message_count') == 2
    assert exchange.get_annotation('stats')['user_msgs'] == 1
    assert not exchange.has_annotation('missing_annotation')
    assert exchange.get_annotation('missing_annotation', 'default') == 'default'
    
    # # Test backward compatibility with tags property
    # tags = exchange.tags
    # assert len(tags) == 3
    # tag_names = [tag.name for tag in tags]
    # assert 'has_greeting' in tag_names
    # assert 'message_count' in tag_names
    # assert 'stats' in tag_names


# def test_exchange_tags_compatibility():
#     """Test that setting tags still works via backward compatibility."""
#     exchange = Exchange.create('conv_1', [])
    
#     # Set tags the old way
#     old_tags = [
#         Tag('simple'),
#         Tag('valued', value=42),
#         Tag('complex', count=3, type='test')
#     ]
#     exchange.tags = old_tags
    
#     # Should be converted to annotations
#     assert exchange.has_annotation('simple')
#     assert exchange.get_annotation('simple') is True
#     assert exchange.get_annotation('valued') == 42
#     assert exchange.get_annotation('complex') == {'count': 3, 'type': 'test'}


def test_exchange_merging_annotations():
    """Test merging exchanges preserves annotations."""
    messages1=[
        {'author': {'role': 'user'}, 'content': {'text': 'First'}, 'create_time': 1000},
        {'author': {'role': 'assistant'}, 'content': {'text': 'Response'}, 'create_time': 2000}
    ]
    messages1 = [MessageOpenAI(data=msg) for msg in messages1]
    exchange_1 = Exchange.create('conv_1', messages1)
    exchange_1.add_annotation('has_code', True)
    exchange_1.add_annotation('part', 1)

    messages2=[
        {'author': {'role': 'user'}, 'content': {'text': 'Continue'}, 'create_time': 3000},
        {'author': {'role': 'assistant'}, 'content': {'text': 'More'}, 'create_time': 4000}
    ]
    messages2 = [MessageOpenAI(data=msg) for msg in messages2]
    exchange_2 = Exchange.create('conv_1', messages2)
    exchange_2.add_annotation('has_continuation', True)
    exchange_2.add_annotation('part', 2)
    
    merged = exchange_1 + exchange_2
    
    assert len(merged) == 4
    assert merged.has_annotation('has_code')
    assert merged.has_annotation('has_continuation')
    assert merged.get_annotation('part') == 2  # Second exchange value wins
    
    # Verify time ordering
    times = [msg.created_date for msg in merged.messages]
    assert times == [1000, 2000, 3000, 4000]


def test_conversation_annotations():
    """Test conversation annotation functionality."""
    exchanges = [
        Exchange.create('conv_1', [
            MessageOpenAI(data={'author': {'role': 'user'}, 'content': {'text': 'Q1'}, 'create_time': 1000}),
            MessageOpenAI(data={'author': {'role': 'assistant'}, 'content': {'text': 'A1'}, 'create_time': 2000})
        ]),
        Exchange.create('conv_1', [
            MessageOpenAI(data={'author': {'role': 'user'}, 'content': {'text': 'Q2'}, 'create_time': 3000}),
            MessageOpenAI(data={'author': {'role': 'assistant'}, 'content': {'text': 'A2'}, 'create_time': 4000})
        ])  
    ]
    
    # Add annotations to exchanges
    exchanges[0].add_annotation('has_greeting', True)
    exchanges[1].add_annotation('has_code', True)
    
    conv = Conversation('conv_1', 'Test Chat', exchanges)
    
    # Test conversation-level annotations
    conv.add_annotation('is_technical', True)
    conv.add_annotation('complexity', 'medium')
    
    assert conv.has_annotation('is_technical')
    assert conv.get_annotation('complexity') == 'medium'
    
    # Test aggregated annotations from exchanges
    assert conv.has_annotation('has_greeting')
    assert conv.has_annotation('has_code')
    
    # Test properties still work
    assert conv.exchange_count == 2
    assert conv.total_message_count == 4
    assert 'Q1' in conv.get_all_user_text()


# def test_conversation_tags_compatibility():
#     """Test conversation backward compatibility with tags."""
#     conv = Conversation('conv_1', 'Test', [])
    
#     # Set tags the old way
#     old_tags = [
#         Tag('multi_turn'),
#         Tag('length', category='medium', count=5)
#     ]
#     conv.tags = old_tags
    
#     # Should be converted to annotations
#     assert conv.has_annotation('multi_turn')
#     assert conv.get_annotation('multi_turn') is True
#     assert conv.get_annotation('length') == {'category': 'medium', 'count': 5}
    
#     # Test getting tags back
#     tags = conv.tags
#     tag_names = [tag.name for tag in tags]
#     assert 'multi_turn' in tag_names
#     assert 'length' in tag_names


@pytest.fixture
def sample_conversation_data():
    """Sample conversation data for parsing tests."""
    return {
        'conversation_id': 'test_conv',
        'title': 'Test Chat',
        'mapping': {
            'msg1': {
                'message': {
                    'author': {'role': 'user'},
                    'create_time': 1000,
                    'content': {'text': 'Hello'}
                }
            },
            'msg2': {
                'message': {
                    'author': {'role': 'assistant'},
                    'create_time': 2000,
                    'content': {'text': 'Hi there!'}
                }
            }
        }
    }


def test_simple_parsing(sample_conversation_data):
    """Test basic conversation parsing with annotations."""
    from conversation_tagger.core.exchange_parser import ExchangeParserOAI

    parser = ExchangeParserOAI()
    conversation = parser.parse_conversation(sample_conversation_data)
    
    assert isinstance(conversation, Conversation)
    assert conversation.conversation_id == 'test_conv'
    assert conversation.exchange_count == 1
    assert 'Hello' in conversation.get_all_user_text()
    
    # Should initially only have source annotation.
    assert len(conversation.annotations) == 1
    assert conversation.get_annotation('source') == 'oai'

    # Test adding annotations
    conversation.add_annotation('parsed', True)
    assert conversation.has_annotation('parsed')



---
File: attic/tests/conversation_tagger/test_detection.py
---
# tests/conversation_tagger/test_detection.py
"""
Unit tests for detection.py detection functions.
Updated to test annotation-based system.
"""

import pytest
from conversation_tagger.core.detection import *
from conversation_tagger.core.exchange import Exchange
from conversation_tagger.core.conversation import Conversation
from conversation_tagger.core.tag import Tag

from conversation_tagger.core.message import MessageOpenAI

######################
# Conversation Tests #
######################

def test_create_conversation_length_annotation():
    """Test conversation length annotation creation."""
    # Single exchange
    conv_single = Conversation('test', 'Single', [Exchange.create('test', [])])
    annotation = create_conversation_length_annotation(conv_single)
    assert 'conversation_length' in annotation
    length_data = annotation['conversation_length']
    assert length_data['count'] == 1
    assert length_data['category'] == 'single'
    
    # Short (3 exchanges)
    exchanges = [Exchange.create('test', []) for _ in range(3)]
    conv_short = Conversation('test', 'Short', exchanges)
    annotation = create_conversation_length_annotation(conv_short)
    length_data = annotation['conversation_length']
    assert length_data['count'] == 3
    assert length_data['category'] == 'short'
    
    # Medium (7 exchanges)
    exchanges = [Exchange.create('test', []) for _ in range(7)]
    conv_medium = Conversation('test', 'Medium', exchanges)
    annotation = create_conversation_length_annotation(conv_medium)
    length_data = annotation['conversation_length']
    assert length_data['count'] == 7
    assert length_data['category'] == 'medium'
    
    # Long (15 exchanges)
    exchanges = [Exchange.create('test', []) for _ in range(15)]
    conv_long = Conversation('test', 'Long', exchanges)
    annotation = create_conversation_length_annotation(conv_long)
    length_data = annotation['conversation_length']
    assert length_data['count'] == 15
    assert length_data['category'] == 'long'
    
    # Very long (30 exchanges)
    exchanges = [Exchange.create('test', []) for _ in range(30)]
    conv_very_long = Conversation('test', 'VeryLong', exchanges)
    annotation = create_conversation_length_annotation(conv_very_long)
    length_data = annotation['conversation_length']
    assert length_data['count'] == 30
    assert length_data['category'] == 'very_long'


def test_conversation_feature_summary():
    """Test feature aggregation across exchanges."""
    # Create exchanges with different feature annotations
    exchange1 = Exchange.create('test', [])
    exchange1.add_annotation('has_code_blocks', True)
    exchange1.add_annotation('has_web_search', True)
    exchange1.add_annotation('gizmo_1', {'gizmo_id': 'gpt-4'})
    
    exchange2 = Exchange.create('test', [])
    exchange2.add_annotation('has_code_blocks', True)
    exchange2.add_annotation('has_github_repos', True)
    exchange2.add_annotation('plugin_1', {'plugin_id': 'web'})
    
    exchange3 = Exchange.create('test', [])
    exchange3.add_annotation('has_latex_math', True)
    
    conv = Conversation('test', 'Test', [exchange1, exchange2, exchange3])
    annotations = conversation_feature_summary(conv)
    
    # has_code_blocks appears in 2/3 exchanges
    assert 'conversation_has_code_blocks' in annotations
    code_data = annotations['conversation_has_code_blocks']
    assert code_data['exchange_count'] == 2
    assert code_data['total_exchanges'] == 3
    assert code_data['percentage'] == 66.7
    
    assert 'conversation_has_web_search' in annotations
    search_data = annotations['conversation_has_web_search']
    assert search_data['exchange_count'] == 1
    assert search_data['percentage'] == 33.3
    
    assert 'conversation_has_gizmo_usage' in annotations
    gizmo_data = annotations['conversation_has_gizmo_usage']
    assert gizmo_data['exchange_count'] == 1
    assert gizmo_data['percentage'] == 33.3
    
    assert 'conversation_has_plugin_usage' in annotations
    plugin_data = annotations['conversation_has_plugin_usage']
    assert plugin_data['exchange_count'] == 1
    assert plugin_data['percentage'] == 33.3

    assert 'conversation_has_latex_math' in annotations
    
    # Empty conversation should return empty dict
    empty_conv = Conversation('test', 'Empty', [])
    empty_annotations = conversation_feature_summary(empty_conv)
    assert empty_annotations == {}


def test_conversation_gizmo_plugin_summary():
    """Test gizmo/plugin aggregation across exchanges."""
    # Create exchanges with gizmo/plugin annotations
    exchange1 = Exchange.create('test', [])
    exchange1.add_annotation('gizmo_1', {'gizmo_id': 'gpt-4'})
    exchange1.add_annotation('plugin_1', {'plugin_id': 'web_browser'})
    
    exchange2 = Exchange.create('test', [])
    exchange2.add_annotation('gizmo_2', {'gizmo_id': 'gpt-4'})  # Same gizmo again
    exchange2.add_annotation('plugin_2', {'plugin_id': 'python'})
    
    exchange3 = Exchange.create('test', [])
    exchange3.add_annotation('gizmo_3', {'gizmo_id': 'dalle'})
    
    conv = Conversation('test', 'Test', [exchange1, exchange2, exchange3])
    annotations = conversation_gizmo_plugin_summary(conv)
    
    # Should have gizmo and plugin summary annotations
    assert len(annotations) == 2
    
    gizmo_data = annotations['conversation_gizmo_usage']
    assert gizmo_data['unique_gizmos'] == 2  # gpt-4, dalle
    assert gizmo_data['total_usage'] == 3   # gpt-4 used twice
    assert set(gizmo_data['gizmo_list']) == {'gpt-4', 'dalle'}
    
    plugin_data = annotations['conversation_plugin_usage']
    assert plugin_data['unique_plugins'] == 2  # web_browser, python
    assert plugin_data['total_usage'] == 2
    assert set(plugin_data['plugin_list']) == {'web_browser', 'python'}
    
    # No gizmo/plugin usage should return empty dict
    empty_exchange = Exchange.create('test', [])
    empty_conv = Conversation('test', 'Empty', [empty_exchange])
    empty_annotations = conversation_gizmo_plugin_summary(empty_conv)
    assert empty_annotations == {}


######################
#  Exchange Tests    #
######################

def test_has_github_repos():
    """Test GitHub repository detection."""
    # Exchange with GitHub repos
    msg_with_repos = MessageOpenAI(data={
        'author': {'role': 'user'},
        'metadata': {'selected_github_repos': ['owner/repo1', 'owner/repo2']},
        'content': {'text': 'Help with code'},
        'create_time': 1700000000.0
    })
    exchange_with = Exchange.create('test', [msg_with_repos])
    assert has_github_repos(exchange_with) == True
    
    # Exchange without GitHub repos
    msg_without_repos = MessageOpenAI(data={
        'author': {'role': 'user'},
        'metadata': {'selected_github_repos': []},
        'content': {'text': 'General question'},
        'create_time': 1700000000.0
    })
    exchange_without = Exchange.create('test', [msg_without_repos])
    assert has_github_repos(exchange_without) == False


def test_get_gizmo_annotations():
    """Test gizmo annotation generation."""
    # Exchange with single gizmo
    msg_with_gizmo = MessageOpenAI(data={
        'author': {'role': 'assistant'},
        'metadata': {'gizmo_id': 'gpt-4-turbo'},
        'content': {'text': 'Response from specialized model'},
        'create_time': 1700000000.0
    })
    exchange_single = Exchange.create('test', [msg_with_gizmo])
    annotations = get_gizmo_annotations(exchange_single)
    assert len(annotations) == 1
    assert 'gizmo_1' in annotations
    assert annotations['gizmo_1']['gizmo_id'] == 'gpt-4-turbo'
    
    # Exchange with multiple messages using different gizmos
    msg_gizmo1 = MessageOpenAI(data={
        'author': {'role': 'assistant'},
        'metadata': {'gizmo_id': 'gpt-4'},
        'content': {'text': 'First response'},
        'create_time': 1700000000.0
    })
    msg_gizmo2 = MessageOpenAI(data={
        'author': {'role': 'assistant'},
        'metadata': {'gizmo_id': 'dalle'},
        'content': {'text': 'Second response'},
        'create_time': 1700000000.0
    })
    exchange_multiple = Exchange.create('test', [msg_gizmo1, msg_gizmo2])
    annotations = get_gizmo_annotations(exchange_multiple)
    assert len(annotations) == 2
    gizmo_ids = {data['gizmo_id'] for data in annotations.values()}
    assert gizmo_ids == {'gpt-4', 'dalle'}
    
    # Exchange without gizmo usage
    msg_no_gizmo = MessageOpenAI(data={
        'author': {'role': 'assistant'},
        'metadata': {},
        'content': {'text': 'Regular response'},
        'create_time': 1700000000.0
    })
    exchange_none = Exchange.create('test', [msg_no_gizmo])
    annotations = get_gizmo_annotations(exchange_none)
    assert annotations == {}


def test_get_plugin_annotations():
    """Test plugin annotation generation."""
    # Exchange with plugin_id only
    msg_plugin_id = MessageOpenAI(data={
        'author': {'role': 'assistant'},
        'metadata': {'invoked_plugin': {'plugin_id': 'web_browser'}},
        'content': {'text': 'Searching web'},
        'create_time': 1700000000.0
    })
    exchange_plugin_id = Exchange.create('test', [msg_plugin_id])
    annotations = get_plugin_annotations(exchange_plugin_id)
    assert len(annotations) == 1
    assert 'plugin_1' in annotations
    assert annotations['plugin_1']['plugin_id'] == 'web_browser'
    
    # Exchange with both plugin_id and namespace
    msg_both = MessageOpenAI(data={
        'author': {'role': 'assistant'},
        'metadata': {'invoked_plugin': {'plugin_id': 'image_gen', 'namespace': 'dalle'}},
        'content': {'text': 'Generating image'},
        'create_time': 1700000000.0
    })
    exchange_both = Exchange.create('test', [msg_both])
    annotations = get_plugin_annotations(exchange_both)
    assert len(annotations) == 2
    plugin_ids = {data['plugin_id'] for data in annotations.values()}
    assert plugin_ids == {'image_gen', 'dalle'}
    
    # Exchange without plugin usage
    msg_none = MessageOpenAI(data={
        'author': {'role': 'assistant'},
        'metadata': {},
        'content': {'text': 'Regular response'},
        'create_time': 1700000000.0
    })
    exchange_none = Exchange.create('test', [msg_none])
    annotations = get_plugin_annotations(exchange_none)
    assert annotations == {}


def test_has_code_blocks():
    """Test code block detection."""
    # Exchange with code blocks
    user_msg = MessageOpenAI(data={'author': {'role': 'user'}, 'content': {'text': 'Fix this: ```python\nprint("hello")\n```'}, 'create_time': 1700000000.0})
    assistant_msg = MessageOpenAI(data={'author': {'role': 'assistant'}, 'content': {'text': 'Here is the fix: ```python\nprint("Hello!")\n```'}, 'create_time': 1700000000.0})
    exchange_with = Exchange.create('test', [user_msg, assistant_msg])
    assert has_code_blocks(exchange_with) == True
    
    # Exchange without code blocks
    user_msg_no_code = MessageOpenAI(data={'author': {'role': 'user'}, 'content': {'text': 'What is Python?'}, 'create_time': 1700000000.0})
    assistant_msg_no_code = MessageOpenAI(data={'author': {'role': 'assistant'}, 'content': {'text': 'Python is a programming language'}, 'create_time': 1700000000.0}) 
    exchange_without = Exchange.create('test', [user_msg_no_code, assistant_msg_no_code])
    assert has_code_blocks(exchange_without) == False


def test_has_latex_math():
    """Test LaTeX math detection."""
    # Block math
    msg_block_math = MessageOpenAI(data={
        'author': {'role': 'assistant'},
        'content': {'text': 'The quadratic formula is: $$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$'},
        'create_time': 1700000000.0
    })
    exchange_block = Exchange.create('test', [msg_block_math])
    assert has_latex_math(exchange_block) == True
    
    # LaTeX commands
    msg_latex_commands = MessageOpenAI(data={
        'author': {'role': 'assistant'},
        'content': {'text': 'The integral \\int_{0}^{\\infty} e^{-x} dx = 1'},
        'create_time': 1700000000.0
    })
    exchange_commands = Exchange.create('test', [msg_latex_commands])
    assert has_latex_math(exchange_commands) == True
    
    # No math
    msg_no_math = MessageOpenAI(data={
        'author': {'role': 'assistant'},
        'content': {'text': 'This is regular text without any mathematical notation'},
        'create_time': 1700000000.0
    })
    exchange_no_math = Exchange.create('test', [msg_no_math])
    assert has_latex_math(exchange_no_math) == False


def test_first_user_has_large_content():
    """Test large content detection in first user message."""
    # Large content (over 2000 chars)
    large_text = 'x' * 2500
    msg_large = MessageOpenAI(data={
        'author': {'role': 'user'},
        'content': {'text': large_text},
        'create_time': 1700000000.0
    })
    exchange_large = Exchange.create('test', [msg_large])
    assert first_user_has_large_content(exchange_large) == True
    
    # Small content
    msg_small = MessageOpenAI(data={
        'author': {'role': 'user'},
        'content': {'text': 'Short question'},
        'create_time': 1700000000.0
    })
    exchange_small = Exchange.create('test', [msg_small])
    assert first_user_has_large_content(exchange_small) == False
    
    # Custom threshold
    medium_text = 'x' * 1500
    msg_medium = MessageOpenAI(data={
        'author': {'role': 'user'},
        'content': {'text': medium_text},
        'create_time': 1700000000.0
    })
    exchange_medium = Exchange.create('test', [msg_medium])
    assert first_user_has_large_content(exchange_medium, min_length=1000) == True
    assert first_user_has_large_content(exchange_medium, min_length=2000) == False


def test_user_has_attachments():
    """Test user attachment detection."""
    # User with attachments 
    msg_with_attachments = MessageOpenAI(data={
        'author': {'role': 'user'},
        'metadata': {'attachments': [{'id': 'file1', 'name': 'document.pdf'}]},
        'content': {'text': 'Please analyze this file'},
        'create_time': 1700000000.0
    })
    exchange_with = Exchange.create('test', [msg_with_attachments])
    assert user_has_attachments(exchange_with) == True
    
    # User without attachments
    msg_without_attachments = MessageOpenAI(data={
        'author': {'role': 'user'},
        'metadata': {'attachments': []},
        'content': {'text': 'General question'},
        'create_time': 1700000000.0
    })  
    exchange_without = Exchange.create('test', [msg_without_attachments])
    assert user_has_attachments(exchange_without) == False


def test_extract_proposed_title():
    """Test proposed title extraction from assistant messages."""
    # Test markdown header title (single #)
    msg_markdown_h1 = MessageOpenAI(data={
        'author': {'role': 'assistant'},
        'content': {'text': '# Introduction to Python\n\nPython is a programming language...'},
        'create_time': 1700000000.0
    })
    exchange_h1 = Exchange.create('test', [msg_markdown_h1])
    title_h1 = extract_proposed_title(exchange_h1)
    assert title_h1 == 'Introduction to Python'
    
    # Test bold title
    msg_bold_title = MessageOpenAI(data={
        'author': {'role': 'assistant'},
        'content': {'text': '**Machine Learning Basics**\n\nMachine learning is...'},
        'create_time': 1700000000.0
    })
    exchange_bold = Exchange.create('test', [msg_bold_title])
    title_bold = extract_proposed_title(exchange_bold)
    assert title_bold == 'Machine Learning Basics'
    
    # Test no title format (regular text)
    msg_no_title = MessageOpenAI(data={
        'author': {'role': 'assistant'},
        'content': {'text': 'This is just regular text without any title formatting.'},
        'create_time': 1700000000.0
    })
    exchange_no_title = Exchange.create('test', [msg_no_title])
    title_none = extract_proposed_title(exchange_no_title)
    assert title_none is None


def test_naive_title_extraction():
    """Test the helper function directly."""
    # Test markdown headers
    assert naive_title_extraction('# Simple Title') == 'Simple Title'
    assert naive_title_extraction('## Header Level 2') == 'Header Level 2'
    
    # Test bold titles
    assert naive_title_extraction('**Bold Title**') == 'Bold Title'
    
    # Test no title formats
    assert naive_title_extraction('Regular text') is None
    assert naive_title_extraction('Not a title format') is None
    
    # Test with whitespace
    assert naive_title_extraction('  # Title with spaces  ') == 'Title with spaces'



---
File: attic/tests/conversation_tagger/test_integration.py
---
# tests/test_integration.py
"""
Integration tests for the complete conversation tagging system.
Updated to test annotation-based system.
"""

import pytest
from conversation_tagger import create_default_tagger, ConversationTagger
from conversation_tagger.core.exchange import Exchange

@pytest.fixture
def sample_coding_conversation():
    """A realistic conversation about coding that should trigger multiple annotations."""
    return {
        'conversation_id': 'coding_conv',
        'title': 'Python help session',
        'mapping': {
            'msg1': {
                'message': {
                    'author': {'role': 'user'},
                    'create_time': 1000,
                    'content': {'text': 'Can you help me write a Python function to calculate fibonacci numbers?'}
                }
            },
            'msg2': {
                'message': {
                    'author': {'role': 'assistant'},
                    'create_time': 2000,
                    'content': {'text': 'Sure! Here\'s a simple fibonacci function:\n\n```python\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n```'}
                }
            },
            'msg3': {
                'message': {
                    'author': {'role': 'user'},
                    'create_time': 3000,
                    'content': {'text': 'Can you make it more efficient?'}
                }
            },
            'msg4': {
                'message': {
                    'author': {'role': 'assistant'},
                    'create_time': 4000,
                    'content': {'text': 'Yes, here\'s a dynamic programming version:\n\n```python\ndef fibonacci_dp(n):\n    if n <= 1:\n        return n\n    \n    dp = [0, 1]\n    for i in range(2, n + 1):\n        dp.append(dp[i-1] + dp[i-2])\n    \n    return dp[n]\n```'}
                }
            }
        }
    }


def test_default_tagger_creation():
    """Test that default tagger is created with expected rules."""
    tagger = create_default_tagger()
    
    assert isinstance(tagger, ConversationTagger)
    assert len(tagger.exchange_parser.exchange_tagger.rules) > 0
    
    # Should have some default exchange rules
    rule_names = list(tagger.exchange_parser.exchange_tagger.rules.keys())
    assert 'has_wiki_links' in rule_names


def test_end_to_end_tagging_with_annotations(sample_coding_conversation):
    """Test complete tagging pipeline with realistic conversation."""
    tagger = create_default_tagger()
    
    # Add a custom rule for testing
    def mentions_python(exchange):
        text = (' '.join(exchange.get_user_texts()) + ' ' + ' '.join(exchange.get_assistant_texts())).lower()
        return 'python' in text
    
    def count_code_blocks(exchange):
        """Return annotation with count of code blocks."""
        all_text = ' '.join(exchange.get_user_texts() + exchange.get_assistant_texts())
        count = all_text.count('```')
        if count > 0:
            return {'code_block_markers': count, 'has_code_blocks': True}
        return False
    
    tagger.add_exchange_rule('mentions_python', mentions_python)
    tagger.add_exchange_rule('code_analysis', count_code_blocks)
    
    result = tagger.tag_conversation(sample_coding_conversation)
    
    # Basic structure checks
    assert result.conversation_id == 'coding_conv'
    assert result.exchange_count == 2  # Two separate exchanges
    assert result.total_message_count == 4
    
    # Check that our custom rules fired
    all_annotations = {}
    for exchange in result.exchanges:
        all_annotations.update(exchange.annotations)
    
    assert 'mentions_python' in all_annotations
    assert all_annotations['mentions_python'] is True
    
    # Should detect code blocks
    assert 'has_code_blocks' in all_annotations or any(
        exchange.has_annotation('has_code_blocks') for exchange in result.exchanges
    )



def test_conversation_with_attachments():
    """Test conversation that includes file attachments."""
    conversation_with_file = {
        'conversation_id': 'file_conv',
        'title': 'File analysis',
        'mapping': {
            'msg1': {
                'message': {
                    'author': {'role': 'user'},
                    'create_time': 1000,
                    'content': {'text': 'Can you analyze this Python file?'},
                    'metadata': {
                        'attachments': [
                            {'id': 'file1', 'name': 'script.py', 'mime_type': 'text/x-python'}
                        ]
                    }
                }
            },
            'msg2': {
                'message': {
                    'author': {'role': 'assistant'},
                    'create_time': 2000,
                    'content': {'text': 'I can help analyze your Python script...'}
                }
            }
        }
    }
    
    tagger = create_default_tagger()
    result = tagger.tag_conversation(conversation_with_file)
    
    # Should detect attachment-related annotations
    all_annotations = {}
    for exchange in result.exchanges:
        all_annotations.update(exchange.annotations)
    
    assert 'first_user_has_attachments' in all_annotations
    assert 'first_user_has_code_attachments' in all_annotations
    
    # Test values
    assert all_annotations['first_user_has_attachments'] is True
    assert all_annotations['first_user_has_code_attachments'] is True


def test_math_conversation():
    """Test conversation with mathematical content."""
    math_conversation = {
        'conversation_id': 'math_conv',
        'title': 'Math help',
        'mapping': {
            'msg1': {
                'message': {
                    'author': {'role': 'user'},
                    'create_time': 1000,
                    'content': {'text': 'Explain the quadratic formula'}
                }
            },
            'msg2': {
                'message': {
                    'author': {'role': 'assistant'},
                    'create_time': 2000,
                    'content': {'text': 'The quadratic formula is: $$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$'}
                }
            }
        }
    }
    
    tagger = create_default_tagger()
    result = tagger.tag_conversation(math_conversation)
    
    # Should detect LaTeX math
    exchange = result.exchanges[0]
    assert exchange.has_annotation('has_latex_math')
    assert exchange.get_annotation('has_latex_math') is True


def test_large_content_detection():
    """Test detection of large content messages."""
    large_content = 'x' * 2500  # Over the threshold
    
    large_message_conversation = {
        'conversation_id': 'large_conv',
        'title': 'Large content',
        'mapping': {
            'msg1': {
                'message': {
                    'author': {'role': 'user'},
                    'create_time': 1000,
                    'content': {'text': large_content}
                }
            },
            'msg2': {
                'message': {
                    'author': {'role': 'assistant'},
                    'create_time': 2000,
                    'content': {'text': 'That\'s a lot of content!'}
                }
            }
        }
    }
    
    tagger = create_default_tagger()
    result = tagger.tag_conversation(large_message_conversation)
    
    exchange = result.exchanges[0]
    assert exchange.has_annotation('first_user_has_large_content')
    assert exchange.get_annotation('first_user_has_large_content') is True


def test_conversation_level_annotations():
    """Test conversation-level annotation aggregation."""
    tagger = create_default_tagger()
    
    # Multi-exchange conversation
    conversation_data = {
        'conversation_id': 'multi_conv',
        'title': 'Multi-exchange test',
        'mapping': {
            'msg1': {'message': {'author': {'role': 'user'}, 'create_time': 1000, 'content': {'text': 'First question'}}},
            'msg2': {'message': {'author': {'role': 'assistant'}, 'create_time': 2000, 'content': {'text': 'First answer'}}},
            'msg3': {'message': {'author': {'role': 'user'}, 'create_time': 3000, 'content': {'text': 'Second question'}}},
            'msg4': {'message': {'author': {'role': 'assistant'}, 'create_time': 4000, 'content': {'text': 'Second answer'}}},
            'msg5': {'message': {'author': {'role': 'user'}, 'create_time': 5000, 'content': {'text': 'Third question'}}},
            'msg6': {'message': {'author': {'role': 'assistant'}, 'create_time': 6000, 'content': {'text': 'Third answer'}}}
        }
    }
    
    result = tagger.tag_conversation(conversation_data)
    
    # Should have conversation-level length annotation
    assert result.has_annotation('conversation_length')
    length_data = result.get_annotation('conversation_length')
    assert length_data['count'] == 3
    assert length_data['category'] == 'short'  # 3 exchanges = short


def test_gizmo_plugin_annotations():
    """Test gizmo and plugin annotation detection."""
    gizmo_conversation = {
        'conversation_id': 'gizmo_conv',
        'title': 'Gizmo usage',
        'mapping': {
            'msg1': {
                'message': {
                    'author': {'role': 'user'},
                    'create_time': 1000,
                    'content': {'text': 'Generate an image'}
                }
            },
            'msg2': {
                'message': {
                    'author': {'role': 'assistant'},
                    'create_time': 2000,
                    'content': {'text': 'I\'ll generate that for you'},
                    'metadata': {'gizmo_id': 'dalle-3'}
                }
            }
        }
    }
    
    tagger = create_default_tagger()
    result = tagger.tag_conversation(gizmo_conversation)
    
    # Should detect gizmo usage at exchange level
    exchange = result.exchanges[0]
    
    # Check for gizmo annotations (could be gizmo_1, gizmo_2, etc.)
    gizmo_annotations = {k: v for k, v in exchange.annotations.items() if k.startswith('gizmo_')}
    assert len(gizmo_annotations) >= 1
    
    # At least one should have dalle-3 as gizmo_id
    found_dalle = False
    for annotation_value in gizmo_annotations.values():
        if isinstance(annotation_value, dict) and annotation_value.get('gizmo_id') == 'dalle-3':
            found_dalle = True
            break
    assert found_dalle


def test_empty_conversation_handling():
    """Test handling of edge cases like empty conversations."""
    empty_conversation = {
        'conversation_id': 'empty_conv',
        'title': 'Empty',
        'mapping': {}
    }
    
    tagger = create_default_tagger()
    result = tagger.tag_conversation(empty_conversation)
    
    assert result.conversation_id == 'empty_conv'
    assert result.exchange_count == 0
    assert result.total_message_count == 0
    assert len(result.annotations) >= 0  # May have some conversation-level annotations


# def test_annotation_vs_tag_consistency():
#     """Test that annotation and tag interfaces give consistent results."""
#     tagger = create_default_tagger()
    
#     # Add custom rule that returns complex data
#     def complex_analysis(exchange):
#         return {
#             'message_count': len(exchange.messages),
#             'user_word_count': len(' '.join(exchange.get_user_texts()).split()),
#             'assistant_word_count': len(' '.join(exchange.get_assistant_texts()).split())
#         }
    
#     tagger.add_exchange_rule('analysis', complex_analysis)
    
#     conversation_data = {
#         'conversation_id': 'test_conv',
#         'title': 'Test',
#         'mapping': {
#             'msg1': {'message': {'author': {'role': 'user'}, 'create_time': 1000, 'content': {'text': 'Hello world test'}}},
#             'msg2': {'message': {'author': {'role': 'assistant'}, 'create_time': 2000, 'content': {'text': 'Hi there friend'}}}
#         }
#     }
    
#     result = tagger.tag_conversation(conversation_data)
#     exchange = result.exchanges[0]
    
#     # Test annotation interface
#     assert exchange.has_annotation('message_count')
#     assert exchange.get_annotation('message_count') == 2
#     assert exchange.get_annotation('user_word_count') == 3
#     assert exchange.get_annotation('assistant_word_count') == 3
    
#     # Test tag interface (backward compatibility)
#     tags = exchange.tags
    
#     # Find the analysis-related tags
#     analysis_tags = [tag for tag in tags if 'message_count' in tag.name or 'word_count' in tag.name]
#     assert len(analysis_tags) >= 3  # Should have all three annotations as separate tags or one combined tag
    
#     # Test round-trip: annotations -> tags -> annotations
#     original_annotations = exchange.annotations.copy()
    
#     # Convert to tags and back
#     tag_list = exchange.tags
#     new_exchange = Exchange.create('test', [])
#     new_exchange.tags = tag_list
    
#     # Should preserve the key data (exact format may differ)
#     assert new_exchange.has_annotation('message_count')
#     assert new_exchange.get_annotation('message_count') == 2


def test_claude_conversation_parsing():
    """Test parsing a Claude conversation."""
    claude_conversation = {
        'uuid': 'test-uuid',
        'name': 'Test Claude Chat',
        'chat_messages': [
            {
                'uuid': 'msg1-uuid',
                'text': 'Hello Claude',
                'sender': 'user',
                'created_at': '2024-01-01T12:00:00Z',
                'content': [{'type': 'text', 'text': 'Hello Claude'}],
                'attachments': []
            },
            {
                'uuid': 'msg2-uuid', 
                'text': 'Hello! How can I help you today?',
                'sender': 'assistant',
                'created_at': '2024-01-01T12:00:01Z',
                'content': [{'type': 'text', 'text': 'Hello! How can I help you today?'}],
                'attachments': []
            }
        ]
    }
    
    tagger = create_default_tagger(source="claude")
    result = tagger.tag_conversation(claude_conversation)
    
    assert result.conversation_id == 'test-uuid'
    assert result.exchange_count == 1
    assert 'Hello Claude' in result.get_all_user_text()


---
File: attic/tests/conversation_tagger/test_message_ids.py
---
# tests/conversation_tagger/test_message_ids.py
"""
Tests for message ID functionality.
"""

import pytest
from conversation_tagger.core.message import MessageOpenAI, MessageClaude
from conversation_tagger.core.exchange import Exchange


def test_message_openai_id():
    """Test that OpenAI messages extract IDs correctly."""
    msg_data = {
        'id': 'msg-12345',
        'author': {'role': 'user'},
        'create_time': 1700000000.0,
        'content': {'text': 'Hello world'},
        'metadata': {}
    }
    
    message = MessageOpenAI(data=msg_data)
    assert message.id == 'msg-12345'
    
    # Test missing ID
    msg_data_no_id = {
        'author': {'role': 'user'},
        'create_time': 1700000000.0,
        'content': {'text': 'Hello world'},
        'metadata': {}
    }
    
    message_no_id = MessageOpenAI(data=msg_data_no_id)
    assert message_no_id.id is None


def test_message_claude_id():
    """Test that Claude messages extract IDs correctly."""
    msg_data = {
        'uuid': 'claude-msg-67890',
        'text': 'Hello world',
        'sender': 'user',
        'created_at': '2024-01-01T12:00:00Z',
        'updated_at': '2024-01-01T12:00:00Z',
        'content': [{'type': 'text', 'text': 'Hello world'}],
        'attachments': [],
        'files': []
    }
    
    message = MessageClaude(data=msg_data)
    assert message.id == 'claude-msg-67890'
    
    # Test missing UUID
    msg_data_no_uuid = {
        'text': 'Hello world',
        'sender': 'user',
        'created_at': '2024-01-01T12:00:00Z',
        'updated_at': '2024-01-01T12:00:00Z',
        'content': [{'type': 'text', 'text': 'Hello world'}],
        'attachments': [],
        'files': []
    }
    
    message_no_uuid = MessageClaude(data=msg_data_no_uuid)
    assert message_no_uuid.id is None


def test_exchange_get_message_ids():
    """Test that exchanges can return all message IDs."""
    # Create messages with IDs
    msg1_data = {
        'id': 'user-msg-1',
        'author': {'role': 'user'},
        'create_time': 1700000000.0,
        'content': {'text': 'Question'},
        'metadata': {}
    }
    
    msg2_data = {
        'id': 'assistant-msg-1',
        'author': {'role': 'assistant'},
        'create_time': 1700000001.0,
        'content': {'text': 'Answer'},
        'metadata': {}
    }
    
    msg3_data = {
        'id': 'user-msg-2',
        'author': {'role': 'user'},
        'create_time': 1700000002.0,
        'content': {'text': 'Follow up'},
        'metadata': {}
    }
    
    messages = [
        MessageOpenAI(data=msg1_data),
        MessageOpenAI(data=msg2_data),
        MessageOpenAI(data=msg3_data)
    ]
    
    exchange = Exchange.create('test_conv', messages)
    message_ids = exchange.get_message_ids()
    
    assert len(message_ids) == 3
    assert 'user-msg-1' in message_ids
    assert 'assistant-msg-1' in message_ids
    assert 'user-msg-2' in message_ids
    assert message_ids == ['user-msg-1', 'assistant-msg-1', 'user-msg-2']


def test_exchange_get_message_ids_with_missing_ids():
    """Test exchange message ID extraction with some missing IDs."""
    # Mix of messages with and without IDs
    msg1_data = {
        'id': 'msg-with-id',
        'author': {'role': 'user'},
        'create_time': 1700000000.0,
        'content': {'text': 'Question'},
        'metadata': {}
    }
    
    msg2_data = {
        # No ID field
        'author': {'role': 'assistant'},
        'create_time': 1700000001.0,
        'content': {'text': 'Answer'},
        'metadata': {}
    }
    
    messages = [
        MessageOpenAI(data=msg1_data),
        MessageOpenAI(data=msg2_data)
    ]
    
    exchange = Exchange.create('test_conv', messages)
    message_ids = exchange.get_message_ids()
    
    # Should only include messages with non-empty IDs
    assert len(message_ids) == 1
    assert 'msg-with-id' in message_ids


def test_exchange_get_message_ids_empty_exchange():
    """Test get_message_ids with empty exchange."""
    exchange = Exchange.create('test_conv', [])
    message_ids = exchange.get_message_ids()
    
    assert message_ids == []


def test_exchange_merging_preserves_message_ids():
    """Test that merging exchanges preserves all message IDs."""
    # First exchange
    msg1_data = {
        'id': 'msg-1',
        'author': {'role': 'user'},
        'create_time': 1700000000.0,
        'content': {'text': 'First question'},
        'metadata': {}
    }
    
    msg2_data = {
        'id': 'msg-2',
        'author': {'role': 'assistant'},
        'create_time': 1700000001.0,
        'content': {'text': 'First answer'},
        'metadata': {}
    }
    
    exchange1 = Exchange.create('test_conv', [
        MessageOpenAI(data=msg1_data),
        MessageOpenAI(data=msg2_data)
    ])
    
    # Second exchange
    msg3_data = {
        'id': 'msg-3',
        'author': {'role': 'user'},
        'create_time': 1700000002.0,
        'content': {'text': 'Continue'},
        'metadata': {}
    }
    
    msg4_data = {
        'id': 'msg-4',
        'author': {'role': 'assistant'},
        'create_time': 1700000003.0,
        'content': {'text': 'Continued answer'},
        'metadata': {}
    }
    
    exchange2 = Exchange.create('test_conv', [
        MessageOpenAI(data=msg3_data),
        MessageOpenAI(data=msg4_data)
    ])
    
    # Merge exchanges
    merged = exchange1 + exchange2
    message_ids = merged.get_message_ids()
    
    assert len(message_ids) == 4
    assert 'msg-1' in message_ids
    assert 'msg-2' in message_ids
    assert 'msg-3' in message_ids
    assert 'msg-4' in message_ids
    
    # Should be in chronological order
    assert message_ids == ['msg-1', 'msg-2', 'msg-3', 'msg-4']


def test_mixed_message_types_with_ids():
    """Test exchange with mixed OpenAI and Claude messages (hypothetical scenario)."""
    # This tests the interface consistency between message types
    oai_msg_data = {
        'id': 'oai-msg-1',
        'author': {'role': 'user'},
        'create_time': 1700000000.0,
        'content': {'text': 'OpenAI question'},
        'metadata': {}
    }
    
    claude_msg_data = {
        'uuid': 'claude-msg-1',
        'text': 'Claude response',
        'sender': 'assistant',
        'created_at': '2024-01-01T12:00:01Z',
        'updated_at': '2024-01-01T12:00:01Z',
        'content': [{'type': 'text', 'text': 'Claude response'}],
        'attachments': [],
        'files': []
    }
    
    # In practice, exchanges would typically have one message type,
    # but this tests interface consistency
    oai_message = MessageOpenAI(data=oai_msg_data)
    claude_message = MessageClaude(data=claude_msg_data)
    
    assert oai_message.id == 'oai-msg-1'
    assert claude_message.id == 'claude-msg-1'
    
    # Both implement the same id interface
    assert hasattr(oai_message, 'id')
    assert hasattr(claude_message, 'id')


# Add to existing test files as needed
def test_message_id_property_in_existing_tests():
    """Test that existing message objects now have id property."""
    # From existing test patterns
    simple_user_message = {
        'id': 'test-user-msg',
        'author': {'role': 'user'},
        'create_time': 1000,
        'content': {'text': 'Hello, how are you?'}
    }
    
    simple_assistant_message = {
        'id': 'test-assistant-msg', 
        'author': {'role': 'assistant'},
        'create_time': 2000,
        'content': {'text': 'I am doing well, thank you!'}
    }
    
    user_msg = MessageOpenAI(data=simple_user_message)
    assistant_msg = MessageOpenAI(data=simple_assistant_message)
    
    assert user_msg.id == 'test-user-msg'
    assert assistant_msg.id == 'test-assistant-msg'
    
    exchange = Exchange.create('test_conv', [user_msg, assistant_msg])
    message_ids = exchange.get_message_ids()
    
    assert message_ids == ['test-user-msg', 'test-assistant-msg']


---
File: attic/tests/conversation_tagger/test_parameterized.py
---
# tests/conversation_tagger/test_basic_working_parameterized.py
"""
Updated basic tests using streamlined parameterization approach.
"""

import pytest
from conversation_tagger import create_default_tagger
from conversation_tagger.core.exchange import Exchange
from conversation_tagger.core.exchange_tagger import ExchangeTagger
from conversation_tagger.core.tag import Tag, create_annotation, merge_annotations
from conversation_tagger.core.message import MessageOpenAI, MessageClaude


def get_simple_conversation_data():
    """Return simple conversation data for both sources."""
    oai_data = {
        'conversation_id': 'test_oai',
        'title': 'Test ChatGPT',
        'mapping': {
            'msg1': {
                'message': {
                    'author': {'role': 'user'},
                    'create_time': 1700000000.0,
                    'content': {'text': 'Hello world'},
                    'metadata': {}
                }
            },
            'msg2': {
                'message': {
                    'author': {'role': 'assistant'},
                    'create_time': 1700000001.0,
                    'content': {'text': 'Hi there!'},
                    'metadata': {}
                }
            }
        }
    }
    
    claude_data = {
        'uuid': 'test-claude',
        'name': 'Test Claude',
        'created_at': '2024-01-01T12:00:00Z',
        'updated_at': '2024-01-01T12:00:02Z',
        'account': {'uuid': 'account-uuid'},
        'chat_messages': [
            {
                'uuid': 'msg1-uuid',
                'text': 'Hello world',
                'sender': 'user',
                'created_at': '2024-01-01T12:00:00Z',
                'updated_at': '2024-01-01T12:00:00Z',
                'content': [{'type': 'text', 'text': 'Hello world'}],
                'attachments': [],
                'files': []
            },
            {
                'uuid': 'msg2-uuid',
                'text': 'Hi there!',
                'sender': 'assistant',
                'created_at': '2024-01-01T12:00:01Z',
                'updated_at': '2024-01-01T12:00:01Z',
                'content': [{'type': 'text', 'text': 'Hi there!'}],
                'attachments': [],
                'files': []
            }
        ]
    }
    
    return oai_data, claude_data


class TestBasicFunctionality:
    """Test basic functionality across both data sources."""
    
    def test_annotation_functionality(self):
        """Test that annotation helpers work correctly."""
        # Simple annotation
        simple = create_annotation('test_annotation', True)
        assert simple == {'test_annotation': True}
        
        # Valued annotation
        valued = create_annotation('count', 42)
        assert valued == {'count': 42}
        
        # Complex annotation
        complex_data = {'type': 'test', 'score': 0.95}
        complex_ann = create_annotation('analysis', complex_data)
        assert complex_ann == {'analysis': complex_data}
        
        # Merge annotations
        merged = merge_annotations(simple, valued, complex_ann)
        assert 'test_annotation' in merged
        assert 'count' in merged
        assert 'analysis' in merged
        assert merged['count'] == 42

    @pytest.mark.parametrize("source,data", [
        ("oai", get_simple_conversation_data()[0]),
        ("claude", get_simple_conversation_data()[1])
    ])
    def test_conversation_parsing_basic(self, source, data):
        """Test basic conversation parsing works for both sources."""
        tagger = create_default_tagger(source=source)
        result = tagger.tag_conversation(data)
        
        # Basic structure checks
        assert result.conversation_id in ['test_oai', 'test-claude']
        assert 'Test' in result.title
        assert result.exchange_count == 1
        assert result.total_message_count == 2
        
        # Text extraction works
        user_text = result.get_all_user_text()
        assistant_text = result.get_all_assistant_text()
        assert 'Hello world' in user_text
        assert 'Hi there!' in assistant_text

    @pytest.mark.parametrize("source,data", [
        ("oai", get_simple_conversation_data()[0]),
        ("claude", get_simple_conversation_data()[1])
    ])
    def test_exchange_annotations(self, source, data):
        """Test exchange annotation handling across sources."""
        tagger = create_default_tagger(source=source)
        
        # Add custom annotation rule
        def greeting_detector(exchange):
            user_texts = exchange.get_user_texts()
            if user_texts:
                text = ' '.join(user_texts).lower()
                if any(greeting in text for greeting in ['hello', 'hi', 'hey']):
                    return {
                        'has_greeting': True,
                        'greeting_type': 'informal' if 'hi' in text or 'hey' in text else 'formal'
                    }
            return False
        
        tagger.add_exchange_rule('greeting_analysis', greeting_detector)
        
        result = tagger.tag_conversation(data)
        exchange = result.exchanges[0]
        
        # Check that annotations were applied
        assert exchange.has_annotation('has_greeting')
        assert exchange.get_annotation('has_greeting') is True
        assert exchange.get_annotation('greeting_type') == 'formal'  # "hello" is formal

    @pytest.mark.parametrize("source", ["oai", "claude"])
    def test_default_tagger_creation(self, source):
        """Test that default tagger can be created for both sources."""
        tagger = create_default_tagger(source=source)
        assert tagger is not None
        assert hasattr(tagger, 'exchange_parser')
        assert hasattr(tagger.exchange_parser, 'exchange_tagger')
        
        # Should have some default rules
        assert len(tagger.exchange_parser.exchange_tagger.rules) > 0

    @pytest.mark.parametrize("source", ["oai", "claude"])
    def test_exchange_tagger_rule_handling(self, source):
        """Test exchange tagger rule handling across sources."""
        tagger = ExchangeTagger()
        
        def bool_rule(exchange):
            return True
        
        def dict_rule(exchange):
            return {
                'message_count': len(exchange.messages),
                'has_user': len(exchange.get_user_messages()) > 0
            }
        
        def false_rule(exchange):
            return False
        
        tagger.add_rule('bool_test', bool_rule)
        tagger.add_rule('dict_test', dict_rule)
        tagger.add_rule('false_test', false_rule)
        
        # Create message object based on source
        if source == "oai":
            message_data = {
                'author': {'role': 'user'},
                'create_time': 1700000000.0,
                'content': {'text': 'test'},
                'metadata': {}
            }
            message_obj = MessageOpenAI(data=message_data)
        else:
            message_data = {
                'uuid': 'test-uuid',
                'text': 'test',
                'sender': 'user',
                'created_at': '2024-01-01T12:00:00Z',
                'updated_at': '2024-01-01T12:00:00Z',
                'content': [{'type': 'text', 'text': 'test'}],
                'attachments': [],
                'files': []
            }
            message_obj = MessageClaude(data=message_data)
        
        exchange = Exchange.create('test', [message_obj])
        tagged = tagger.tag_exchange(exchange)
        
        # Check annotations
        assert tagged.get_annotation('bool_test') is True
        assert tagged.get_annotation('message_count') == 1
        assert tagged.get_annotation('has_user') is True
        assert not tagged.has_annotation('false_test')  # False shouldn't create annotation


class TestTextExtraction:
    """Test text extraction APIs work consistently across sources."""
    
    @pytest.mark.parametrize("source", ["oai", "claude"])
    def test_user_text_extraction(self, source):
        """Test user text extraction works for both sources."""
        if source == "oai":
            message_data = {
                'author': {'role': 'user'},
                'create_time': 1700000000.0,
                'content': {'text': 'This is a test message'},
                'metadata': {}
            }
            message_obj = MessageOpenAI(data=message_data)
        else:
            message_data = {
                'uuid': 'user-text-uuid',
                'text': 'This is a test message',
                'sender': 'user',
                'created_at': '2024-01-01T12:00:00Z',
                'updated_at': '2024-01-01T12:00:00Z',
                'content': [{'type': 'text', 'text': 'This is a test message'}],
                'attachments': [],
                'files': []
            }
            message_obj = MessageClaude(data=message_data)
        
        exchange = Exchange.create('test', [message_obj])
        user_texts = exchange.get_user_texts()
        
        assert isinstance(user_texts, list)
        assert len(user_texts) == 1
        assert 'This is a test message' in user_texts[0]

    @pytest.mark.parametrize("source", ["oai", "claude"])
    def test_assistant_text_extraction(self, source):
        """Test assistant text extraction works for both sources."""
        if source == "oai":
            message_data = {
                'author': {'role': 'assistant'},
                'create_time': 1700000000.0,
                'content': {'text': 'This is an assistant response'},
                'metadata': {}
            }
            message_obj = MessageOpenAI(data=message_data)
        else:
            message_data = {
                'uuid': 'assistant-text-uuid',
                'text': 'This is an assistant response',
                'sender': 'assistant',
                'created_at': '2024-01-01T12:00:00Z',
                'updated_at': '2024-01-01T12:00:00Z',
                'content': [{'type': 'text', 'text': 'This is an assistant response'}],
                'attachments': [],
                'files': []
            }
            message_obj = MessageClaude(data=message_data)
        
        exchange = Exchange.create('test', [message_obj])
        assistant_texts = exchange.get_assistant_texts()
        
        assert isinstance(assistant_texts, list)
        assert len(assistant_texts) == 1
        assert 'This is an assistant response' in assistant_texts[0]


class TestExchangeMerging:
    """Test exchange merging functionality works consistently."""
    
    @pytest.mark.parametrize("source", ["oai", "claude"])
    def test_exchange_merging_preserves_annotations(self, source):
        """Test that merging exchanges preserves annotations from both."""
        if source == "oai":
            msg1_data = {
                'author': {'role': 'user'},
                'create_time': 1700000000.0,
                'content': {'text': 'First'},
                'metadata': {}
            }
            msg2_data = {
                'author': {'role': 'assistant'},
                'create_time': 1700000001.0,
                'content': {'text': 'Response 1'},
                'metadata': {}
            }
            msg3_data = {
                'author': {'role': 'user'},
                'create_time': 1700000002.0,
                'content': {'text': 'Second'},
                'metadata': {}
            }
            msg4_data = {
                'author': {'role': 'assistant'},
                'create_time': 1700000003.0,
                'content': {'text': 'Response 2'},
                'metadata': {}
            }
            
            msg1_obj = MessageOpenAI(data=msg1_data)
            msg2_obj = MessageOpenAI(data=msg2_data)
            msg3_obj = MessageOpenAI(data=msg3_data)
            msg4_obj = MessageOpenAI(data=msg4_data)
        else:
            msg1_data = {
                'uuid': 'msg1-uuid',
                'text': 'First',
                'sender': 'user',
                'created_at': '2024-01-01T12:00:00Z',
                'updated_at': '2024-01-01T12:00:00Z',
                'content': [{'type': 'text', 'text': 'First'}],
                'attachments': [],
                'files': []
            }
            msg2_data = {
                'uuid': 'msg2-uuid',
                'text': 'Response 1',
                'sender': 'assistant',
                'created_at': '2024-01-01T12:00:01Z',
                'updated_at': '2024-01-01T12:00:01Z',
                'content': [{'type': 'text', 'text': 'Response 1'}],
                'attachments': [],
                'files': []
            }
            msg3_data = {
                'uuid': 'msg3-uuid',
                'text': 'Second',
                'sender': 'user',
                'created_at': '2024-01-01T12:00:02Z',
                'updated_at': '2024-01-01T12:00:02Z',
                'content': [{'type': 'text', 'text': 'Second'}],
                'attachments': [],
                'files': []
            }
            msg4_data = {
                'uuid': 'msg4-uuid',
                'text': 'Response 2',
                'sender': 'assistant',
                'created_at': '2024-01-01T12:00:03Z',
                'updated_at': '2024-01-01T12:00:03Z',
                'content': [{'type': 'text', 'text': 'Response 2'}],
                'attachments': [],
                'files': []
            }
            
            msg1_obj = MessageClaude(data=msg1_data)
            msg2_obj = MessageClaude(data=msg2_data)
            msg3_obj = MessageClaude(data=msg3_data)
            msg4_obj = MessageClaude(data=msg4_data)
        
        exchange1 = Exchange.create('test', [msg1_obj, msg2_obj])
        exchange1.add_annotation('first_exchange', True)
        exchange1.add_annotation('part', 1)
        
        exchange2 = Exchange.create('test', [msg3_obj, msg4_obj])
        exchange2.add_annotation('second_exchange', True)
        exchange2.add_annotation('part', 2)
        
        merged = exchange1 + exchange2
        
        # Check basic structure
        assert len(merged.messages) == 4
        
        # Check annotations were merged
        assert merged.has_annotation('first_exchange')
        assert merged.has_annotation('second_exchange')
        assert merged.get_annotation('part') == 2  # Second exchange wins
        
        # Check message ordering is preserved
        user_texts = merged.get_user_texts()
        assert 'First' in user_texts[0]
        assert 'Second' in user_texts[1]


# Test error handling
@pytest.mark.parametrize("source", ["oai", "claude"])
def test_empty_conversation_handling(source):
    """Test handling of empty conversations."""
    if source == "oai":
        empty_data = {
            'conversation_id': 'empty_oai',
            'title': 'Empty',
            'mapping': {}
        }
    else:
        empty_data = {
            'uuid': 'empty-claude',
            'name': 'Empty',
            'created_at': '2024-01-01T12:00:00Z',
            'updated_at': '2024-01-01T12:00:00Z',
            'account': {'uuid': 'account-uuid'},
            'chat_messages': []
        }
    
    tagger = create_default_tagger(source=source)
    result = tagger.tag_conversation(empty_data)
    
    assert result.exchange_count == 0
    assert result.total_message_count == 0


@pytest.mark.parametrize("source", ["oai", "claude"])
def test_rule_error_handling(source):
    """Test that broken rules don't crash the system."""
    tagger = create_default_tagger(source=source)
    
    def broken_rule(exchange):
        raise ValueError("This rule always fails")
    
    def working_rule(exchange):
        return True
    
    tagger.add_exchange_rule('broken', broken_rule)
    tagger.add_exchange_rule('working', working_rule)
    
    if source == "oai":
        data = {
            'conversation_id': 'test',
            'title': 'Test',
            'mapping': {
                'msg1': {
                    'message': {
                        'author': {'role': 'user'},
                        'create_time': 1700000000.0,
                        'content': {'text': 'Hello'},
                        'metadata': {}
                    }
                }
            }
        }
    else:
        data = {
            'uuid': 'test-uuid',
            'name': 'Test',
            'created_at': '2024-01-01T12:00:00Z',
            'updated_at': '2024-01-01T12:00:00Z',
            'account': {'uuid': 'account-uuid'},
            'chat_messages': [
                {
                    'uuid': 'msg1-uuid',
                    'text': 'Hello',
                    'sender': 'user',
                    'created_at': '2024-01-01T12:00:00Z',
                    'updated_at': '2024-01-01T12:00:00Z',
                    'content': [{'type': 'text', 'text': 'Hello'}],
                    'attachments': [],
                    'files': []
                }
            ]
        }
    
    # Should not raise exception
    result = tagger.tag_conversation(data)
    
    exchange = result.exchanges[0]
    # Working rule should apply, broken rule should be skipped
    assert exchange.has_annotation('working')
    assert not exchange.has_annotation('broken')


---
File: attic/tests/conversation_tagger/test_tagging.py
---
# tests/test_tagging.py
"""
Tagging functionality tests for exchanges and conversations.
Updated to test annotation-based system.
"""

import pytest
from conversation_tagger.core.exchange_tagger import ExchangeTagger
from conversation_tagger.core.tagger import ConversationTagger
from conversation_tagger.core.exchange import Exchange
from conversation_tagger.core.message import Message, MessageOpenAI
#from conversation_tagger.core.tag import Tag


def test_exchange_tagger_annotations():
    """Test basic exchange tagging with annotations."""
    tagger = ExchangeTagger()
    
    def has_greeting(exchange):
        user_text = ' '.join(exchange.get_user_texts()).lower()
        return 'hello' in user_text or 'hi' in user_text
    
    def message_stats(exchange):
        """Return multiple annotations."""
        user_count = len(exchange.get_user_messages())
        assistant_count = len(exchange.get_assistant_messages())
        return {
            'user_message_count': user_count,
            'assistant_message_count': assistant_count,
            'total_messages': user_count + assistant_count
        }
    
    tagger.add_rule('greeting', has_greeting)
    tagger.add_rule('stats', message_stats)
    
    # Test with greeting
    exchange = Exchange.create('test', [
        MessageOpenAI({'author': {'role': 'user'}, 'content': {'text': 'Hello world!'}, 'create_time': 1000}),
        MessageOpenAI({'author': {'role': 'assistant'}, 'content': {'text': 'Hi there!'}, 'create_time': 2000}) 
    ])
    
    tagged = tagger.tag_exchange(exchange)
    
    # Check simple boolean annotation
    assert tagged.has_annotation('greeting')
    assert tagged.get_annotation('greeting') is True
    
    # Check multiple annotations from one rule
    assert tagged.has_annotation('user_message_count')
    assert tagged.get_annotation('user_message_count') == 1
    assert tagged.has_annotation('assistant_message_count')
    assert tagged.get_annotation('assistant_message_count') == 1
    assert tagged.get_annotation('total_messages') == 2
    
    # Test without greeting
    exchange_no_greeting = Exchange.create('test', [
        MessageOpenAI({'author': {'role': 'user'}, 'content': {'text': 'What is Python?'}, 'create_time': 1000})
    ])
    
    tagged_no_greeting = tagger.tag_exchange(exchange_no_greeting)
    assert not tagged_no_greeting.has_annotation('greeting')
    assert tagged_no_greeting.get_annotation('user_message_count') == 1
    assert tagged_no_greeting.get_annotation('assistant_message_count') == 0


# def test_exchange_tagger_with_legacy_tags():
#     """Test exchange tagging with legacy Tag return values."""
#     tagger = ExchangeTagger()
    
#     def length_category(exchange):
#         """Return a legacy Tag object."""
#         text = ' '.join(exchange.get_user_texts())
#         length = len(text)
#         if length > 50:
#             return Tag('message_length', size='long', chars=length)
#         elif length > 10:
#             return Tag('message_length', size='medium', chars=length)
#         return False
    
#     tagger.add_rule('length_category', length_category)
    
#     long_exchange = Exchange.create('test', [
#         {'author': {'role': 'user'}, 'content': {'text': 'This is a very long message that should definitely be tagged as long since it exceeds the threshold'}, 'create_time': 1000}
#     ])
    
#     tagged = tagger.tag_exchange(long_exchange)
    
#     # Should convert Tag to annotation
#     assert tagged.has_annotation('message_length')
#     length_data = tagged.get_annotation('message_length')
#     assert length_data['size'] == 'long'
#     assert length_data['chars'] > 50


def test_exchange_tagger_with_string_values():
    """Test exchange tagging with string return values."""
    tagger = ExchangeTagger()
    
    def get_language(exchange):
        """Return a string value."""
        text = ' '.join(exchange.get_user_texts()).lower()
        if 'python' in text:
            return 'python'
        elif 'javascript' in text:
            return 'javascript'
        return None
    
    tagger.add_rule('language', get_language)
    
    python_exchange = Exchange.create('test', [
        MessageOpenAI({'author': {'role': 'user'}, 'content': {'text': 'Help with Python code'}, 'create_time': 1000})
    ])
    
    tagged = tagger.tag_exchange(python_exchange)
    assert tagged.has_annotation('language')
    assert tagged.get_annotation('language') == 'python'


def test_conversation_tagger_annotations():
    """Test conversation-level tagging with annotations."""
    tagger = ConversationTagger()
    
    def is_multi_turn(conversation):
        return conversation.exchange_count > 1
    
    def exchange_summary(conversation):
        """Return structured annotation data."""
        return {
            'exchange_count': conversation.exchange_count,
            'total_messages': conversation.total_message_count,
            'has_continuations': conversation.has_continuations
        }
    
    tagger.add_conversation_rule('multi_turn', is_multi_turn)
    tagger.add_conversation_rule('summary', exchange_summary)
    
    # Create conversation data with multiple exchanges
    conversation_data = {
        'conversation_id': 'test_conv',
        'title': 'Multi-turn conversation',
        'mapping': {
            'msg1': {'message': {'author': {'role': 'user'}, 'create_time': 1000, 'content': {'text': 'First'}}},
            'msg2': {'message': {'author': {'role': 'assistant'}, 'create_time': 2000, 'content': {'text': 'Response 1'}}},
            'msg3': {'message': {'author': {'role': 'user'}, 'create_time': 3000, 'content': {'text': 'Second'}}},
            'msg4': {'message': {'author': {'role': 'assistant'}, 'create_time': 4000, 'content': {'text': 'Response 2'}}}
        }
    }
    
    result = tagger.tag_conversation(conversation_data)
    
    # Check annotations
    assert result.has_annotation('multi_turn')
    assert result.get_annotation('multi_turn') is True
    
    assert result.has_annotation('exchange_count')
    assert result.get_annotation('exchange_count') == 2
    assert result.has_annotation('total_messages')
    assert result.get_annotation('total_messages') == 4


# def test_conversation_tagger_with_legacy_tags():
#     """Test conversation tagger with legacy Tag objects."""
#     tagger = ConversationTagger()
    
#     def complexity_tag(conversation):
#         """Return legacy Tag object."""
#         if conversation.exchange_count > 5:
#             return Tag('complexity', level='high', exchanges=conversation.exchange_count)
#         return Tag('complexity', level='low', exchanges=conversation.exchange_count)
    
#     tagger.add_conversation_rule('complexity', complexity_tag)
    
#     conversation_data = {
#         'conversation_id': 'test_conv',
#         'title': 'Simple conversation',
#         'mapping': {
#             'msg1': {'message': {'author': {'role': 'user'}, 'create_time': 1000, 'content': {'text': 'Hello'}}},
#             'msg2': {'message': {'author': {'role': 'assistant'}, 'create_time': 2000, 'content': {'text': 'Hi there!'}}}
#         }
#     }
    
#     result = tagger.tag_conversation(conversation_data)
    
#     # Should convert Tag to annotation
#     assert result.has_annotation('complexity')
#     complexity_data = result.get_annotation('complexity')
#     assert complexity_data['level'] == 'low'
#     assert complexity_data['exchanges'] == 1


def test_tagging_error_handling():
    """Test that tagging rules handle errors gracefully."""
    tagger = ExchangeTagger()
    
    def broken_rule(exchange):
        raise ValueError("This rule always fails")
    
    def working_rule(exchange):
        return True
    
    tagger.add_rule('broken', broken_rule)
    tagger.add_rule('working', working_rule)
    
    exchange = Exchange.create('test', [
        MessageOpenAI({'author': {'role': 'user'}, 'content': {'text': 'Hello'}, 'create_time': 1000})
    ])
    
    # Should not raise exception
    tagged = tagger.tag_exchange(exchange)
    
    # Working rule should apply, broken rule should be skipped
    assert tagged.has_annotation('working')
    assert tagged.get_annotation('working') is True
    assert not tagged.has_annotation('broken')


# def test_exchange_backward_compatibility():
#     """Test that old Tag-based code still works."""
#     exchange = Exchange.create('test', [
#         {'author': {'role': 'user'}, 'content': {'text': 'Hello'}, 'create_time': 1000}
#     ])
    
#     # Add annotations directly
#     exchange.add_annotation('has_greeting', True)
#     exchange.add_annotation('length', 50)
#     exchange.add_annotation('stats', {'words': 1, 'chars': 5})
    
#     # Test getting tags (backward compatibility)
#     tags = exchange.tags
#     tag_names = [tag.name for tag in tags]
#     assert 'has_greeting' in tag_names
#     assert 'length' in tag_names
#     assert 'stats' in tag_names
    
#     # Find specific tags
#     greeting_tag = next(tag for tag in tags if tag.name == 'has_greeting')
#     assert greeting_tag.attributes == {}  # Simple boolean becomes empty attributes
    
#     length_tag = next(tag for tag in tags if tag.name == 'length')
#     assert length_tag.attributes == {'value': 50}  # Single value
    
#     stats_tag = next(tag for tag in tags if tag.name == 'stats')
#     assert stats_tag.attributes == {'words': 1, 'chars': 5}  # Multiple attributes


@pytest.fixture
def conversation_with_continuation():
    """Conversation data that should trigger continuation merging."""
    return {
        'conversation_id': 'test_conv',
        'title': 'Continuation test',
        'mapping': {
            'msg1': {'message': {'author': {'role': 'user'}, 'create_time': 1000, 'content': {'text': 'Tell me about Python'}}},
            'msg2': {'message': {'author': {'role': 'assistant'}, 'create_time': 2000, 'content': {'text': 'Python is a language...'}}},
            'msg3': {'message': {'author': {'role': 'user'}, 'create_time': 3000, 'content': {'text': 'continue'}}},
            'msg4': {'message': {'author': {'role': 'assistant'}, 'create_time': 4000, 'content': {'text': 'It was created by Guido...'}}}
        }
    }


def test_continuation_detection_with_annotations(conversation_with_continuation):
    """Test that continuation patterns merge exchanges correctly."""
    tagger = ConversationTagger()
    
    # Add a rule that detects continuations
    def detect_continuation(exchange):
        return exchange.has_continuations()
    
    tagger.add_exchange_rule('has_continuation', detect_continuation)
    
    result = tagger.tag_conversation(conversation_with_continuation)
    
    # Should merge into single exchange due to continuation
    assert result.exchange_count == 1
    
    exchange = result.exchanges[0]
    assert len(exchange) == 4
    assert exchange.has_continuations()
    
    # Should have continuation annotation
    assert exchange.has_annotation('has_continuation')
    assert exchange.get_annotation('has_continuation') is True
    
    user_text = ' '.join(exchange.get_user_texts())
    assert 'Tell me about Python' in user_text
    assert 'continue' in user_text


# def test_mixed_annotation_and_tag_workflow():
#     """Test workflow mixing new annotations with legacy Tag objects."""
#     tagger = ExchangeTagger()
    
#     def modern_rule(exchange):
#         """Modern rule returning dict of annotations."""
#         return {
#             'message_count': len(exchange.messages),
#             'has_user': len(exchange.get_user_messages()) > 0,
#             'has_assistant': len(exchange.get_assistant_messages()) > 0
#         }
    
#     def legacy_rule(exchange):
#         """Legacy rule returning Tag object."""
#         if len(exchange.get_user_texts()) > 0:
#             text_length = len(' '.join(exchange.get_user_texts()))
#             return Tag('user_text_stats', length=text_length, word_count=len(' '.join(exchange.get_user_texts()).split()))
#         return False
    
#     tagger.add_rule('modern', modern_rule)
#     tagger.add_rule('legacy', legacy_rule)
    
#     exchange = Exchange.create('test', [
#         {'author': {'role': 'user'}, 'content': {'text': 'Hello world'}, 'create_time': 1000},
#         {'author': {'role': 'assistant'}, 'content': {'text': 'Hi there!'}, 'create_time': 2000}
#     ])
    
#     tagged = tagger.tag_exchange(exchange)
    
#     # Modern annotations
#     assert tagged.get_annotation('message_count') == 2
#     assert tagged.get_annotation('has_user') is True
#     assert tagged.get_annotation('has_assistant') is True
    
#     # Legacy tag converted to annotation
#     assert tagged.has_annotation('user_text_stats')
#     stats = tagged.get_annotation('user_text_stats')
#     assert stats['length'] == 11  # "Hello world"
#     assert stats['word_count'] == 2
    
#     # Test backward compatibility - can still get as tags
#     tags = tagged.tags
#     tag_names = [tag.name for tag in tags]
#     assert 'message_count' in tag_names
#     assert 'has_user' in tag_names
#     assert 'user_text_stats' in tag_names



---
File: docker-compose.yml
---
# docker-compose.yml
services:
  db:
    image: pgvector/pgvector:pg16
    container_name: llm_archive_db
    env_file:
      - .env
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-llm_archive}"]
      interval: 5s
      timeout: 5s
      retries: 5


---
File: docs/annotators.md
---
<!-- docs/annotators.md -->
# Annotation System

## Overview

The annotation system applies labels, tags, and metadata to entities using a typed table architecture. Multiple detection strategies can target the same semantic concept with priority-based resolution.

## Architecture

### Typed Annotation Tables

Each entity type has four annotation tables based on value type:

```
derived.{entity}_annotations_flag      # Boolean presence (key only)
derived.{entity}_annotations_string    # Text values
derived.{entity}_annotations_numeric   # Numeric values
derived.{entity}_annotations_json      # JSONB values
```

**Supported entity types:**
- `content_part` - Individual content segments
- `message` - Complete messages
- `prompt_response` - User-assistant pairs
- `dialogue` - Entire conversations

**Table schema example** (for `prompt_response_annotations_string`):

```sql
CREATE TABLE derived.prompt_response_annotations_string (
    id                  uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    entity_id           uuid NOT NULL REFERENCES derived.prompt_responses(id),
    annotation_key      text NOT NULL,
    annotation_value    text NOT NULL,
    
    confidence          float,
    reason              text,
    source              text NOT NULL,
    source_version      text,
    created_at          timestamptz DEFAULT now(),
    
    UNIQUE (entity_id, annotation_key, annotation_value)
);
```

### Design Benefits

| Benefit | Description |
|---------|-------------|
| **Query Performance** | Direct filtering on typed columns |
| **Type Safety** | Database enforces value types |
| **Efficient Indexes** | Separate indexes per value type |
| **Null Handling** | Flags don't waste space on NULL values |
| **Schema Clarity** | Explicit value types in table names |

## Core Components

### AnnotationResult

Return value from annotator logic:

```python
from llm_archive.annotations.core import AnnotationResult, ValueType

# Flag annotation (presence = true)
AnnotationResult(
    key='has_code_blocks',
    value_type=ValueType.FLAG,
)

# String annotation
AnnotationResult(
    key='exchange_type',
    value='wiki_article',
    value_type=ValueType.STRING,
    confidence=0.9,
)

# Numeric annotation
AnnotationResult(
    key='response_quality_score',
    value=0.85,
    value_type=ValueType.NUMERIC,
    confidence=0.7,
)

# JSON annotation
AnnotationResult(
    key='code_blocks',
    value={'python': 3, 'javascript': 1},
    value_type=ValueType.JSON,
)
```

### AnnotationWriter

Inserts annotations into typed tables:

```python
from llm_archive.annotations.core import AnnotationWriter, EntityType

writer = AnnotationWriter(session)

# Write flag
writer.write_flag(
    entity_type=EntityType.PROMPT_RESPONSE,
    entity_id=pr_id,
    key='is_wiki_candidate',
    source='WikiCandidateAnnotator',
    source_version='1.0',
)

# Write string
writer.write_string(
    entity_type=EntityType.PROMPT_RESPONSE,
    entity_id=pr_id,
    key='proposed_title',
    value='Introduction to Machine Learning',
    confidence=0.8,
    source='NaiveTitleAnnotator',
)

# Write from AnnotationResult
result = AnnotationResult(key='tag', value='technical', value_type=ValueType.STRING)
writer.write_result(
    entity_type=EntityType.MESSAGE,
    entity_id=msg_id,
    result=result,
)
```

### AnnotationReader

Queries annotations from typed tables:

```python
from llm_archive.annotations.core import AnnotationReader, EntityType

reader = AnnotationReader(session)

# Check if flag exists
if reader.has_flag(EntityType.PROMPT_RESPONSE, pr_id, 'is_wiki_candidate'):
    print("This is a wiki candidate")

# Get string values
titles = reader.get_string(EntityType.PROMPT_RESPONSE, pr_id, 'proposed_title')
print(f"Proposed titles: {titles}")

# Get numeric value
score = reader.get_numeric(EntityType.MESSAGE, msg_id, 'sentiment_score')

# Get JSON value
metadata = reader.get_json(EntityType.DIALOGUE, dialogue_id, 'summary_stats')
```

## PromptResponseAnnotator Base Class

### Basic Structure

```python
from llm_archive.annotators.prompt_response import (
    PromptResponseAnnotator,
    PromptResponseData,
)
from llm_archive.annotations.core import AnnotationResult, ValueType

class MyAnnotator(PromptResponseAnnotator):
    # Metadata
    ANNOTATION_KEY = 'my_classification'
    VALUE_TYPE = ValueType.STRING
    PRIORITY = 50  # Higher runs first
    VERSION = '1.0'
    SOURCE = 'heuristic'
    
    # Optional: Filtering prerequisites
    REQUIRES_FLAGS = []  # Only process entities with these flags
    REQUIRES_STRINGS = []  # Only process entities with these (key, value) pairs
    SKIP_IF_FLAGS = []  # Skip entities with these flags
    SKIP_IF_STRINGS = []  # Skip entities with these key or (key, value) pairs
    
    def annotate(self, data: PromptResponseData) -> list[AnnotationResult]:
        """
        Annotate a single prompt-response pair.
        
        Args:
            data: PromptResponseData with all fields populated
        
        Returns:
            List of AnnotationResult objects (empty if no annotations)
        """
        if self._matches_criteria(data):
            return [AnnotationResult(
                key=self.ANNOTATION_KEY,
                value='my_value',
                confidence=0.9,
                reason='Matched pattern X',
            )]
        return []
```

### PromptResponseData

Data passed to annotation logic:

```python
@dataclass
class PromptResponseData:
    prompt_response_id: UUID
    dialogue_id: UUID
    prompt_message_id: UUID
    response_message_id: UUID
    prompt_text: str | None
    response_text: str | None
    prompt_word_count: int | None
    response_word_count: int | None
    prompt_role: str
    response_role: str
    created_at: datetime | None
```

### Annotation Filtering

Annotators can specify prerequisites and skip conditions:

```python
class WikiArticleAnnotator(PromptResponseAnnotator):
    ANNOTATION_KEY = 'is_wiki_article'
    VALUE_TYPE = ValueType.FLAG
    
    # Only process wiki candidates
    REQUIRES_STRINGS = [('exchange_type', 'wiki_article')]
    
    # Skip if already marked as low quality
    SKIP_IF_FLAGS = ['low_quality']
    
    def annotate(self, data: PromptResponseData) -> list[AnnotationResult]:
        # This only runs on wiki_article candidates without low_quality flag
        if self._is_complete_article(data.response_text):
            return [AnnotationResult(key=self.ANNOTATION_KEY)]
        return []
```

## Example Annotators

### WikiCandidateAnnotator

Identifies potential wiki articles:

```python
class WikiCandidateAnnotator(PromptResponseAnnotator):
    """Flag prompt-responses that look like wiki article requests/responses."""
    
    ANNOTATION_KEY = 'exchange_type'
    VALUE_TYPE = ValueType.STRING
    PRIORITY = 80
    VERSION = '1.0'
    
    WIKI_KEYWORDS = [
        'write an article', 'create an article', 'wiki article',
        'wikipedia style', 'encyclopedia entry', 'comprehensive guide',
    ]
    
    def annotate(self, data: PromptResponseData) -> list[AnnotationResult]:
        # Check prompt for wiki-like requests
        if not data.prompt_text:
            return []
        
        prompt_lower = data.prompt_text.lower()
        
        # Check for explicit wiki keywords
        for keyword in self.WIKI_KEYWORDS:
            if keyword in prompt_lower:
                return [AnnotationResult(
                    key=self.ANNOTATION_KEY,
                    value='wiki_article',
                    confidence=0.9,
                    reason=f'Matched keyword: {keyword}',
                )]
        
        # Check response length (wiki articles tend to be long)
        if data.response_word_count and data.response_word_count > 500:
            # Check for article structure (sections, etc.)
            if self._has_article_structure(data.response_text):
                return [AnnotationResult(
                    key=self.ANNOTATION_KEY,
                    value='wiki_article',
                    confidence=0.7,
                    reason='Long response with article structure',
                )]
        
        return []
    
    def _has_article_structure(self, text: str) -> bool:
        """Check if text has typical article structure."""
        if not text:
            return False
        
        # Look for multiple headings
        heading_patterns = ['\n## ', '\n### ', '\n#### ']
        heading_count = sum(text.count(pattern) for pattern in heading_patterns)
        
        return heading_count >= 3
```

### NaiveTitleAnnotator

Extracts potential titles from wiki articles:

```python
class NaiveTitleAnnotator(PromptResponseAnnotator):
    """
    Extract potential article title from response.
    
    Only runs on wiki_article candidates.
    """
    
    ANNOTATION_KEY = 'proposed_title'
    VALUE_TYPE = ValueType.STRING
    PRIORITY = 50
    VERSION = '1.0'
    
    # Only process wiki candidates
    REQUIRES_STRINGS = [('exchange_type', 'wiki_article')]
    
    def annotate(self, data: PromptResponseData) -> list[AnnotationResult]:
        if not data.response_text:
            return []
        
        # Strategy 1: Look for markdown H1
        lines = data.response_text.split('\n')
        for line in lines[:5]:  # Check first 5 lines
            if line.startswith('# '):
                title = line[2:].strip()
                if title:
                    return [AnnotationResult(
                        key=self.ANNOTATION_KEY,
                        value=title,
                        confidence=0.9,
                        reason='Found markdown H1',
                    )]
        
        # Strategy 2: Use first line if short enough
        first_line = lines[0].strip()
        if 5 <= len(first_line.split()) <= 10:
            return [AnnotationResult(
                key=self.ANNOTATION_KEY,
                value=first_line,
                confidence=0.6,
                reason='Used first line',
            )]
        
        return []
```

## Priority System

Annotators execute in priority order (highest first):

```mermaid
flowchart LR
    Start["Annotators"] --> Sort["Sort by<br/>PRIORITY<br/>(descending)"]
    
    Sort --> P100["Priority 90-100<br/>Platform Truth"]
    P100 --> P80["Priority 70-89<br/>Explicit Syntax"]
    P80 --> P50["Priority 40-69<br/>Statistical/ML"]
    P50 --> P30["Priority 1-39<br/>Heuristics"]
```

| Priority Range | Use Case | Example |
|----------------|----------|---------|
| 90-100 | Platform ground truth | ChatGPT code execution metadata |
| 70-89 | Explicit syntax detection | Code blocks, citations |
| 40-69 | Statistical/ML models | Semantic classification |
| 1-39 | Heuristics | Keyword matching |

## Incremental Processing

### Cursor-Based Execution

Annotators track processing state with cursors:

```sql
CREATE TABLE derived.annotator_cursors (
    id                      uuid PRIMARY KEY,
    annotator_name          text NOT NULL,
    annotator_version       text NOT NULL,
    entity_type             text NOT NULL,
    high_water_mark         timestamptz NOT NULL,
    entities_processed      int DEFAULT 0,
    annotations_created     int DEFAULT 0,
    updated_at              timestamptz DEFAULT now(),
    
    UNIQUE (annotator_name, annotator_version, entity_type)
);
```

### Execution Flow

```python
def compute(self) -> int:
    """Run annotation with cursor tracking."""
    # Get or create cursor
    cursor = self._get_cursor()
    
    # Query entities created after cursor
    entities = self._iter_prompt_responses_after(cursor.high_water_mark)
    
    count = 0
    latest_created_at = cursor.high_water_mark
    
    for data in entities:
        results = self.annotate(data)
        for result in results:
            if self._write_result(data.prompt_response_id, result):
                count += 1
        
        # Track latest timestamp
        if data.created_at and data.created_at > latest_created_at:
            latest_created_at = data.created_at
    
    # Update cursor
    self._update_cursor(cursor, count, latest_created_at)
    
    return count
```

## Running Annotators

### Via CLI

```bash
# Run all registered annotators
llm-archive annotate

# Run specific annotator
llm-archive annotate WikiCandidateAnnotator

# Clear and re-run (ignores cursors)
llm-archive annotate --clear
```

### Via Python

```python
from llm_archive.cli import AnnotationManager
from sqlalchemy.orm import Session

# Create manager
manager = AnnotationManager(session)

# Register annotators
manager.register(WikiCandidateAnnotator)
manager.register(NaiveTitleAnnotator)

# Run all (respects priorities and cursors)
results = manager.run_all()
print(f"Created {results['total_annotations']} annotations")

# Run specific annotator
results = manager.run_one('WikiCandidateAnnotator')
```

## Querying Annotations

### Via Views

Use convenience views for common queries:

```sql
-- Get all wiki article candidates with proposed titles
SELECT 
    pr.id,
    pr.dialogue_id,
    prc.prompt_text,
    prc.response_text,
    (SELECT annotation_value 
     FROM derived.prompt_response_annotations_string 
     WHERE entity_id = pr.id AND annotation_key = 'proposed_title') as title
FROM derived.prompt_responses pr
JOIN derived.prompt_response_content prc ON prc.prompt_response_id = pr.id
JOIN derived.prompt_response_annotations_string ans ON ans.entity_id = pr.id
WHERE ans.annotation_key = 'exchange_type' AND ans.annotation_value = 'wiki_article';
```

### Via Python

```python
# Get all wiki candidates
wiki_candidates = (
    session.query(PromptResponse)
    .join(
        prompt_response_annotations_string,
        PromptResponse.id == prompt_response_annotations_string.c.entity_id
    )
    .filter(
        prompt_response_annotations_string.c.annotation_key == 'exchange_type',
        prompt_response_annotations_string.c.annotation_value == 'wiki_article',
    )
    .all()
)
```

## Best Practices

### 1. Choose Appropriate Priority

```python
# Platform truth = 90-100
class ChatGPTCodeExecutionAnnotator(PromptResponseAnnotator):
    PRIORITY = 100

# Explicit syntax = 70-89
class CodeBlockDetector(PromptResponseAnnotator):
    PRIORITY = 80

# Heuristics = 1-39
class KeywordMatcher(PromptResponseAnnotator):
    PRIORITY = 30
```

### 2. Return Appropriate Confidence

```python
# Ground truth
AnnotationResult(value='has_code_execution', confidence=1.0)

# Strong signal
AnnotationResult(value='has_code_blocks', confidence=1.0)

# Moderate signal
AnnotationResult(value='technical_content', confidence=0.7)

# Weak signal
AnnotationResult(value='possibly_tutorial', confidence=0.5)
```

### 3. Use Filtering Effectively

```python
# Only process specific types
REQUIRES_STRINGS = [('exchange_type', 'wiki_article')]

# Avoid reprocessing
SKIP_IF_FLAGS = ['already_processed']

# Avoid conflicts
SKIP_IF_STRINGS = [('proposed_title',)]  # Skip if any title exists
```

### 4. Version Appropriately

```python
class MyAnnotator(PromptResponseAnnotator):
    VERSION = '1.0'  # Initial
    # VERSION = '1.1'  # Bug fix
    # VERSION = '2.0'  # Major logic change
```

Changing VERSION creates a new cursor, allowing re-annotation without clearing.

## Testing

### Unit Tests

Test annotation logic without database:

```python
def test_wiki_candidate_detection():
    """Test wiki candidate detection logic."""
    annotator = WikiCandidateAnnotator(mock_session)
    
    data = PromptResponseData(
        prompt_response_id=uuid4(),
        dialogue_id=uuid4(),
        prompt_message_id=uuid4(),
        response_message_id=uuid4(),
        prompt_text="Write a wiki article about Python",
        response_text="# Python\n\nPython is...",
        prompt_word_count=6,
        response_word_count=500,
        prompt_role='user',
        response_role='assistant',
        created_at=datetime.now(),
    )
    
    results = annotator.annotate(data)
    assert len(results) == 1
    assert results[0].value == 'wiki_article'
```

### Integration Tests

Test full annotation cycle:

```python
def test_annotation_creates_records(session, sample_prompt_response):
    """Test annotation creates database records."""
    annotator = WikiCandidateAnnotator(session)
    count = annotator.compute()
    
    assert count > 0
    
    # Verify record exists
    reader = AnnotationReader(session)
    assert reader.has_string(
        EntityType.PROMPT_RESPONSE,
        sample_prompt_response.id,
        'exchange_type',
        'wiki_article',
    )
```

## Related Documentation

- [Architecture Overview](architecture.md)
- [Schema Design](schema.md) - Annotation table schema
- [Models](models.md) - Annotation models
- [CLI Reference](cli.md) - Running annotators via CLI
- [Builders](builders.md) - Creating entities to annotate



---
File: docs/architecture.md
---
<!-- docs/architecture.md -->
# LLM Archive: System Architecture

## Overview

LLM Archive is a system for importing, normalizing, analyzing, and annotating conversation data from multiple LLM platforms (ChatGPT, Claude, and future sources). It transforms heterogeneous export formats into a unified data model optimized for annotation and downstream processing.

## Design Philosophy

### Core Principles

1. **Source Fidelity**: Raw data is preserved exactly as received; normalization happens in derived layers
2. **Schema Separation**: Clear distinction between raw (immutable imports) and derived (computed analysis)
3. **Incremental Processing**: All analysis is cursor-based to support efficient updates
4. **Platform Abstraction**: Common abstractions with platform-specific extensions
5. **Typed Annotations**: Separate tables per annotation type for query performance

### Key Architectural Decisions

| Decision | Rationale |
|----------|-----------|
| Two-schema design (raw/derived) | Preserves original data while enabling computed views |
| Tree-native message structure | ChatGPT exports are trees; maintain in raw layer |
| Prompt-response as fundamental unit | Simpler than exchange model, no tree dependency |
| Typed annotation tables | Better query performance than polymorphic table |
| Cursor-based incremental processing | Efficient re-annotation without full reprocessing |

## System Architecture

```mermaid
flowchart TB
    subgraph Sources["Data Sources"]
        ChatGPT["ChatGPT Export<br/>(JSON)"]
        Claude["Claude Export<br/>(JSON)"]
        Future["Future Sources<br/>(...)"]
    end

    subgraph Extractors["Extraction Layer"]
        BaseExtractor["BaseExtractor"]
        ChatGPTExtractor["ChatGPTExtractor"]
        ClaudeExtractor["ClaudeExtractor"]
    end

    subgraph RawSchema["raw.* Schema"]
        Dialogues["dialogues"]
        Messages["messages"]
        ContentParts["content_parts"]
        Citations["citations"]
        Attachments["attachments"]
        
        subgraph ChatGPTExt["ChatGPT Extensions"]
            CGPTMeta["chatgpt_message_meta"]
            CGPTSearch["chatgpt_search_*"]
            CGPTCode["chatgpt_code_*"]
            CGPTCanvas["chatgpt_canvas_docs"]
            CGPTDALLE["chatgpt_dalle_generations"]
        end
        
        subgraph ClaudeExt["Claude Extensions"]
            ClaudeMeta["claude_message_meta"]
        end
    end

    subgraph Builders["Builder Layer"]
        PRBuilder["PromptResponseBuilder"]
    end

    subgraph DerivedSchema["derived.* Schema"]
        PRs["prompt_responses"]
        PRContent["prompt_response_content"]
        
        subgraph Annotations["Typed Annotations"]
            AnnFlag["*_annotations_flag"]
            AnnString["*_annotations_string"]
            AnnNumeric["*_annotations_numeric"]
            AnnJSON["*_annotations_json"]
        end
        
        Cursors["annotator_cursors"]
    end

    subgraph AnnotationSystem["Annotation System"]
        AnnotationWriter["AnnotationWriter"]
        AnnotationReader["AnnotationReader"]
        Annotators["Annotators"]
    end

    Sources --> Extractors
    Extractors --> RawSchema
    RawSchema --> Builders
    Builders --> DerivedSchema
    DerivedSchema --> AnnotationSystem
    AnnotationSystem --> DerivedSchema
```

## Data Flow

### Import Flow

```mermaid
sequenceDiagram
    participant User
    participant CLI
    participant Extractor
    participant RawDB as raw.* Tables
    participant Builder
    participant DerivedDB as derived.* Tables
    participant Annotator

    User->>CLI: import_chatgpt [file]
    CLI->>Extractor: extract(file_path)
    
    loop For each conversation
        Extractor->>RawDB: Insert dialogue
        loop For each message
            Extractor->>RawDB: Insert message
            Extractor->>RawDB: Insert content_parts
            Extractor->>RawDB: Insert platform extensions
        end
    end
    
    CLI->>Builder: build_prompt_responses()
    Builder->>RawDB: Query messages
    loop For each dialogue
        Builder->>DerivedDB: Insert prompt_responses
        Builder->>DerivedDB: Insert prompt_response_content
    end
    
    CLI->>Annotator: run_all()
    loop For each annotator (by priority)
        Annotator->>DerivedDB: Check cursor
        Annotator->>DerivedDB: Query entities > cursor
        loop For each entity
            Annotator->>DerivedDB: Insert annotations
        end
        Annotator->>DerivedDB: Update cursor
    end
    
    Annotator-->>User: Complete
```

### Incremental Update Flow

```mermaid
sequenceDiagram
    participant CLI
    participant Extractor
    participant RawDB
    participant Builder
    participant DerivedDB
    participant Annotator

    CLI->>Extractor: import(file, mode=incremental)
    
    loop For each conversation
        Extractor->>RawDB: Check source_id exists
        alt New conversation
            Extractor->>RawDB: INSERT dialogue + messages
        else Existing conversation
            Extractor->>RawDB: Compare content_hash
            alt Changed
                Extractor->>RawDB: UPDATE messages
            end
        end
    end
    
    CLI->>Builder: build_for_dialogue(new/updated)
    Builder->>DerivedDB: Clear existing records
    Builder->>DerivedDB: Rebuild prompt-responses
    
    CLI->>Annotator: run_all()
    Annotator->>DerivedDB: Get cursor (high_water_mark)
    Annotator->>DerivedDB: Query WHERE created_at > cursor
    Note over Annotator: Only processes new entities
```

## Component Responsibilities

### Extractors

Transform platform-specific export formats into the universal raw schema:

| Component | Responsibility |
|-----------|---------------|
| `BaseExtractor` | Common interface, deduplication, transaction management |
| `ChatGPTExtractor` | Parse conversations.json, handle tree structure, extract platform features |
| `ClaudeExtractor` | Parse Claude exports, synthesize linear parent-child relationships |

### Builders

Compute derived structures from raw data:

| Component | Responsibility |
|-----------|---------------|
| `PromptResponseBuilder` | Create userâ†’assistant pairs using parent_id with sequential fallback |

### Annotations

Apply labels, tags, and metadata to entities:

| Component | Responsibility |
|-----------|---------------|
| `AnnotationWriter` | Insert annotations into typed tables |
| `AnnotationReader` | Query annotations from typed tables |
| `PromptResponseAnnotator` | Base class for annotating prompt-response pairs |
| Example annotators | `WikiCandidateAnnotator`, `NaiveTitleAnnotator` |

## Deployment Architecture

```mermaid
flowchart LR
    subgraph Host["Host Machine"]
        CLI["CLI<br/>(Python)"]
        PG["PostgreSQL<br/>(Docker)"]
        Data["Export Files<br/>(.json)"]
    end
    
    CLI -->|"psycopg2"| PG
    Data -->|"read"| CLI
    
    subgraph PGInternal["PostgreSQL Instance"]
        Raw["raw schema"]
        Derived["derived schema"]
    end
```

### Container Setup

```yaml
# docker-compose.yml
services:
  postgres:
    image: pgvector/pgvector:pg16
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: llm_archive
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    volumes:
      - pgdata:/var/lib/postgresql/data
```

## Extension Points

### Adding a New Source

1. Create extractor class extending `BaseExtractor`
2. Add source entry to `raw.sources` table
3. Create platform extension tables if needed
4. Implement message parent-child linking logic

### Adding a New Annotator

1. Extend `PromptResponseAnnotator` base class
2. Define `ANNOTATION_KEY` for grouping
3. Set `PRIORITY` relative to existing annotators
4. Choose `VALUE_TYPE` (flag, string, numeric, json)
5. Implement `annotate()` method returning `AnnotationResult` list
6. Register with CLI

### Adding a New Derived Structure

1. Design schema in new SQL file (e.g., `schema/009_*.sql`)
2. Create SQLAlchemy model in `models/derived.py`
3. Create builder class in `builders/`
4. Integrate with CLI pipeline

### Adding a New Entity Type for Annotations

1. Add entity type to `EntityType` enum in `annotations/core.py`
2. Create annotation tables in schema (4 tables per entity type)
3. Update `AnnotationWriter`/`AnnotationReader` table templates
4. Create base annotator class for the entity type

## Performance Considerations

### Indexing Strategy

- Raw tables: Indexed on `dialogue_id`, `parent_id`, `created_at`
- Derived tables: Indexed on foreign keys and filtered columns
- Annotation tables: Indexed on `entity_id`, `annotation_key`, and `annotation_value`

### Batch Processing

- Extractors use batch inserts (1000 messages/batch)
- Builders process dialogues individually with periodic commits
- Annotators use cursor-based incremental processing

### Memory Management

- Builders iterate dialogues without loading all into memory
- Content aggregation uses database-side text concatenation
- Large dialogues processed incrementally

## Security Considerations

- All data stored locally (no cloud dependencies)
- Database credentials via environment variables
- Source JSON preserved for audit trail
- No PII-specific handling (user responsibility)

## Annotation System Architecture

### Typed Annotation Tables

Each entity type has 4 annotation tables:

```
derived.{entity}_annotations_flag      # Boolean presence
derived.{entity}_annotations_string    # Text values
derived.{entity}_annotations_numeric   # Numeric values
derived.{entity}_annotations_json      # JSONB values
```

### Annotation Workflow

1. **Annotator Registration**: Annotators are ordered by priority
2. **Cursor Check**: Each annotator checks its high-water mark
3. **Entity Query**: Query entities created after cursor
4. **Annotation Logic**: Run detection/classification logic
5. **Result Writing**: Write results to appropriate typed table
6. **Cursor Update**: Update high-water mark for next run

### Priority System

| Priority Range | Use Case | Example |
|----------------|----------|---------|
| 90-100 | Platform ground truth | ChatGPT metadata flags |
| 70-89 | Explicit syntax detection | Code block detection |
| 40-69 | Statistical/ML models | Semantic classification |
| 1-39 | Heuristics | Keyword matching |

## Related Documentation

- [Schema Design](schema.md) - Database schema details
- [Models](models.md) - SQLAlchemy ORM models  
- [Extractors](extractors.md) - Data extraction system
- [Builders](builders.md) - Derived data construction
- [Annotators](annotators.md) - Annotation system
- [CLI Reference](cli.md) - Command-line interface
- [Testing](testing.md) - Testing strategy



---
File: docs/builders.md
---
<!-- docs/builders.md -->
# Builders: Derived Data Construction

## Overview

Builders transform raw imported data into derived structures optimized for annotation and analysis. The current builder system focuses on creating prompt-response pairs directly from message parent-child relationships.

## PromptResponseBuilder

### Purpose

The `PromptResponseBuilder` creates direct userâ†’assistant associations without depending on tree analysis. This is simpler and more robust than the previous exchange-based model.

### Key Features

- **Direct associations**: Uses `parent_id` for tree-aware pairing
- **Sequential fallback**: Falls back to most recent user message when parent_id unavailable
- **Handles regenerations**: Multiple responses can share the same prompt
- **Denormalized content**: Aggregates text content for efficient queries

### Data Model

```mermaid
erDiagram
    PromptResponse ||--|| Message : "prompt (user)"
    PromptResponse ||--|| Message : "response (assistant)"
    PromptResponse ||--|| PromptResponseContent : "has content"
    PromptResponse }o--|| Dialogue : "belongs to"
    
    PromptResponse {
        uuid id PK
        uuid dialogue_id FK
        uuid prompt_message_id FK
        uuid response_message_id FK
        int prompt_position
        int response_position
        string prompt_role
        string response_role
    }
    
    PromptResponseContent {
        uuid prompt_response_id PK_FK
        text prompt_text
        text response_text
        int prompt_word_count
        int response_word_count
    }
```

### Pairing Strategy

```mermaid
flowchart TD
    Start["Process Message"] --> RoleCheck{"Role?"}
    
    RoleCheck -->|"user"| UpdateLast["Update<br/>last_user_msg"]
    RoleCheck -->|"assistant"| FindPrompt["Find Prompt"]
    RoleCheck -->|"system/tool"| Skip["Skip"]
    
    FindPrompt --> HasParent{"Has<br/>parent_id?"}
    
    HasParent -->|"Yes"| CheckParentRole{"Parent<br/>is user?"}
    CheckParentRole -->|"Yes"| UseParent["Use parent"]
    CheckParentRole -->|"No"| WalkUp["Walk up tree"]
    
    WalkUp --> FoundUser{"Found<br/>user?"}
    FoundUser -->|"Yes"| UseFound["Use found"]
    FoundUser -->|"No"| UseLast["Use last_user_msg"]
    
    HasParent -->|"No"| UseLast
    
    UseParent --> CreatePR["Create<br/>PromptResponse"]
    UseFound --> CreatePR
    UseLast --> CreatePR
    
    CreatePR --> UpdateLast
    UpdateLast --> NextMsg["Next Message"]
    Skip --> NextMsg
```

### Handling Regenerations

When a user regenerates an assistant response, ChatGPT creates multiple sibling messages with the same parent:

```mermaid
flowchart TB
    User["user: 'Write a story'"] --> Asst1["assistant v1"]
    User --> Asst2["assistant v2<br/>(regeneration)"]
    User --> Asst3["assistant v3<br/>(regeneration)"]
    
    style Asst2 fill:#ffe0b2
    style Asst3 fill:#ffe0b2
```

Each regeneration creates a separate `PromptResponse` record:
- All three records have the same `prompt_message_id`
- Each has a unique `response_message_id`
- Position tracking maintains sequential order

## Implementation

### Core Algorithm

```python
class PromptResponseBuilder:
    def build_for_dialogue(self, dialogue_id: UUID):
        # Get all messages ordered by created_at
        messages = (
            self.session.query(Message)
            .filter(Message.dialogue_id == dialogue_id)
            .filter(Message.deleted_at.is_(None))
            .order_by(Message.created_at.nulls_first(), Message.id)
            .all()
        )
        
        # Build lookup structures
        msg_by_id = {m.id: m for m in messages}
        position_by_id = {m.id: i for i, m in enumerate(messages)}
        
        # Track most recent user for sequential fallback
        last_user_msg = None
        
        for msg in messages:
            if msg.role == 'user':
                last_user_msg = msg
                continue
            
            # Find the prompt for this response
            prompt_msg = self._find_prompt(msg, msg_by_id, last_user_msg)
            
            if prompt_msg:
                self._create_prompt_response(
                    dialogue_id=dialogue_id,
                    prompt_msg=prompt_msg,
                    response_msg=msg,
                    prompt_position=position_by_id[prompt_msg.id],
                    response_position=position_by_id[msg.id],
                )
```

### Finding the Prompt

```python
def _find_prompt(
    self,
    response_msg: Message,
    msg_by_id: dict[UUID, Message],
    last_user_msg: Message | None,
) -> Message | None:
    """Find the user prompt that elicited this response."""
    
    # Strategy 1: Use parent_id if it points to a user message
    if response_msg.parent_id and response_msg.parent_id in msg_by_id:
        parent = msg_by_id[response_msg.parent_id]
        if parent.role == 'user':
            return parent
        
        # Parent exists but isn't user - walk up tree
        current = parent
        visited = {response_msg.id}
        while current and current.id not in visited:
            visited.add(current.id)
            if current.role == 'user':
                return current
            if current.parent_id and current.parent_id in msg_by_id:
                current = msg_by_id[current.parent_id]
            else:
                break
    
    # Strategy 2: Fall back to most recent user message
    return last_user_msg
```

### Content Aggregation

After creating prompt-response records, content is aggregated:

```python
def _build_content(self, dialogue_id: UUID) -> int:
    """Build content records for all prompt-responses in a dialogue."""
    result = self.session.execute(
        text("""
            INSERT INTO derived.prompt_response_content 
                (prompt_response_id, prompt_text, response_text, 
                 prompt_word_count, response_word_count)
            SELECT 
                pr.id,
                (SELECT string_agg(cp.text_content, ' ' ORDER BY cp.sequence)
                 FROM raw.content_parts cp 
                 WHERE cp.message_id = pr.prompt_message_id),
                (SELECT string_agg(cp.text_content, ' ' ORDER BY cp.sequence)
                 FROM raw.content_parts cp 
                 WHERE cp.message_id = pr.response_message_id),
                (SELECT sum(array_length(
                    string_to_array(cp.text_content, ' '), 1))
                 FROM raw.content_parts cp 
                 WHERE cp.message_id = pr.prompt_message_id),
                (SELECT sum(array_length(
                    string_to_array(cp.text_content, ' '), 1))
                 FROM raw.content_parts cp 
                 WHERE cp.message_id = pr.response_message_id)
            FROM derived.prompt_responses pr
            WHERE pr.dialogue_id = :dialogue_id
            ON CONFLICT (prompt_response_id) DO UPDATE SET
                prompt_text = EXCLUDED.prompt_text,
                response_text = EXCLUDED.response_text,
                prompt_word_count = EXCLUDED.prompt_word_count,
                response_word_count = EXCLUDED.response_word_count
        """),
        {'dialogue_id': dialogue_id}
    )
    return result.rowcount
```

## Build Modes

### Full Rebuild

Clears and rebuilds all prompt-responses:

```bash
# Rebuild all dialogues
llm-archive build_prompt_responses

# Rebuild specific dialogue
llm-archive build_prompt_responses --dialogue_id=<uuid>
```

```python
def build_all(self):
    """Rebuild all prompt-responses."""
    # Clear all existing
    self.session.execute(
        text("DELETE FROM derived.prompt_response_content")
    )
    self.session.execute(
        text("DELETE FROM derived.prompt_responses")
    )
    
    # Rebuild all dialogues
    dialogues = self.session.query(Dialogue.id).all()
    for (dialogue_id,) in dialogues:
        self.build_for_dialogue(dialogue_id)
    
    self.session.commit()
```

### Incremental Build

Builds only for new/updated dialogues:

```python
def build_incremental(self):
    """Build prompt-responses for dialogues without them."""
    # Find dialogues without prompt-responses
    dialogues_without_prs = (
        self.session.query(Dialogue.id)
        .outerjoin(PromptResponse, Dialogue.id == PromptResponse.dialogue_id)
        .filter(PromptResponse.id.is_(None))
        .all()
    )
    
    for (dialogue_id,) in dialogues_without_prs:
        self.build_for_dialogue(dialogue_id)
    
    self.session.commit()
```

## Edge Cases

### System Messages

System messages are typically not part of userâ†’assistant pairs:

```python
# System message is skipped
if msg.role in ('system', 'tool', 'tool_result'):
    continue
```

### Empty Content

Messages with no content parts are still included:

```python
# Content may be NULL if message has no content_parts
prompt_text = None
response_text = None
```

### Tool Use

Tool use messages are treated as non-user, non-assistant messages:

```python
# Tool messages skipped, but assistant response after tool_result is paired
if msg.role in ('tool', 'tool_result'):
    continue
```

## Performance Considerations

### Batch Processing

```python
def build_all(self, batch_size: int = 100):
    """Build with periodic commits."""
    dialogues = self.session.query(Dialogue.id).all()
    
    for i, (dialogue_id,) in enumerate(dialogues):
        self.build_for_dialogue(dialogue_id)
        
        if (i + 1) % batch_size == 0:
            self.session.commit()
            logger.info(f"Processed {i + 1} dialogues")
    
    self.session.commit()
```

### Memory Efficiency

- Uses database-side text aggregation for content
- Processes one dialogue at a time
- Clears per-dialogue structures after processing

## Views

Several views provide convenient access to prompt-response data:

### prompt_response_content_v

Joins prompt_responses with their content for annotation:

```sql
CREATE VIEW derived.prompt_response_content_v AS
SELECT 
    pr.id as prompt_response_id,
    pr.dialogue_id,
    pr.prompt_message_id,
    pr.response_message_id,
    prc.prompt_text,
    prc.response_text,
    prc.prompt_word_count,
    prc.response_word_count,
    pr.prompt_role,
    pr.response_role,
    pr.prompt_position,
    pr.response_position
FROM derived.prompt_responses pr
LEFT JOIN derived.prompt_response_content prc 
    ON prc.prompt_response_id = pr.id;
```

### prompt_exchanges

Groups responses by their prompt (shows regenerations):

```sql
CREATE VIEW derived.prompt_exchanges AS
SELECT 
    prompt_message_id,
    dialogue_id,
    ARRAY_AGG(response_message_id ORDER BY response_position) as response_ids,
    COUNT(*) as response_count,
    COUNT(*) > 1 as has_regenerations,
    MIN(prompt_text) as prompt_text
FROM derived.prompt_responses pr
LEFT JOIN derived.prompt_response_content prc 
    ON prc.prompt_response_id = pr.id
GROUP BY prompt_message_id, dialogue_id;
```

## Testing

### Unit Tests

Test pairing logic with mock data:

```python
def test_find_prompt_uses_parent_id(builder, messages):
    """Test that parent_id takes precedence."""
    user_msg = messages[0]  # role='user'
    asst_msg = messages[1]  # role='assistant', parent_id=user_msg.id
    
    msg_by_id = {m.id: m for m in messages}
    prompt = builder._find_prompt(asst_msg, msg_by_id, user_msg)
    
    assert prompt.id == user_msg.id
```

### Integration Tests

Test full build with real database:

```python
def test_build_creates_prompt_responses(session, sample_dialogue):
    """Test building prompt-responses for a dialogue."""
    builder = PromptResponseBuilder(session)
    stats = builder.build_for_dialogue(sample_dialogue.id)
    
    assert stats['prompt_responses'] > 0
    
    # Verify records exist
    prs = session.query(PromptResponse).filter(
        PromptResponse.dialogue_id == sample_dialogue.id
    ).all()
    assert len(prs) == stats['prompt_responses']
```

## Related Documentation

- [Architecture Overview](architecture.md)
- [Schema Design](schema.md) - Prompt-response table definitions
- [Models](models.md) - PromptResponse SQLAlchemy model
- [Extractors](extractors.md) - Raw data source
- [Annotators](annotators.md) - Downstream annotation



---
File: docs/chatgpt.schema.json
---
{'$schema': 'http://json-schema.org/schema#',
 'type': 'array',
 'items': {'type': 'object',
  'properties': {'mapping': {'type': 'object',
    'patternProperties': {'^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$': {'type': 'object',
      'properties': {'id': {'type': 'string'},
       'message': {'anyOf': [{'type': 'null'},
         {'type': 'object',
          'properties': {'id': {'type': 'string'},
           'author': {'type': 'object',
            'properties': {'role': {'type': 'string'},
             'name': {'type': ['null', 'string']},
             'metadata': {'type': 'object',
              'properties': {'real_author': {'type': 'string'}}}},
            'required': ['metadata', 'name', 'role']},
           'create_time': {'type': ['null', 'number']},
           'update_time': {'type': ['null', 'number']},
           'content': {'type': 'object',
            'properties': {'content_type': {'type': 'string'},
             'parts': {'type': 'array',
              'items': {'anyOf': [{'type': 'string'},
                {'type': 'object',
                 'properties': {'content_type': {'type': 'string'},
                  'asset_pointer': {'type': 'string'},
                  'size_bytes': {'type': 'integer'},
                  'width': {'type': 'integer'},
                  'height': {'type': 'integer'},
                  'fovea': {'type': ['integer', 'null']},
                  'metadata': {'anyOf': [{'type': 'null'},
                    {'type': 'object',
                     'properties': {'dalle': {'anyOf': [{'type': 'null'},
                        {'type': 'object',
                         'properties': {'gen_id': {'type': 'string'},
                          'prompt': {'type': 'string'},
                          'seed': {'type': ['integer', 'null']},
                          'parent_gen_id': {'type': ['null', 'string']},
                          'edit_op': {'type': ['null', 'string']},
                          'serialization_title': {'type': 'string'}},
                         'required': ['edit_op',
                          'gen_id',
                          'parent_gen_id',
                          'prompt',
                          'seed',
                          'serialization_title']}]},
                      'gizmo': {'type': 'null'},
                      'generation': {'anyOf': [{'type': 'null'},
                        {'type': 'object',
                         'properties': {'gen_id': {'type': 'string'},
                          'gen_size': {'type': 'string'},
                          'seed': {'type': 'null'},
                          'parent_gen_id': {'type': 'null'},
                          'height': {'type': 'integer'},
                          'width': {'type': 'integer'},
                          'transparent_background': {'type': 'boolean'},
                          'serialization_title': {'type': 'string'}},
                         'required': ['gen_id',
                          'gen_size',
                          'height',
                          'parent_gen_id',
                          'seed',
                          'serialization_title',
                          'transparent_background',
                          'width']}]},
                      'container_pixel_height': {'type': ['integer', 'null']},
                      'container_pixel_width': {'type': ['integer', 'null']},
                      'emu_omit_glimpse_image': {'type': 'null'},
                      'emu_patches_override': {'type': 'null'},
                      'sanitized': {'type': 'boolean'},
                      'asset_pointer_link': {'type': 'null'},
                      'watermarked_asset_pointer': {'type': 'null'},
                      'start_timestamp': {'type': 'null'},
                      'end_timestamp': {'type': 'null'},
                      'pretokenized_vq': {'type': 'null'},
                      'interruptions': {'type': 'null'},
                      'original_audio_source': {'type': 'null'},
                      'transcription': {'type': 'null'},
                      'word_transcription': {'type': 'null'},
                      'start': {'type': 'number'},
                      'end': {'type': 'number'}}}]},
                  'expiry_datetime': {'type': 'null'},
                  'frames_asset_pointers': {'type': 'array'},
                  'video_container_asset_pointer': {'type': 'null'},
                  'audio_asset_pointer': {'type': 'object',
                   'properties': {'expiry_datetime': {'type': 'null'},
                    'content_type': {'type': 'string'},
                    'asset_pointer': {'type': 'string'},
                    'size_bytes': {'type': 'integer'},
                    'format': {'type': 'string'},
                    'metadata': {'type': 'object',
                     'properties': {'start_timestamp': {'type': 'null'},
                      'end_timestamp': {'type': 'null'},
                      'pretokenized_vq': {'type': 'null'},
                      'interruptions': {'type': 'null'},
                      'original_audio_source': {'type': 'null'},
                      'transcription': {'type': 'null'},
                      'word_transcription': {'type': 'null'},
                      'start': {'type': 'number'},
                      'end': {'type': 'number'}},
                     'required': ['end',
                      'end_timestamp',
                      'interruptions',
                      'original_audio_source',
                      'pretokenized_vq',
                      'start',
                      'start_timestamp',
                      'transcription',
                      'word_transcription']}},
                   'required': ['asset_pointer',
                    'content_type',
                    'expiry_datetime',
                    'format',
                    'metadata',
                    'size_bytes']},
                  'audio_start_timestamp': {'type': 'number'},
                  'text': {'type': 'string'},
                  'direction': {'type': 'string'},
                  'decoding_id': {'type': 'null'},
                  'format': {'type': 'string'}},
                 'required': ['content_type']}]}},
             'language': {'type': 'string'},
             'response_format_name': {'type': 'null'},
             'text': {'type': 'string'},
             'thoughts': {'type': 'array',
              'items': {'type': 'object',
               'properties': {'summary': {'type': 'string'},
                'content': {'type': 'string'}},
               'required': ['content', 'summary']}},
             'source_analysis_msg_id': {'type': 'string'},
             'content': {'type': 'string'},
             'result': {'type': 'string'},
             'summary': {'type': ['null', 'string']},
             'assets': {'type': 'array'},
             'tether_id': {'type': 'null'},
             'url': {'type': 'string'},
             'domain': {'type': 'string'},
             'title': {'type': 'string'},
             'snippet': {'type': 'string'},
             'pub_date': {'type': 'null'},
             'crawl_date': {'type': 'null'},
             'pub_timestamp': {'type': 'number'},
             'ref_id': {'type': 'string'},
             'name': {'type': 'string'}},
            'required': ['content_type']},
           'status': {'type': 'string'},
           'end_turn': {'type': ['boolean', 'null']},
           'weight': {'type': 'number'},
           'metadata': {'type': 'object',
            'properties': {'is_visually_hidden_from_conversation': {'type': 'boolean'},
             'selected_sources': {'type': 'array',
              'items': {'type': 'string'}},
             'selected_github_repos': {'type': 'array'},
             'serialization_metadata': {'type': 'object',
              'properties': {'custom_symbol_offsets': {'type': 'array',
                'items': {'type': 'object',
                 'properties': {'symbol': {'type': 'string'},
                  'startIndex': {'type': 'integer'},
                  'endIndex': {'type': 'integer'}},
                 'required': ['endIndex', 'startIndex', 'symbol']}}},
              'required': ['custom_symbol_offsets']},
             'request_id': {'type': ['null', 'string']},
             'message_source': {'type': 'null'},
             'timestamp_': {'type': 'string'},
             'message_type': {'type': ['null', 'string']},
             'model_slug': {'type': 'string'},
             'default_model_slug': {'type': 'string'},
             'parent_id': {'type': 'string'},
             'is_complete': {'type': 'boolean'},
             'finish_details': {'type': 'object',
              'properties': {'type': {'type': 'string'},
               'stop_tokens': {'type': 'array', 'items': {'type': 'integer'}},
               'stop': {'type': 'string'}},
              'required': ['type']},
             'sonic_classification_result': {'type': 'object',
              'properties': {'latency_ms': {'type': ['null', 'number']},
               'search_prob': {'type': ['null', 'number']},
               'force_search_threshold': {'type': ['null', 'number']},
               'classifier_config_name': {'type': 'string'}},
              'required': ['latency_ms', 'search_prob']},
             'citations': {'type': 'array',
              'items': {'type': 'object',
               'properties': {'start_ix': {'type': 'integer'},
                'end_ix': {'type': 'integer'},
                'invalid_reason': {'type': 'string'},
                'citation_format_type': {'type': 'string'},
                'metadata': {'type': 'object',
                 'properties': {'type': {'type': 'string'},
                  'title': {'type': 'string'},
                  'url': {'type': 'string'},
                  'text': {'type': 'string'},
                  'pub_date': {'type': ['null', 'string']},
                  'extra': {'anyOf': [{'type': 'null'},
                    {'type': 'object',
                     'properties': {'cited_message_idx': {'type': 'integer'},
                      'search_result_idx': {'type': ['integer', 'null']},
                      'evidence_text': {'type': 'string'},
                      'cloud_doc_url': {'type': 'null'}},
                     'required': ['cited_message_idx', 'evidence_text']}]},
                  'og_tags': {'type': 'null'}},
                 'required': ['extra',
                  'pub_date',
                  'text',
                  'title',
                  'type',
                  'url']}},
               'required': ['end_ix', 'start_ix']}},
             'content_references': {'type': 'array',
              'items': {'type': 'object',
               'properties': {'matched_text': {'type': 'string'},
                'start_idx': {'type': 'integer'},
                'end_idx': {'type': 'integer'},
                'refs': {'type': 'array',
                 'items': {'anyOf': [{'type': 'string'},
                   {'type': 'object',
                    'properties': {'turn_index': {'type': 'integer'},
                     'ref_type': {'type': 'string'},
                     'ref_index': {'type': 'integer'}},
                    'required': ['ref_index', 'ref_type', 'turn_index']}]}},
                'alt': {'type': ['null', 'string']},
                'prompt_text': {'type': ['null', 'string']},
                'type': {'type': 'string'},
                'invalid': {'type': 'boolean'},
                'safe_urls': {'type': 'array', 'items': {'type': 'string'}},
                'attributable_index': {'type': 'string'},
                'attributions': {'type': 'null'},
                'attributions_debug': {'type': 'null'},
                'items': {'type': 'array',
                 'items': {'type': 'object',
                  'properties': {'title': {'type': 'string'},
                   'url': {'type': 'string'},
                   'pub_date': {'type': ['null', 'number']},
                   'snippet': {'type': ['null', 'string']},
                   'attribution_segments': {'anyOf': [{'type': 'null'},
                     {'type': 'array', 'items': {'type': 'string'}}]},
                   'supporting_websites': {'type': 'array',
                    'items': {'type': 'object',
                     'properties': {'title': {'type': 'string'},
                      'url': {'type': 'string'},
                      'pub_date': {'type': ['null', 'number']},
                      'snippet': {'type': 'string'},
                      'attribution': {'type': 'string'}},
                     'required': ['attribution',
                      'pub_date',
                      'snippet',
                      'title',
                      'url']}},
                   'refs': {'type': 'array',
                    'items': {'type': 'object',
                     'properties': {'turn_index': {'type': 'integer'},
                      'ref_type': {'type': 'string'},
                      'ref_index': {'type': 'integer'}},
                     'required': ['ref_index', 'ref_type', 'turn_index']}},
                   'hue': {'type': 'null'},
                   'attributions': {'type': 'null'},
                   'attribution': {'type': 'string'}},
                  'required': ['pub_date', 'snippet', 'title', 'url']}},
                'status': {'type': 'string'},
                'error': {'type': 'null'},
                'style': {'type': ['null', 'string']},
                'sources': {'type': 'array',
                 'items': {'type': 'object',
                  'properties': {'title': {'type': 'string'},
                   'url': {'type': 'string'},
                   'attribution': {'type': 'string'}},
                  'required': ['attribution', 'title', 'url']}},
                'has_images': {'type': 'boolean'},
                'images': {'type': 'array',
                 'items': {'type': 'object',
                  'properties': {'url': {'type': 'string'},
                   'content_url': {'type': 'string'},
                   'thumbnail_url': {'type': 'string'},
                   'title': {'type': 'string'},
                   'content_size': {'type': 'object',
                    'properties': {'width': {'type': 'integer'},
                     'height': {'type': 'integer'}},
                    'required': ['height', 'width']},
                   'thumbnail_size': {'type': 'object',
                    'properties': {'width': {'type': 'integer'},
                     'height': {'type': 'integer'}},
                    'required': ['height', 'width']},
                   'thumbnail_crop_info': {'type': 'null'},
                   'attribution': {'type': 'string'}},
                  'required': ['attribution',
                   'content_size',
                   'content_url',
                   'thumbnail_crop_info',
                   'thumbnail_size',
                   'thumbnail_url',
                   'title',
                   'url']}},
                'title': {'type': 'string'},
                'url': {'type': 'string'},
                'pub_date': {'type': ['null', 'number']},
                'snippet': {'type': 'string'},
                'attribution': {'type': 'string'},
                'icon_type': {'type': 'null'}},
               'required': ['end_idx', 'matched_text', 'start_idx', 'type']}},
             'command': {'type': 'string'},
             'status': {'type': 'string'},
             'search_source': {'type': 'string'},
             'client_reported_search_source': {'type': ['null', 'string']},
             'debug_sonic_thread_id': {'type': 'string'},
             'search_result_groups': {'type': 'array',
              'items': {'type': 'object',
               'properties': {'type': {'type': 'string'},
                'domain': {'type': 'string'},
                'entries': {'type': 'array',
                 'items': {'type': 'object',
                  'properties': {'type': {'type': 'string'},
                   'url': {'type': 'string'},
                   'title': {'type': 'string'},
                   'snippet': {'type': 'string'},
                   'ref_id': {'anyOf': [{'type': 'null'},
                     {'type': 'object',
                      'properties': {'turn_index': {'type': 'integer'},
                       'ref_type': {'type': 'string'},
                       'ref_index': {'type': 'integer'}},
                      'required': ['ref_index', 'ref_type', 'turn_index']}]},
                   'content_type': {'type': 'null'},
                   'pub_date': {'type': ['null', 'number']},
                   'attributions': {'type': 'null'},
                   'attribution': {'type': 'string'},
                   'attributions_debug': {'type': 'null'}},
                  'required': ['pub_date',
                   'ref_id',
                   'snippet',
                   'title',
                   'type',
                   'url']}}},
               'required': ['domain', 'entries', 'type']}},
             'safe_urls': {'type': 'array', 'items': {'type': 'string'}},
             'message_locale': {'type': 'string'},
             'image_results': {'type': 'array',
              'items': {'type': 'object',
               'properties': {'url': {'type': 'string'},
                'content_url': {'type': 'string'},
                'thumbnail_url': {'type': 'string'},
                'title': {'type': 'string'},
                'content_size': {'type': 'object',
                 'properties': {'width': {'type': 'integer'},
                  'height': {'type': 'integer'}},
                 'required': ['height', 'width']},
                'thumbnail_size': {'type': 'object',
                 'properties': {'width': {'type': 'integer'},
                  'height': {'type': 'integer'}},
                 'required': ['height', 'width']},
                'thumbnail_crop_info': {'type': 'null'},
                'attribution': {'type': 'string'}},
               'required': ['attribution',
                'content_size',
                'content_url',
                'thumbnail_crop_info',
                'thumbnail_size',
                'thumbnail_url',
                'title',
                'url']}},
             'rebase_developer_message': {'type': 'boolean'},
             'reasoning_status': {'type': 'string'},
             'search_queries': {'type': 'array',
              'items': {'type': 'object',
               'properties': {'type': {'type': 'string'},
                'q': {'type': 'string'}},
               'required': ['q', 'type']}},
             'search_display_string': {'type': 'string'},
             'searched_display_string': {'type': 'string'},
             'finished_duration_sec': {'type': 'integer'},
             'canvas': {'type': 'object',
              'properties': {'textdoc_id': {'type': 'string'},
               'textdoc_type': {'type': 'string'},
               'version': {'type': 'integer'},
               'title': {'type': 'string'},
               'create_source': {'type': 'string'},
               'from_version': {'type': 'integer'},
               'textdoc_content_length': {'type': 'integer'},
               'user_message_type': {'type': 'string'},
               'selection_metadata': {'type': 'object',
                'properties': {'selection_type': {'type': 'string'},
                 'selection_position_range': {'type': 'object',
                  'properties': {'start': {'type': 'integer'},
                   'end': {'type': 'integer'}},
                  'required': ['end', 'start']}},
                'required': ['selection_position_range', 'selection_type']},
               'comment_ids': {'type': 'array', 'items': {'type': 'string'}},
               'has_user_edit': {'type': 'boolean'},
               'is_failure': {'type': 'boolean'}}},
             'targeted_reply': {'type': 'string'},
             'targeted_reply_label': {'type': 'string'},
             'attachments': {'type': 'array',
              'items': {'type': 'object',
               'properties': {'id': {'type': 'string'},
                'size': {'type': 'integer'},
                'name': {'type': 'string'},
                'mime_type': {'type': 'string'},
                'width': {'type': 'integer'},
                'height': {'type': 'integer'},
                'mimeType': {'type': 'string'}},
               'required': ['id', 'name']}},
             'caterpillar_selected_sources': {'type': 'array',
              'items': {'type': 'string'}},
             'gizmo_id': {'type': ['null', 'string']},
             'rebase_system_message': {'type': 'boolean'},
             'category_suggestions': {'type': 'array',
              'items': {'type': 'object',
               'properties': {'title': {'type': 'string'},
                'category_id': {'type': 'string'},
                'suggestions': {'anyOf': [{'type': 'null'},
                  {'type': 'array',
                   'items': {'type': 'object',
                    'properties': {'title': {'type': 'null'},
                     'description': {'type': 'string'},
                     'prompt': {'type': 'string'},
                     'system_hint': {'type': 'string'},
                     'category_id': {'type': 'string'},
                     'cta_label': {'type': 'null'},
                     'image_url': {'type': 'null'},
                     'model_override': {'type': 'null'},
                     'id': {'type': 'string'}},
                    'required': ['category_id',
                     'cta_label',
                     'description',
                     'id',
                     'prompt',
                     'system_hint',
                     'title']}}]},
                'style': {'type': 'string'}},
               'required': ['category_id', 'suggestions', 'title']}},
             'finished_text': {'type': 'string'},
             'initial_text': {'type': 'string'},
             '_cite_metadata': {'type': 'object',
              'properties': {'citation_format': {'type': 'object',
                'properties': {'name': {'type': 'string'},
                 'regex': {'type': 'string'}},
                'required': ['name']},
               'metadata_list': {'type': 'array',
                'items': {'type': 'object',
                 'properties': {'type': {'type': 'string'},
                  'title': {'type': 'string'},
                  'url': {'type': 'string'},
                  'text': {'type': 'string'},
                  'pub_date': {'type': ['null', 'string']},
                  'extra': {'type': 'null'},
                  'og_tags': {'type': 'null'},
                  'name': {'type': 'string'},
                  'id': {'type': 'string'},
                  'source': {'type': 'string'}},
                 'required': ['extra', 'text', 'type']}},
               'original_query': {'type': 'null'}},
              'required': ['citation_format',
               'metadata_list',
               'original_query']},
             'args': {'type': 'array',
              'items': {'anyOf': [{'type': ['integer', 'string']},
                {'type': 'array', 'items': {'type': 'integer'}}]}},
             'system_hints': {'type': 'array', 'items': {'type': 'string'}},
             'cloud_doc_urls': {'type': 'array', 'items': {'type': 'null'}},
             'search_engine': {'type': 'string'},
             'aggregate_result': {'type': 'object',
              'properties': {'status': {'type': 'string'},
               'run_id': {'type': 'string'},
               'start_time': {'type': 'number'},
               'update_time': {'type': 'number'},
               'code': {'type': 'string'},
               'end_time': {'type': ['null', 'number']},
               'final_expression_output': {'type': ['null', 'string']},
               'in_kernel_exception': {'anyOf': [{'type': 'null'},
                 {'type': 'object',
                  'properties': {'name': {'type': 'string'},
                   'traceback': {'type': 'array', 'items': {'type': 'string'}},
                   'args': {'type': 'array', 'items': {'type': 'string'}},
                   'notes': {'type': 'array'}},
                  'required': ['args', 'name', 'notes', 'traceback']}]},
               'system_exception': {'type': 'null'},
               'messages': {'type': 'array',
                'items': {'type': 'object',
                 'properties': {'message_type': {'type': 'string'},
                  'time': {'type': 'number'},
                  'sender': {'type': 'string'},
                  'image_payload': {'type': 'null'},
                  'image_url': {'type': 'string'},
                  'width': {'type': 'integer'},
                  'height': {'type': 'integer'},
                  'stream_name': {'type': 'string'},
                  'text': {'type': 'string'},
                  'timeout_triggered': {'type': 'number'}},
                 'required': ['message_type', 'sender', 'time']}},
               'jupyter_messages': {'type': 'array',
                'items': {'type': 'object',
                 'properties': {'msg_type': {'type': 'string'},
                  'parent_header': {'type': 'object',
                   'properties': {'msg_id': {'type': 'string'},
                    'version': {'type': 'string'}},
                   'required': ['msg_id', 'version']},
                  'content': {'type': 'object',
                   'properties': {'execution_state': {'type': 'string'},
                    'data': {'type': 'object',
                     'properties': {'text/plain': {'type': 'string'},
                      'text/html': {'type': 'string'},
                      'image/vnd.openai.fileservice2.png': {'type': 'string'},
                      'image/vnd.openai.fileservice.png': {'type': 'string'}},
                     'required': ['text/plain']},
                    'name': {'type': 'string'},
                    'text': {'type': 'string'},
                    'traceback': {'type': 'array',
                     'items': {'type': 'string'}},
                    'ename': {'type': 'string'},
                    'evalue': {'type': 'string'}}},
                  'timeout': {'type': 'number'}},
                 'required': ['msg_type']}},
               'timeout_triggered': {'type': ['null', 'number']}},
              'required': ['code',
               'end_time',
               'final_expression_output',
               'in_kernel_exception',
               'jupyter_messages',
               'messages',
               'run_id',
               'start_time',
               'status',
               'system_exception',
               'timeout_triggered',
               'update_time']},
             'paragen_variants_info': {'type': 'object',
              'properties': {'type': {'type': 'string'},
               'num_variants_in_stream': {'type': 'integer'},
               'display_treatment': {'type': 'string'},
               'conversation_id': {'type': 'string'}},
              'required': ['conversation_id',
               'display_treatment',
               'num_variants_in_stream',
               'type']},
             'paragen_variant_choice': {'type': 'string'},
             'voice_mode_message': {'type': 'boolean'},
             'kwargs': {'type': 'object',
              'properties': {'message_id': {'type': 'string'},
               'pending_message_id': {'type': ['null', 'string']},
               'sync_write': {'type': 'boolean'}},
              'required': ['message_id']},
             'augmented_paragen_prompt_label': {'type': ['null', 'string']},
             'exclusive_key': {'type': 'string'},
             'model_switcher_deny': {'type': 'array',
              'items': {'type': 'object',
               'properties': {'slug': {'type': 'string'},
                'context': {'type': 'string'},
                'reason': {'type': 'string'},
                'description': {'type': 'string'}},
               'required': ['context', 'description', 'reason', 'slug']}},
             'pad': {'type': 'string'},
             'real_time_audio_has_video': {'type': 'boolean'},
             'ada_visualizations': {'type': 'array',
              'items': {'type': 'object',
               'properties': {'type': {'type': 'string'},
                'file_id': {'type': 'string'},
                'title': {'type': 'string'},
                'chart_type': {'type': 'string'},
                'fallback_to_image': {'type': 'boolean'}},
               'required': ['chart_type',
                'fallback_to_image',
                'file_id',
                'title',
                'type']}},
             'requested_model_slug': {'type': 'string'},
             'dalle': {'type': 'object',
              'properties': {'from_client': {'type': 'object',
                'properties': {'operation': {'type': 'object',
                  'properties': {'type': {'type': 'string'},
                   'original_gen_id': {'type': 'string'},
                   'original_file_id': {'type': 'string'}},
                  'required': ['original_file_id',
                   'original_gen_id',
                   'type']}},
                'required': ['operation']}},
              'required': ['from_client']},
             'filter_out_for_training': {'type': 'boolean'},
             'jit_plugin_data': {'type': 'object',
              'properties': {'from_server': {'type': 'object',
                'properties': {'type': {'type': 'string'},
                 'body': {'type': 'object',
                  'properties': {'domain': {'type': 'string'},
                   'is_consequential': {'type': 'boolean'},
                   'privacy_policy': {'type': 'string'},
                   'method': {'type': 'string'},
                   'path': {'type': 'string'},
                   'operation': {'type': 'string'},
                   'params': {'type': 'object',
                    'properties': {'country_name': {'type': 'string'},
                     'state_name': {'type': 'string'},
                     'city_name': {'type': 'string'},
                     'filters': {'type': 'string'},
                     'num_trails': {'type': 'integer'},
                     'raw_query': {'type': 'string'},
                     'location_helper': {'type': 'string'}},
                    'required': ['city_name',
                     'country_name',
                     'filters',
                     'location_helper',
                     'num_trails',
                     'raw_query',
                     'state_name']},
                   'actions': {'type': 'array',
                    'items': {'type': 'object',
                     'properties': {'name': {'type': 'string'},
                      'type': {'type': 'string'},
                      'allow': {'type': 'object',
                       'properties': {'target_message_id': {'type': 'string'}},
                       'required': ['target_message_id']},
                      'always_allow': {'type': 'object',
                       'properties': {'target_message_id': {'type': 'string'},
                        'operation_hash': {'type': 'string'}},
                       'required': ['operation_hash', 'target_message_id']},
                      'deny': {'type': 'object',
                       'properties': {'target_message_id': {'type': 'string'}},
                       'required': ['target_message_id']}},
                     'required': ['type']}}},
                  'required': ['actions',
                   'domain',
                   'is_consequential',
                   'method',
                   'operation',
                   'params',
                   'path',
                   'privacy_policy']}},
                'required': ['body', 'type']},
               'from_client': {'type': 'object',
                'properties': {'user_action': {'type': 'object',
                  'properties': {'data': {'type': 'object',
                    'properties': {'type': {'type': 'string'}},
                    'required': ['type']},
                   'target_message_id': {'type': 'string'}},
                  'required': ['data', 'target_message_id']}},
                'required': ['user_action']}}},
             'invoked_plugin': {'type': 'object',
              'properties': {'type': {'type': 'string'},
               'namespace': {'type': 'string'},
               'plugin_id': {'type': 'string'},
               'http_response_status': {'type': 'integer'}},
              'required': ['http_response_status',
               'namespace',
               'plugin_id',
               'type']}}},
           'recipient': {'type': 'string'},
           'channel': {'type': ['null', 'string']}},
          'required': ['author',
           'channel',
           'content',
           'create_time',
           'end_turn',
           'id',
           'metadata',
           'recipient',
           'status',
           'update_time',
           'weight']}]},
       'parent': {'type': ['null', 'string']},
       'children': {'type': 'array', 'items': {'type': 'string'}}},
      'required': ['children', 'id', 'message', 'parent']},
     '^client-created-': {'type': 'object',
      'properties': {'id': {'type': 'string'},
       'message': {'type': 'null'},
       'parent': {'type': 'null'},
       'children': {'type': 'array', 'items': {'type': 'string'}}},
      'required': ['children', 'id', 'message', 'parent']}}},
   'title': {'type': 'string'},
   'create_time': {'type': 'number'},
   'update_time': {'type': 'number'},
   'moderation_results': {'type': 'array'},
   'current_node': {'type': 'string'},
   'plugin_ids': {'anyOf': [{'type': 'null'},
     {'type': 'array', 'items': {'type': 'string'}}]},
   'conversation_id': {'type': 'string'},
   'conversation_template_id': {'type': ['null', 'string']},
   'gizmo_id': {'type': ['null', 'string']},
   'gizmo_type': {'type': ['null', 'string']},
   'is_archived': {'type': 'boolean'},
   'is_starred': {'type': 'null'},
   'safe_urls': {'type': 'array', 'items': {'type': 'string'}},
   'blocked_urls': {'type': 'array'},
   'default_model_slug': {'type': ['null', 'string']},
   'conversation_origin': {'type': 'null'},
   'voice': {'type': ['null', 'string']},
   'async_status': {'type': ['integer', 'null']},
   'disabled_tool_ids': {'type': 'array'},
   'is_do_not_remember': {'type': ['boolean', 'null']},
   'memory_scope': {'type': 'string'},
   'id': {'type': 'string'}},
  'required': ['async_status',
   'blocked_urls',
   'conversation_id',
   'conversation_origin',
   'conversation_template_id',
   'create_time',
   'current_node',
   'default_model_slug',
   'disabled_tool_ids',
   'gizmo_id',
   'gizmo_type',
   'id',
   'is_archived',
   'is_do_not_remember',
   'is_starred',
   'mapping',
   'memory_scope',
   'moderation_results',
   'plugin_ids',
   'safe_urls',
   'title',
   'update_time',
   'voice']}}



---
File: docs/claude.schema.json
---
{'$schema': 'http://json-schema.org/schema#',
 'type': 'array',
 'items': {'type': 'object',
  'properties': {'uuid': {'pattern': '^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$',
    'type': 'string'},
   'name': {'type': 'string'},
   'created_at': {'format': 'date-time', 'type': 'string'},
   'updated_at': {'format': 'date-time', 'type': 'string'},
   'account': {'type': 'object',
    'properties': {'uuid': {'pattern': '^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$',
      'type': 'string'}},
    'required': ['uuid']},
   'chat_messages': {'type': 'array',
    'items': {'type': 'object',
     'properties': {'uuid': {'pattern': '^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$',
       'type': 'string'},
      'text': {'type': 'string'},
      'content': {'type': 'array',
       'items': {'type': 'object',
        'properties': {'start_timestamp': {'anyOf': [{'type': 'null'},
           {'format': 'date-time', 'type': 'string'}]},
         'stop_timestamp': {'anyOf': [{'type': 'null'},
           {'format': 'date-time', 'type': 'string'}]},
         'type': {'type': 'string'},
         'text': {'type': 'string'},
         'citations': {'type': 'array',
          'items': {'type': 'object',
           'properties': {'uuid': {'type': 'string'},
            'start_index': {'type': 'integer'},
            'end_index': {'type': 'integer'},
            'details': {'type': 'object',
             'properties': {'type': {'type': 'string'},
              'url': {'type': 'string'}},
             'required': ['type', 'url']}},
           'required': ['details', 'end_index', 'start_index', 'uuid']}},
         'name': {'type': 'string'},
         'input': {'type': 'object',
          'properties': {'query': {'type': 'string'},
           'id': {'type': 'string'},
           'type': {'type': 'string'},
           'title': {'type': 'string'},
           'command': {'type': 'string'},
           'content': {'type': 'string'},
           'language': {'type': 'string'},
           'version_uuid': {'type': 'string'},
           'new_str': {'type': 'string'},
           'old_str': {'type': 'string'},
           'code': {'type': 'string'},
           'url': {'type': 'string'}}},
         'message': {'type': ['null', 'string']},
         'integration_name': {'type': ['null', 'string']},
         'integration_icon_url': {'type': ['null', 'string']},
         'context': {'type': 'null'},
         'display_content': {'anyOf': [{'type': 'null'},
           {'type': 'object',
            'properties': {'type': {'type': 'string'},
             'link': {'type': 'object',
              'properties': {'title': {'type': 'string'},
               'subtitles': {'type': 'null'},
               'url': {'type': 'string'},
               'resource_type': {'type': 'null'},
               'icon_url': {'type': 'string'},
               'source': {'type': 'string'}},
              'required': ['icon_url',
               'resource_type',
               'source',
               'subtitles',
               'title',
               'url']},
             'is_trusted': {'type': 'boolean'},
             'table': {'type': 'array',
              'items': {'type': 'array', 'items': {'type': 'string'}}}},
            'required': ['type']}]},
         'approval_options': {'type': 'null'},
         'approval_key': {'type': 'null'},
         'content': {'type': 'array',
          'items': {'type': 'object',
           'properties': {'type': {'type': 'string'},
            'title': {'type': 'string'},
            'url': {'type': 'string'},
            'metadata': {'type': 'object',
             'properties': {'type': {'type': 'string'},
              'site_domain': {'type': 'string'},
              'favicon_url': {'type': 'string'},
              'site_name': {'type': 'string'}},
             'required': ['favicon_url', 'site_domain', 'site_name', 'type']},
            'is_missing': {'type': 'boolean'},
            'text': {'type': 'string'},
            'is_citable': {'type': 'boolean'},
            'prompt_context_metadata': {'type': 'object',
             'properties': {'url': {'type': 'string'},
              'age': {'type': 'string'},
              'content_type': {'type': 'string'}}},
            'uuid': {'type': 'string'}},
           'required': ['text', 'type']}},
         'is_error': {'type': 'boolean'},
         'thinking': {'type': 'string'},
         'summaries': {'type': 'array',
          'items': {'type': 'object',
           'properties': {'summary': {'type': 'string'}},
           'required': ['summary']}},
         'cut_off': {'type': 'boolean'}},
        'required': ['start_timestamp', 'stop_timestamp', 'type']}},
      'sender': {'type': 'string'},
      'created_at': {'format': 'date-time', 'type': 'string'},
      'updated_at': {'format': 'date-time', 'type': 'string'},
      'attachments': {'type': 'array',
       'items': {'type': 'object',
        'properties': {'file_name': {'type': 'string'},
         'file_size': {'type': 'integer'},
         'file_type': {'type': 'string'},
         'extracted_content': {'type': 'string'}},
        'required': ['extracted_content',
         'file_name',
         'file_size',
         'file_type']}},
      'files': {'type': 'array',
       'items': {'type': 'object',
        'properties': {'file_name': {'type': 'string'}},
        'required': ['file_name']}}},
     'required': ['attachments',
      'content',
      'created_at',
      'files',
      'sender',
      'text',
      'updated_at',
      'uuid']}}},
  'required': ['account',
   'chat_messages',
   'created_at',
   'name',
   'updated_at',
   'uuid']}}



---
File: docs/cli.md
---
<!-- docs/cli.md -->
# Command-Line Interface

## Overview

The CLI provides the primary interface for importing data, building derived structures, and running annotations. It's built with Python Fire for automatic command generation.

## Installation

```bash
# Install package with CLI
pip install -e .

# Or run directly with uv
uv run llm-archive <command>
```

## Configuration

### Environment Variables

```bash
export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/llm_archive"

# Or individual components
export POSTGRES_HOST=localhost
export POSTGRES_PORT=5432
export POSTGRES_DB=llm_archive
export POSTGRES_USER=postgres
export POSTGRES_PASSWORD=postgres
```

### Configuration File

```yaml
# config.yaml
database:
  host: localhost
  port: 5432
  database: llm_archive
  user: postgres
  password: postgres

import:
  batch_size: 1000
  
logging:
  level: INFO
```

---

## Command Reference

### Database Management

#### `init`

Initialize database with schemas:

```bash
# Create schemas and tables from schema/ directory
llm-archive init --schema_dir=schema

# With custom connection
llm-archive init --database_url="postgresql://..." --schema_dir=schema
```

```mermaid
flowchart LR
    Init["init"] --> CreateRaw["CREATE SCHEMA raw"]
    Init --> CreateDerived["CREATE SCHEMA derived"]
    CreateRaw --> RawTables["Create raw.* tables"]
    CreateDerived --> DerivedTables["Create derived.* tables"]
    RawTables --> InsertSources["INSERT sources"]
```

#### `reset`

Reset database (destructive):

```bash
# Drop and recreate all schemas
llm-archive reset --confirm

# Reset only derived schema
llm-archive reset --schema=derived --confirm
```

### Data Import

#### `import_chatgpt`

Import ChatGPT conversation export:

```bash
# Basic import
llm-archive import_chatgpt /path/to/conversations.json

# Incremental import (update only changed)
llm-archive import_chatgpt /path/to/conversations.json --incremental

# With batch size
llm-archive import_chatgpt /path/to/conversations.json --batch_size=500
```

**Arguments:**

| Argument | Description | Default |
|----------|-------------|---------|
| `file` | Path to conversations.json | Required |
| `--incremental` | Only update changed dialogues | False |
| `--batch_size` | Messages per batch | 1000 |

#### `import_claude`

Import Claude conversation export:

```bash
# Basic import
llm-archive import_claude /path/to/claude_export.json

# Incremental import
llm-archive import_claude /path/to/claude_export.json --incremental
```

#### `import_all`

Import from multiple sources:

```bash
llm-archive import_all \
    --chatgpt_path=/path/to/chatgpt.json \
    --claude_path=/path/to/claude.json \
    --incremental
```

### Building Derived Data

#### `build_prompt_responses`

Build prompt-response pairs from imported messages:

```bash
# Build all dialogues
llm-archive build_prompt_responses

# Build specific dialogue
llm-archive build_prompt_responses --dialogue_id="uuid-here"

# Rebuild (clears existing first)
llm-archive build_prompt_responses --rebuild
```

**Arguments:**

| Argument | Description | Default |
|----------|-------------|---------|
| `--dialogue_id` | Build for specific dialogue only | None (all) |
| `--rebuild` | Clear and rebuild | False |
| `--batch_size` | Dialogues per commit | 100 |

```mermaid
flowchart LR
    Build["build_prompt_responses"] --> Clear["Clear existing<br/>(if rebuild)"]
    Clear --> Iterate["Iterate dialogues"]
    
    Iterate --> Process["Process each dialogue"]
    Process --> Pair["Create prompt-response<br/>pairs"]
    Pair --> Content["Aggregate content"]
    Content --> Next["Next dialogue"]
    
    Next --> Commit{"Batch full?"}
    Commit -->|Yes| CommitDB["COMMIT"]
    Commit -->|No| Iterate
    CommitDB --> Iterate
```

### Annotation

#### `annotate`

Run annotators on entities:

```bash
# Run all registered annotators
llm-archive annotate

# Run specific annotator
llm-archive annotate WikiCandidateAnnotator

# Clear cursors and re-run everything
llm-archive annotate --clear
```

**Arguments:**

| Argument | Description | Default |
|----------|-------------|---------|
| `annotator` | Annotator name or None for all | None (all) |
| `--clear` | Clear cursors before running | False |

```mermaid
flowchart TD
    Annotate["annotate"] --> Sort["Sort by priority"]
    Sort --> Loop["For each annotator"]
    
    Loop --> GetCursor["Get/Create cursor"]
    GetCursor --> Query["Query entities > cursor"]
    
    Query --> HasEntities{"Entities?"}
    HasEntities -->|Yes| Process["Process entity"]
    Process --> Write["Write annotations"]
    Write --> Next["Next entity"]
    Next --> HasEntities
    
    HasEntities -->|No| UpdateCursor["Update cursor"]
    UpdateCursor --> NextAnnotator["Next annotator"]
    NextAnnotator --> Loop
```

### Analysis and Queries

#### `stats`

Show database statistics:

```bash
# Show overview
llm-archive stats

# Detailed stats
llm-archive stats --detailed
```

Output includes:
- Dialogue counts by source
- Message counts by role
- Prompt-response pair counts
- Annotation counts by key

#### `query_annotations`

Query entities by annotations:

```bash
# Find all wiki article candidates
llm-archive query_annotations \
    --entity_type=prompt_response \
    --annotation_key=exchange_type \
    --annotation_value=wiki_article

# Find entities with specific flag
llm-archive query_annotations \
    --entity_type=message \
    --annotation_key=has_code_blocks \
    --value_type=flag

# Export results
llm-archive query_annotations \
    --entity_type=prompt_response \
    --annotation_key=exchange_type \
    --annotation_value=wiki_article \
    --output=wiki_articles.json
```

**Arguments:**

| Argument | Description |
|----------|-------------|
| `--entity_type` | Entity type (prompt_response, message, etc.) |
| `--annotation_key` | Annotation key to filter on |
| `--annotation_value` | Annotation value to filter on (optional for flags) |
| `--value_type` | Type of annotation (flag, string, numeric, json) |
| `--output` | Output file path |

### Pipeline Commands

#### `pipeline`

Run full import and processing pipeline:

```bash
# Full pipeline with initialization
llm-archive pipeline \
    --chatgpt_path=/path/to/chatgpt.json \
    --claude_path=/path/to/claude.json \
    --init_db

# Incremental update
llm-archive pipeline \
    --chatgpt_path=/path/to/new_export.json \
    --incremental
```

**Pipeline steps:**
1. Initialize database (if `--init_db`)
2. Import ChatGPT conversations (if `--chatgpt_path`)
3. Import Claude conversations (if `--claude_path`)
4. Build prompt-responses
5. Run all annotators

---

## Usage Examples

### First-Time Setup

```bash
# 1. Initialize database
llm-archive init --schema_dir=schema

# 2. Import ChatGPT export
llm-archive import_chatgpt ~/Downloads/conversations.json

# 3. Import Claude export
llm-archive import_claude ~/Downloads/claude_export.json

# 4. Build prompt-response pairs
llm-archive build_prompt_responses

# 5. Run annotations
llm-archive annotate

# 6. Check statistics
llm-archive stats
```

### Incremental Update

```bash
# Weekly update with new export
llm-archive pipeline \
    --chatgpt_path=new_conversations.json \
    --incremental
```

### Targeted Operations

```bash
# Rebuild prompt-responses for single dialogue
llm-archive build_prompt_responses --dialogue_id="12345678-..."

# Re-run specific annotator
llm-archive annotate WikiCandidateAnnotator --clear

# Query wiki article candidates
llm-archive query_annotations \
    --entity_type=prompt_response \
    --annotation_key=exchange_type \
    --annotation_value=wiki_article \
    --output=wiki_articles.json
```

### Development Workflow

```bash
# Reset database
llm-archive reset --confirm

# Import test data
llm-archive import_chatgpt tests/fixtures/sample.json

# Build and annotate
llm-archive build_prompt_responses
llm-archive annotate

# Check results
llm-archive stats --detailed
```

---

## Error Handling

### Common Issues

**Database connection failed:**
```bash
# Check environment variables
echo $DATABASE_URL

# Or check individual vars
echo $POSTGRES_HOST
echo $POSTGRES_PORT

# Test connection
psql -h localhost -U postgres -d llm_archive
```

**Import fails with duplicate key:**
```bash
# Use incremental mode to update existing
llm-archive import_chatgpt file.json --incremental

# Or clear and reimport
llm-archive reset --schema=raw --confirm
llm-archive import_chatgpt file.json
```

**Annotation cursor stuck:**
```bash
# Clear cursors and re-run
llm-archive annotate --clear
```

### Verbose Output

```bash
# Set log level
export LOG_LEVEL=DEBUG
llm-archive import_chatgpt file.json

# Or use python directly with debug
python -m llm_archive.cli import_chatgpt file.json
```

---

## Shell Completion

### Bash

```bash
# Add to ~/.bashrc
eval "$(llm-archive --completion bash)"
```

### Zsh

```bash
# Add to ~/.zshrc
eval "$(llm-archive --completion zsh)"
```

---

## Related Documentation

- [Architecture Overview](architecture.md)
- [Extractors](extractors.md) - Import details
- [Builders](builders.md) - Build process
- [Annotators](annotators.md) - Annotation system
- [Testing](testing.md) - Testing commands



---
File: docs/extractors.md
---
# docs/extractors.md
# Data Extraction System

## Overview

Extractors transform platform-specific export formats into the universal raw schema. Each extractor handles the idiosyncrasies of its source platform while producing consistent output that can be processed by downstream builders and annotators.

## Architecture

```mermaid
classDiagram
    direction TB
    
    class BaseExtractor {
        <<abstract>>
        +source_id: str
        +session: Session
        +extract(file_path: Path) int
        +extract_dialogue(data: dict) Dialogue
        #_extract_messages(dialogue, data) list[Message]
        #_build_content_parts(data) list[dict]
        #_compute_content_hash(parts) str
    }
    
    class ChatGPTExtractor {
        +source_id = "chatgpt"
        +extract_dialogue(data) Dialogue
        -_extract_message_node(node, dialogue_id, parent_id) Message
        -_extract_content_parts(message_data) list[dict]
        -_extract_search_metadata(message, metadata)
        -_extract_code_execution(message, metadata)
        -_extract_canvas_metadata(message, metadata)
        -_extract_dalle_metadata(content_part, asset_pointer)
    }
    
    class ClaudeExtractor {
        +source_id = "claude"
        +extract_dialogue(data) Dialogue
        -_extract_messages(dialogue, chat_messages) list[Message]
        -_extract_content_parts(content) list[dict]
        -_synthesize_parent_chain(messages) 
    }
    
    BaseExtractor <|-- ChatGPTExtractor
    BaseExtractor <|-- ClaudeExtractor
```

## Base Extractor

The `BaseExtractor` class provides common functionality for all extractors:

### Core Interface

```python
class BaseExtractor(ABC):
    """Base class for platform-specific extractors."""
    
    source_id: str = None  # Override in subclass: 'chatgpt', 'claude'
    
    def __init__(self, session: Session):
        self.session = session
    
    def extract(
        self,
        file_path: Path,
        mode: str = 'full',      # 'full' | 'incremental'
        batch_size: int = 1000,
    ) -> int:
        """
        Extract dialogues from export file.
        
        Args:
            file_path: Path to export JSON file
            mode: 'full' replaces existing, 'incremental' updates only
            batch_size: Messages per batch insert
            
        Returns:
            Number of dialogues processed
        """
    
    @abstractmethod
    def extract_dialogue(self, data: dict) -> Dialogue:
        """Extract a single dialogue from source data."""
        pass
```

### Deduplication Strategy

```mermaid
flowchart TD
    Start["Process Dialogue"] --> Check{"Exists in DB?<br/>(source + source_id)"}
    
    Check -->|No| Insert["INSERT new dialogue"]
    Check -->|Yes| Mode{"Import Mode?"}
    
    Mode -->|full| Replace["DELETE + INSERT"]
    Mode -->|incremental| Compare["Compare content_hash"]
    
    Compare --> Changed{"Changed?"}
    Changed -->|Yes| Update["UPDATE messages"]
    Changed -->|No| Skip["Skip (no changes)"]
    
    Insert --> Done["Continue"]
    Replace --> Done
    Update --> Done
    Skip --> Done
```

### Content Hashing

Content hashes enable efficient change detection:

```python
def _compute_content_hash(self, content_parts: list[dict]) -> str:
    """Compute SHA256 hash of message content for change detection."""
    # Normalize content for consistent hashing
    normalized = []
    for part in sorted(content_parts, key=lambda p: p.get('sequence', 0)):
        normalized.append({
            'type': part.get('part_type'),
            'text': part.get('text_content', ''),
            'language': part.get('language'),
        })
    
    content_str = json.dumps(normalized, sort_keys=True)
    return hashlib.sha256(content_str.encode()).hexdigest()
```

---

## ChatGPT Extractor

### Export Format

ChatGPT exports are ZIP files containing:
- `conversations.json` - Array of conversation objects
- `message_feedback.json` - User feedback (not currently extracted)
- Various media files

### Conversation Structure

```json
{
  "id": "abc123",
  "title": "Conversation Title",
  "create_time": 1699900000.0,
  "update_time": 1699900100.0,
  "mapping": {
    "node-id-1": {
      "id": "node-id-1",
      "parent": null,
      "children": ["node-id-2"],
      "message": {
        "id": "msg-id-1",
        "author": {"role": "system"},
        "content": {"content_type": "text", "parts": ["..."]},
        "create_time": 1699900000.0,
        "metadata": {...}
      }
    },
    "node-id-2": {...}
  },
  "current_node": "node-id-final",
  "gizmo_id": "g-xxx"  // Custom GPT identifier
}
```

### Tree Extraction

ChatGPT conversations are stored as trees in a `mapping` dictionary:

```mermaid
flowchart TD
    subgraph Mapping["ChatGPT mapping{}"]
        Root["system message<br/>(root)"]
        U1["user message"]
        A1["assistant v1"]
        A2["assistant v2<br/>(regeneration)"]
        U2["user follow-up"]
        A3["assistant response"]
    end
    
    Root --> U1
    U1 --> A1
    U1 --> A2
    A1 --> U2
    U2 --> A3
    
    style A2 fill:#ffe0b2
```

```python
def _extract_message_tree(self, dialogue: Dialogue, mapping: dict) -> list[Message]:
    """Extract messages preserving tree structure."""
    messages = []
    
    # Build ID mapping for parent references
    source_to_db_id = {}
    
    # Process in topological order (parents before children)
    for node_id in self._topological_sort(mapping):
        node = mapping[node_id]
        
        if not node.get('message'):
            continue  # Skip empty nodes
        
        parent_source_id = node.get('parent')
        parent_db_id = source_to_db_id.get(parent_source_id)
        
        message = self._extract_message_node(
            node=node,
            dialogue_id=dialogue.id,
            parent_id=parent_db_id,
        )
        
        messages.append(message)
        source_to_db_id[node_id] = message.id
    
    return messages
```

### Content Part Extraction

```python
def _extract_content_parts(self, message_data: dict) -> list[dict]:
    """Extract content parts from ChatGPT message."""
    content = message_data.get('content', {})
    content_type = content.get('content_type', 'text')
    parts = []
    
    if content_type == 'text':
        # Text content: parts is array of strings
        for i, text in enumerate(content.get('parts', [])):
            parts.append({
                'sequence': i,
                'part_type': 'text',
                'text_content': text,
                'source_json': {'type': 'text', 'text': text},
            })
    
    elif content_type == 'code':
        # Code block with language
        parts.append({
            'sequence': 0,
            'part_type': 'code',
            'text_content': content.get('text', ''),
            'language': content.get('language'),
            'source_json': content,
        })
    
    elif content_type == 'multimodal_text':
        # Mixed content: text, images, files
        for i, part in enumerate(content.get('parts', [])):
            if isinstance(part, str):
                parts.append({
                    'sequence': i,
                    'part_type': 'text',
                    'text_content': part,
                })
            elif isinstance(part, dict):
                # Image or file reference
                parts.append(self._extract_multimodal_part(i, part))
    
    return parts
```

### Platform Feature Extraction

#### Web Search

```python
def _extract_search_metadata(self, message: Message, metadata: dict):
    """Extract web search results into extension tables."""
    search_groups = metadata.get('search_result_groups', [])
    
    for group_data in search_groups:
        group = ChatGPTSearchGroup(
            message_id=message.id,
            group_type=group_data.get('group_type'),
            domain=group_data.get('domain'),
            source_json=group_data,
        )
        self.session.add(group)
        
        for i, entry in enumerate(group_data.get('entries', [])):
            search_entry = ChatGPTSearchEntry(
                group_id=group.id,
                sequence=i,
                url=entry.get('url'),
                title=entry.get('title'),
                snippet=entry.get('snippet'),
                source_json=entry,
            )
            self.session.add(search_entry)
```

#### Code Execution

```python
def _extract_code_execution(self, message: Message, metadata: dict):
    """Extract Code Interpreter execution data."""
    aggregate_result = metadata.get('aggregate_result', {})
    
    for run_data in aggregate_result.get('runs', []):
        execution = ChatGPTCodeExecution(
            message_id=message.id,
            run_id=run_data.get('id'),
            status=run_data.get('status'),
            code=run_data.get('code'),
            started_at=self._parse_timestamp(run_data.get('start_time')),
            ended_at=self._parse_timestamp(run_data.get('end_time')),
            final_output=run_data.get('final_output'),
            exception_name=run_data.get('exception', {}).get('name'),
            exception_traceback=run_data.get('exception', {}).get('traceback'),
            source_json=run_data,
        )
        self.session.add(execution)
        
        for i, output in enumerate(run_data.get('outputs', [])):
            code_output = ChatGPTCodeOutput(
                execution_id=execution.id,
                sequence=i,
                output_type=output.get('type'),
                stream_name=output.get('stream'),
                text_content=output.get('text'),
                image_url=output.get('image_url'),
                source_json=output,
            )
            self.session.add(code_output)
```

---

## Claude Extractor

### Export Format

Claude exports are JSON files with this structure:

```json
{
  "uuid": "conversation-uuid",
  "name": "Conversation Title",
  "created_at": "2024-01-15T10:30:00Z",
  "updated_at": "2024-01-15T11:45:00Z",
  "chat_messages": [
    {
      "uuid": "message-uuid",
      "sender": "human",
      "created_at": "2024-01-15T10:30:00Z",
      "updated_at": "2024-01-15T10:30:00Z",
      "content": [
        {"type": "text", "text": "Hello!"}
      ],
      "attachments": []
    },
    {
      "uuid": "message-uuid-2",
      "sender": "assistant",
      "created_at": "2024-01-15T10:30:05Z",
      "content": [
        {"type": "text", "text": "Hello! How can I help?"}
      ]
    }
  ]
}
```

### Linear to Tree Conversion

Claude exports are linear (no branching). The extractor synthesizes parent-child relationships:

```mermaid
flowchart LR
    subgraph Export["Claude Export (Linear)"]
        M1["Message 1"]
        M2["Message 2"]
        M3["Message 3"]
        M4["Message 4"]
        
        M1 --> M2 --> M3 --> M4
    end
    
    subgraph Database["Database (Tree)"]
        D1["Message 1<br/>parent: null"]
        D2["Message 2<br/>parent: M1"]
        D3["Message 3<br/>parent: M2"]
        D4["Message 4<br/>parent: M3"]
        
        D1 --> D2 --> D3 --> D4
    end
    
    Export --> |"synthesize<br/>parent_id"| Database
```

```python
def _synthesize_parent_chain(self, messages: list[Message]):
    """Add parent_id to create linear tree structure."""
    for i, message in enumerate(messages):
        if i > 0:
            message.parent_id = messages[i - 1].id
```

### Content Part Extraction

```python
def _extract_content_parts(self, content: list[dict]) -> list[dict]:
    """Extract content parts from Claude message content array."""
    parts = []
    
    for i, item in enumerate(content):
        content_type = item.get('type', 'text')
        
        if content_type == 'text':
            parts.append({
                'sequence': i,
                'part_type': 'text',
                'text_content': item.get('text', ''),
                'source_json': item,
            })
        
        elif content_type == 'tool_use':
            parts.append({
                'sequence': i,
                'part_type': 'tool_use',
                'tool_name': item.get('name'),
                'tool_use_id': item.get('id'),
                'tool_input': item.get('input'),
                'source_json': item,
            })
        
        elif content_type == 'tool_result':
            parts.append({
                'sequence': i,
                'part_type': 'tool_result',
                'tool_use_id': item.get('tool_use_id'),
                'text_content': self._extract_tool_result_text(item),
                'is_error': item.get('is_error', False),
                'source_json': item,
            })
        
        elif content_type == 'image':
            parts.append({
                'sequence': i,
                'part_type': 'image',
                'media_type': item.get('source', {}).get('media_type'),
                'url': item.get('source', {}).get('url'),
                'source_json': item,
            })
        
        elif content_type == 'thinking':
            parts.append({
                'sequence': i,
                'part_type': 'thinking',
                'text_content': item.get('thinking', ''),
                'source_json': item,
            })
    
    return parts
```

### Tool Use Correlation

Claude's tool use requires correlating `tool_use` with `tool_result`:

```mermaid
sequenceDiagram
    participant User
    participant Assistant
    participant Tool
    
    User->>Assistant: "Search for X"
    Assistant->>Tool: tool_use (id: "toolu_123")
    Tool-->>Assistant: tool_result (tool_use_id: "toolu_123")
    Assistant->>User: "I found..."
```

```python
def _correlate_tool_use(self, messages: list[Message]):
    """Link tool_result parts back to their tool_use parts."""
    tool_uses = {}  # tool_use_id -> ContentPart
    
    for message in messages:
        for part in message.content_parts:
            if part.part_type == 'tool_use':
                tool_uses[part.tool_use_id] = part
            elif part.part_type == 'tool_result':
                # The tool_use_id field already links them
                # But we can add additional correlation if needed
                pass
```

---

## Import Modes

### Full Import

Replaces all existing data for the source:

```python
extractor.extract(file_path, mode='full')
```

1. Delete existing dialogues with matching `source_id`
2. Insert all dialogues from file
3. Rebuild derived structures

### Incremental Import

Updates only changed data:

```python
extractor.extract(file_path, mode='incremental')
```

1. Check if dialogue exists (by `source` + `source_id`)
2. If new: INSERT
3. If exists: Compare `content_hash`
4. If changed: UPDATE messages
5. Mark missing messages as `deleted_at`

```mermaid
stateDiagram-v2
    [*] --> CheckExists
    
    CheckExists --> Insert: Not Found
    CheckExists --> CompareHash: Found
    
    CompareHash --> Skip: Unchanged
    CompareHash --> Update: Changed
    
    Insert --> MarkDeleted
    Update --> MarkDeleted
    Skip --> MarkDeleted
    
    MarkDeleted --> [*]
    
    note right of MarkDeleted
        Messages in DB but not in
        export get deleted_at set
    end note
```

---

## Error Handling

### Malformed Data

```python
def extract_dialogue(self, data: dict) -> Dialogue | None:
    """Extract dialogue, handling malformed data gracefully."""
    try:
        source_id = data.get('id')
        if not source_id:
            logger.warning("Dialogue missing 'id' field, skipping")
            return None
        
        dialogue = Dialogue(
            source=self.source_id,
            source_id=source_id,
            title=data.get('title'),
            source_json=data,
        )
        
        # Extract messages...
        
        return dialogue
        
    except Exception as e:
        logger.error(f"Failed to extract dialogue {data.get('id')}: {e}")
        return None
```

### Transaction Safety

```python
def extract(self, file_path: Path, ...) -> int:
    """Extract with transaction safety."""
    count = 0
    
    with open(file_path) as f:
        conversations = json.load(f)
    
    for data in conversations:
        try:
            dialogue = self.extract_dialogue(data)
            if dialogue:
                self.session.add(dialogue)
                count += 1
                
                # Batch commits
                if count % 100 == 0:
                    self.session.commit()
                    
        except Exception as e:
            logger.error(f"Failed: {e}")
            self.session.rollback()
            continue
    
    self.session.commit()
    return count
```

---

## Adding a New Extractor

### Step 1: Create Extractor Class

```python
# llm_archive/extractors/new_platform.py

class NewPlatformExtractor(BaseExtractor):
    """Extractor for NewPlatform exports."""
    
    source_id = 'new_platform'
    
    def extract_dialogue(self, data: dict) -> Dialogue:
        # 1. Create Dialogue object
        dialogue = Dialogue(
            source=self.source_id,
            source_id=data['conversation_id'],
            title=data.get('title'),
            source_created_at=parse_timestamp(data.get('created')),
            source_json=data,
        )
        
        # 2. Extract messages
        messages = self._extract_messages(dialogue, data['messages'])
        
        # 3. Establish tree structure
        self._build_parent_chain(messages)
        
        # 4. Extract platform-specific features
        self._extract_platform_features(messages, data)
        
        return dialogue
```

### Step 2: Add Source Entry

```sql
INSERT INTO raw.sources (id, display_name, has_native_trees, role_vocabulary)
VALUES ('new_platform', 'New Platform', false, ARRAY['user', 'assistant']);
```

### Step 3: Create Extension Tables (if needed)

```sql
CREATE TABLE raw.new_platform_message_meta (
    message_id uuid PRIMARY KEY REFERENCES raw.messages ON DELETE CASCADE,
    custom_field text,
    source_json jsonb NOT NULL
);
```

### Step 4: Register in CLI

```python
# llm_archive/cli.py

EXTRACTORS = {
    'chatgpt': ChatGPTExtractor,
    'claude': ClaudeExtractor,
    'new_platform': NewPlatformExtractor,
}
```

---

## Related Documentation

- [Architecture Overview](architecture.md)
- [Schema Design](schema.md)
- [Models](models.md)
- [Builders](builders.md) - Post-extraction processing


---
File: docs/index.md
---
<!-- docs/index.md -->
# LLM Archive Documentation

## Overview

LLM Archive is a system for importing, normalizing, analyzing, and annotating conversation data from multiple LLM platforms. It transforms heterogeneous export formats into a unified data model optimized for annotation and downstream analysis.

## Quick Start

```bash
# Initialize database
llm-archive init --schema_dir=schema

# Import conversations
llm-archive import_chatgpt conversations.json
llm-archive import_claude claude_export.json

# Build prompt-response pairs
llm-archive build_prompt_responses

# Run annotations
llm-archive annotate

# View statistics
llm-archive stats
```

## Documentation Guide

### System Design

| Document | Description |
|----------|-------------|
| [Architecture](architecture.md) | High-level system design, data flow, component responsibilities |
| [Schema](schema.md) | Database schema design for raw and derived tables |
| [Models](models.md) | SQLAlchemy ORM models and relationships |

### Components

| Document | Description |
|----------|-------------|
| [Extractors](extractors.md) | Platform-specific data extraction (ChatGPT, Claude) |
| [Builders](builders.md) | Derived data construction (prompt-responses) |
| [Annotators](annotators.md) | Typed annotation system with strategy pattern |
| [CLI](cli.md) | Command-line interface reference |
| [Testing](testing.md) | Testing strategy and guidelines |

## Architecture Overview

```mermaid
flowchart TB
    subgraph Input["Data Sources"]
        ChatGPT["ChatGPT Export"]
        Claude["Claude Export"]
    end
    
    subgraph Extract["Extraction"]
        Extractors["Extractors"]
    end
    
    subgraph Store["Storage"]
        Raw["raw.* Schema"]
        Derived["derived.* Schema"]
    end
    
    subgraph Process["Processing"]
        Build["Prompt-Response<br/>Builder"]
        Annotate["Annotators"]
    end
    
    subgraph Output["Output"]
        Query["Query API"]
        Export["Export"]
    end
    
    ChatGPT --> Extractors
    Claude --> Extractors
    Extractors --> Raw
    Raw --> Build
    Build --> Derived
    Derived --> Annotate
    Annotate --> Derived
    Derived --> Query
    Derived --> Export
```

## Key Concepts

### Two-Schema Architecture

- **raw.\***: Immutable imported data with full source fidelity
- **derived.\***: Computed structures that can be rebuilt

### Prompt-Response Model

- Fundamental interaction unit: user prompt + assistant response
- Direct parent-child associations without tree dependency
- Each response associated with exactly one prompt
- Prompts can have multiple responses (regenerations)

### Typed Annotation System

- Separate tables per (entity_type, value_type) combination
- Four value types: `flag`, `string`, `numeric`, `json`
- Four entity types: `content_part`, `message`, `prompt_response`, `dialogue`
- Cursor-based incremental processing
- Multiple strategies can target the same semantic concept with priority ordering

## Module Structure

```
llm_archive/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ raw.py              # Raw schema models
â”‚   â””â”€â”€ derived.py          # Derived schema models
â”œâ”€â”€ extractors/
â”‚   â”œâ”€â”€ base.py             # Base extractor class
â”‚   â”œâ”€â”€ chatgpt.py          # ChatGPT extractor
â”‚   â””â”€â”€ claude.py           # Claude extractor
â”œâ”€â”€ builders/
â”‚   â””â”€â”€ prompt_response.py  # Prompt-response builder
â”œâ”€â”€ annotations/
â”‚   â””â”€â”€ core.py             # AnnotationWriter/Reader
â”œâ”€â”€ annotators/
â”‚   â””â”€â”€ prompt_response.py  # Prompt-response annotators
â”œâ”€â”€ cli.py                  # Command-line interface
â””â”€â”€ config.py               # Environment configuration
```

## Schema Layers

### Raw Schema (`raw.*`)

- Sources registry
- Dialogues (conversations)
- Messages (with tree structure via parent_id)
- Content parts (text, code, images, tool use)
- Platform-specific extensions (ChatGPT search, code execution, DALL-E, etc.)

### Derived Schema (`derived.*`)

- Prompt-response pairs and content
- Typed annotation tables
- Views for querying annotated content
- Annotator cursor tracking

## Annotation Workflow

```mermaid
flowchart LR
    Entities["Prompt-Response<br/>Pairs"] --> Annotator1["WikiCandidate<br/>Priority=80"]
    Annotator1 --> Annotator2["NaiveTitle<br/>Priority=50"]
    Annotator2 --> Results["Typed<br/>Annotations"]
    
    Results --> FlagTable["*_annotations_flag"]
    Results --> StringTable["*_annotations_string"]
    Results --> NumericTable["*_annotations_numeric"]
    Results --> JSONTable["*_annotations_json"]
```

## Entity Relationships

```mermaid
erDiagram
    Dialogue ||--o{ Message : contains
    Message ||--o{ ContentPart : has
    Message ||--o{ Message : parent_of
    Dialogue ||--o{ PromptResponse : segmented_into
    PromptResponse ||--|| PromptResponseContent : has
    PromptResponse ||--o{ Annotation_Flag : annotated_by
    PromptResponse ||--o{ Annotation_String : annotated_by
    PromptResponse ||--o{ Annotation_Numeric : annotated_by
    PromptResponse ||--o{ Annotation_JSON : annotated_by
    Message ||--o{ Annotation_Flag : annotated_by
    Message ||--o{ Annotation_String : annotated_by
```

## Processing Pipeline

```mermaid
flowchart TD
    Import["Import<br/>(Extractors)"] --> Raw["raw.dialogues<br/>raw.messages<br/>raw.content_parts"]
    
    Raw --> Builder["PromptResponse<br/>Builder"]
    Builder --> PRs["derived.prompt_responses<br/>derived.prompt_response_content"]
    
    PRs --> Annotators["Annotators<br/>(Priority-Ordered)"]
    Annotators --> Annotations["derived.*_annotations_*"]
    
    Annotations --> Query["Query/Export"]
```

## Getting Started

1. **Setup**: Initialize database and import data
2. **Build**: Create prompt-response pairs
3. **Annotate**: Run annotators to classify and tag content
4. **Query**: Use views and annotations to find specific content
5. **Export**: Extract processed data for downstream use

## Support

- **GitHub Issues**: Bug reports and feature requests
- **Documentation**: This docs/ folder
- **Tests**: tests/ folder for examples and integration patterns

## Version History

See CHANGELOG.md for release notes.



---
File: docs/models.md
---
<!-- docs/models.md -->
# SQLAlchemy Models

## Overview

The ORM layer provides Python classes that map to database tables, enabling type-safe data access and relationship navigation. Models are organized into two modules mirroring the database schemas:

- `models/raw.py` - Raw schema models (imports)
- `models/derived.py` - Derived schema models (computed)

## Model Organization

```mermaid
classDiagram
    direction TB
    
    class Base {
        <<declarative_base>>
    }
    
    namespace raw {
        class Source {
            +id: str
            +display_name: str
            +has_native_trees: bool
            +role_vocabulary: list[str]
            +metadata: dict
        }
        
        class Dialogue {
            +id: UUID
            +source: str
            +source_id: str
            +title: str
            +source_created_at: datetime
            +source_updated_at: datetime
            +source_json: dict
        }
        
        class Message {
            +id: UUID
            +dialogue_id: UUID
            +source_id: str
            +parent_id: UUID
            +role: str
            +author_id: str
            +author_name: str
            +content_hash: str
        }
        
        class ContentPart {
            +id: UUID
            +message_id: UUID
            +sequence: int
            +part_type: str
            +text_content: str
        }
    }
    
    namespace derived {
        class PromptResponse {
            +id: UUID
            +dialogue_id: UUID
            +prompt_message_id: UUID
            +response_message_id: UUID
            +prompt_position: int
            +response_position: int
        }
        
        class PromptResponseContent {
            +prompt_response_id: UUID
            +prompt_text: str
            +response_text: str
            +prompt_word_count: int
            +response_word_count: int
        }
    }
    
    Base <|-- Source
    Base <|-- Dialogue
    Base <|-- Message
    Base <|-- ContentPart
    Base <|-- PromptResponse
    Base <|-- PromptResponseContent
    
    Source "1" --> "*" Dialogue : dialogues
    Dialogue "1" --> "*" Message : messages
    Message "1" --> "*" ContentPart : content_parts
    Message "1" --> "*" Message : children
    Dialogue "1" --> "*" PromptResponse : prompt_responses
    PromptResponse "1" --> "1" PromptResponseContent : content
    PromptResponse "1" --> "1" Message : prompt_message
    PromptResponse "1" --> "1" Message : response_message
```

## Raw Models (`models/raw.py`)

### Core Models

#### Source

```python
class Source(Base):
    """Registry of dialogue sources."""
    __tablename__ = "sources"
    __table_args__ = {"schema": "raw"}
    
    id = Column(String, primary_key=True)
    display_name = Column(String, nullable=False)
    has_native_trees = Column(Boolean, nullable=False)
    role_vocabulary = Column(ARRAY(String), nullable=False)
    source_metadata = Column(JSONB, name="metadata")
    
    # Relationships
    dialogues = relationship("Dialogue", back_populates="source_rel")
```

**Usage:**
```python
# Check if source supports tree structure
if source.has_native_trees:
    # Handle ChatGPT-style branching
    pass
    
# Validate role
if message.role not in source.role_vocabulary:
    raise ValueError(f"Invalid role: {message.role}")
```

#### Dialogue

```python
class Dialogue(Base):
    """Universal dialogue container."""
    __tablename__ = "dialogues"
    __table_args__ = {"schema": "raw"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True, 
                server_default=func.gen_random_uuid())
    source = Column(String, ForeignKey("raw.sources.id"), nullable=False)
    source_id = Column(String, nullable=False)
    
    title = Column(String)
    
    # Source timestamps (from archive)
    source_created_at = Column(DateTime(timezone=True))
    source_updated_at = Column(DateTime(timezone=True))
    
    source_json = Column(JSONB, nullable=False)
    
    # DB timestamps
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationships
    source_rel = relationship("Source", back_populates="dialogues")
    messages = relationship("Message", back_populates="dialogue", 
                          cascade="all, delete-orphan")
    prompt_responses = relationship("PromptResponse", back_populates="dialogue",
                                  cascade="all, delete-orphan")
```

**Usage:**
```python
# Get all messages
for message in dialogue.messages:
    print(f"{message.role}: {message.content_parts[0].text_content}")

# Get prompt-response pairs
for pr in dialogue.prompt_responses:
    print(f"Q: {pr.content.prompt_text}")
    print(f"A: {pr.content.response_text}")
```

#### Message

```python
class Message(Base):
    """Universal message with tree structure support."""
    __tablename__ = "messages"
    __table_args__ = {"schema": "raw"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True,
                server_default=func.gen_random_uuid())
    dialogue_id = Column(PG_UUID(as_uuid=True), 
                        ForeignKey("raw.dialogues.id", ondelete="CASCADE"),
                        nullable=False)
    source_id = Column(String, nullable=False)
    
    # Tree structure
    parent_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.messages.id"))
    
    # Normalized fields
    role = Column(String, nullable=False)
    author_id = Column(String)
    author_name = Column(String)
    
    # Source timestamps
    source_created_at = Column(DateTime(timezone=True))
    source_updated_at = Column(DateTime(timezone=True))
    
    # Change tracking
    content_hash = Column(String)
    deleted_at = Column(DateTime(timezone=True))
    
    source_json = Column(JSONB, nullable=False)
    
    # DB timestamps
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationships
    dialogue = relationship("Dialogue", back_populates="messages")
    content_parts = relationship("ContentPart", back_populates="message",
                               cascade="all, delete-orphan",
                               order_by="ContentPart.sequence")
    
    # Tree relationships
    parent = relationship("Message", remote_side=[id], backref="children")
```

**Usage:**
```python
# Get text content
text = ' '.join(part.text_content for part in message.content_parts 
                if part.text_content)

# Navigate tree
if message.parent:
    print(f"Parent: {message.parent.role}")

for child in message.children:
    print(f"Child: {child.role}")

# Check for regenerations (siblings)
if message.parent:
    siblings = [m for m in message.parent.children if m.id != message.id]
    if siblings:
        print(f"This message has {len(siblings)} regeneration(s)")
```

#### ContentPart

```python
class ContentPart(Base):
    """Segmented content within a message."""
    __tablename__ = "content_parts"
    __table_args__ = {"schema": "raw"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True,
                server_default=func.gen_random_uuid())
    message_id = Column(PG_UUID(as_uuid=True),
                       ForeignKey("raw.messages.id", ondelete="CASCADE"),
                       nullable=False)
    sequence = Column(Integer, nullable=False)
    
    part_type = Column(String, nullable=False)
    text_content = Column(Text)
    
    # Code-specific
    language = Column(String)
    
    # Media-specific
    media_type = Column(String)
    url = Column(String)
    
    # Tool use-specific
    tool_name = Column(String)
    tool_use_id = Column(String)
    tool_input = Column(JSONB)
    
    # Relationships
    message = relationship("Message", back_populates="content_parts")
```

**Usage:**
```python
# Filter by type
text_parts = [p for p in message.content_parts if p.part_type == 'text']
code_parts = [p for p in message.content_parts if p.part_type == 'code']

# Get code blocks
for part in code_parts:
    print(f"Language: {part.language}")
    print(part.text_content)
```

### Platform Extension Models

#### ChatGPT Extensions

```python
class ChatGPTMessageMeta(Base):
    """ChatGPT-specific message metadata."""
    __tablename__ = "chatgpt_message_meta"
    __table_args__ = {"schema": "raw"}
    
    message_id = Column(PG_UUID(as_uuid=True),
                       ForeignKey("raw.messages.id", ondelete="CASCADE"),
                       primary_key=True)
    
    weight = Column(Float)
    end_turn = Column(Boolean)
    recipient = Column(String)
    model_slug = Column(String)
    is_complete = Column(Boolean)
    finish_details = Column(JSONB)

class ChatGPTCodeExecution(Base):
    """Code execution results from ChatGPT."""
    # ... similar structure

class ChatGPTSearchGroup(Base):
    """Web search groups from ChatGPT."""
    # ... similar structure
```

#### Claude Extensions

```python
class ClaudeMessageMeta(Base):
    """Claude-specific message metadata."""
    __tablename__ = "claude_message_meta"
    __table_args__ = {"schema": "raw"}
    
    message_id = Column(PG_UUID(as_uuid=True),
                       ForeignKey("raw.messages.id", ondelete="CASCADE"),
                       primary_key=True)
    
    model = Column(String)
    usage_input_tokens = Column(Integer)
    usage_output_tokens = Column(Integer)
```

## Derived Models (`models/derived.py`)

### PromptResponse

```python
class PromptResponse(Base):
    """
    Direct prompt-response association without tree dependency.
    
    Each record pairs a user prompt with one of its responses.
    A prompt can have multiple responses (regenerations).
    Each response appears in exactly one record.
    """
    __tablename__ = "prompt_responses"
    __table_args__ = {"schema": "derived"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True,
                server_default=func.gen_random_uuid())
    dialogue_id = Column(PG_UUID(as_uuid=True),
                        ForeignKey("raw.dialogues.id", ondelete="CASCADE"),
                        nullable=False)
    
    prompt_message_id = Column(PG_UUID(as_uuid=True),
                              ForeignKey("raw.messages.id"),
                              nullable=False)
    response_message_id = Column(PG_UUID(as_uuid=True),
                                ForeignKey("raw.messages.id"),
                                nullable=False)
    
    prompt_position = Column(Integer, nullable=False)
    response_position = Column(Integer, nullable=False)
    
    prompt_role = Column(String, nullable=False)
    response_role = Column(String, nullable=False)
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationships
    dialogue = relationship("Dialogue", back_populates="prompt_responses")
    prompt_message = relationship("Message", foreign_keys=[prompt_message_id])
    response_message = relationship("Message", foreign_keys=[response_message_id])
    content = relationship("PromptResponseContent", uselist=False,
                         back_populates="prompt_response",
                         cascade="all, delete-orphan")
```

**Usage:**
```python
# Access messages
print(f"User: {pr.prompt_message.content_parts[0].text_content}")
print(f"Assistant: {pr.response_message.content_parts[0].text_content}")

# Access denormalized content (faster)
print(f"Q: {pr.content.prompt_text}")
print(f"A: {pr.content.response_text}")

# Find regenerations (same prompt, different responses)
siblings = (
    session.query(PromptResponse)
    .filter(
        PromptResponse.prompt_message_id == pr.prompt_message_id,
        PromptResponse.id != pr.id
    )
    .all()
)
```

### PromptResponseContent

```python
class PromptResponseContent(Base):
    """
    Denormalized text content for annotation/search without joins.
    """
    __tablename__ = "prompt_response_content"
    __table_args__ = {"schema": "derived"}
    
    prompt_response_id = Column(PG_UUID(as_uuid=True),
                                ForeignKey("derived.prompt_responses.id",
                                         ondelete="CASCADE"),
                                primary_key=True)
    
    prompt_text = Column(Text)
    response_text = Column(Text)
    
    prompt_word_count = Column(Integer)
    response_word_count = Column(Integer)
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    
    # Relationships
    prompt_response = relationship("PromptResponse", back_populates="content")
```

**Usage:**
```python
# Query with content filters
long_responses = (
    session.query(PromptResponse)
    .join(PromptResponseContent)
    .filter(PromptResponseContent.response_word_count > 500)
    .all()
)

# Search text content
wiki_articles = (
    session.query(PromptResponse)
    .join(PromptResponseContent)
    .filter(PromptResponseContent.prompt_text.ilike('%write an article%'))
    .all()
)
```

## Querying Patterns

### Basic Queries

```python
from sqlalchemy.orm import Session
from llm_archive.models import Dialogue, Message, PromptResponse

# Get all dialogues from ChatGPT
chatgpt_dialogues = (
    session.query(Dialogue)
    .filter(Dialogue.source == 'chatgpt')
    .all()
)

# Get user messages
user_messages = (
    session.query(Message)
    .filter(Message.role == 'user')
    .all()
)

# Get prompt-responses with long responses
long_prs = (
    session.query(PromptResponse)
    .join(PromptResponseContent)
    .filter(PromptResponseContent.response_word_count > 1000)
    .all()
)
```

### Joining with Annotations

```python
from sqlalchemy import text

# Get wiki article candidates
wiki_candidates = (
    session.query(PromptResponse)
    .join(
        text("""
            derived.prompt_response_annotations_string 
            ON derived.prompt_response_annotations_string.entity_id = derived.prompt_responses.id
        """)
    )
    .filter(
        text("""
            derived.prompt_response_annotations_string.annotation_key = 'exchange_type'
            AND derived.prompt_response_annotations_string.annotation_value = 'wiki_article'
        """)
    )
    .all()
)
```

### Eager Loading

```python
from sqlalchemy.orm import joinedload

# Load dialogue with all messages and content
dialogue = (
    session.query(Dialogue)
    .options(
        joinedload(Dialogue.messages).joinedload(Message.content_parts)
    )
    .filter(Dialogue.id == dialogue_id)
    .one()
)

# Load prompt-response with content
pr = (
    session.query(PromptResponse)
    .options(joinedload(PromptResponse.content))
    .filter(PromptResponse.id == pr_id)
    .one()
)
```

### Tree Navigation

```python
# Get message tree depth
def get_depth(message):
    depth = 0
    current = message
    while current.parent:
        depth += 1
        current = current.parent
    return depth

# Get all descendants
def get_descendants(message):
    descendants = []
    for child in message.children:
        descendants.append(child)
        descendants.extend(get_descendants(child))
    return descendants

# Get root message
def get_root(message):
    current = message
    while current.parent:
        current = current.parent
    return current
```

## Session Management

### Basic Session

```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from llm_archive.config import DATABASE_URL
from llm_archive.models import Base

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(bind=engine)

with SessionLocal() as session:
    dialogues = session.query(Dialogue).all()
    # ... work with data
    session.commit()
```

### Context Manager

```python
from contextlib import contextmanager

@contextmanager
def get_session():
    session = SessionLocal()
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()

# Usage
with get_session() as session:
    dialogue = session.query(Dialogue).first()
```

## Related Documentation

- [Architecture Overview](architecture.md)
- [Schema Design](schema.md) - Database schema details
- [Extractors](extractors.md) - Creating model instances
- [Builders](builders.md) - Building derived models
- [Annotators](annotators.md) - Querying models



---
File: docs/schema.md
---
# docs/schema.md
# Database Schema Design

## Overview

The LLM Archive database uses a two-schema architecture that separates concerns between raw imported data and derived computed structures. This design ensures source fidelity while enabling rich analysis capabilities.

## Schema Philosophy

```mermaid
flowchart LR
    subgraph Sources["External Sources"]
        ChatGPT["ChatGPT<br/>Export"]
        Claude["Claude<br/>Export"]
    end
    
    subgraph RawLayer["raw.* (Immutable)"]
        direction TB
        R1["Source of Truth"]
        R2["Platform-Specific"]
        R3["Preserves Original"]
    end
    
    subgraph DerivedLayer["derived.* (Computed)"]
        direction TB
        D1["Normalized Views"]
        D2["Cross-Platform"]
        D3["Rebuild-Safe"]
    end
    
    Sources --> RawLayer
    RawLayer --> DerivedLayer
    
    style RawLayer fill:#e1f5fe
    style DerivedLayer fill:#f3e5f5
```

### Design Principles

| Principle | Implementation |
|-----------|---------------|
| **Source Fidelity** | `source_json` column preserves original data |
| **Platform Abstraction** | Core tables are platform-agnostic |
| **Extension Tables** | Platform-specific features in separate tables |
| **Temporal Tracking** | Both source and database timestamps |
| **Soft Deletes** | `deleted_at` for removed content |
| **Cascade Deletes** | Referential integrity maintained automatically |

## Raw Schema (`raw.*`)

The raw schema contains imported data exactly as received from source platforms.

### Entity Relationship Diagram

```mermaid
erDiagram
    sources ||--o{ dialogues : "has"
    dialogues ||--o{ messages : "contains"
    messages ||--o{ content_parts : "has"
    messages ||--o{ messages : "parent_of"
    messages ||--o{ attachments : "has"
    content_parts ||--o{ citations : "has"
    
    messages ||--o| chatgpt_message_meta : "extends"
    messages ||--o{ chatgpt_search_groups : "has"
    messages ||--o{ chatgpt_code_executions : "has"
    messages ||--o{ chatgpt_canvas_docs : "has"
    content_parts ||--o{ chatgpt_dalle_generations : "has"
    chatgpt_search_groups ||--o{ chatgpt_search_entries : "contains"
    chatgpt_code_executions ||--o{ chatgpt_code_outputs : "has"
    
    messages ||--o| claude_message_meta : "extends"

    sources {
        text id PK
        text display_name
        boolean has_native_trees
        text[] role_vocabulary
        jsonb metadata
    }
    
    dialogues {
        uuid id PK
        text source FK
        text source_id
        text title
        timestamptz source_created_at
        timestamptz source_updated_at
        jsonb source_json
        timestamptz created_at
        timestamptz updated_at
    }
    
    messages {
        uuid id PK
        uuid dialogue_id FK
        text source_id
        uuid parent_id FK
        text role
        text author_id
        text author_name
        timestamptz source_created_at
        timestamptz source_updated_at
        text content_hash
        timestamptz deleted_at
        jsonb source_json
        timestamptz created_at
        timestamptz updated_at
    }
    
    content_parts {
        uuid id PK
        uuid message_id FK
        int sequence
        text part_type
        text text_content
        text language
        text media_type
        text url
        text tool_name
        text tool_use_id
        jsonb tool_input
        timestamptz started_at
        timestamptz ended_at
        boolean is_error
        jsonb source_json
    }
```

### Core Tables

#### `raw.sources`

Registry of supported data sources. Pre-populated with known platforms.

```sql
CREATE TABLE raw.sources (
    id                  text PRIMARY KEY,          -- 'chatgpt', 'claude'
    display_name        text NOT NULL,             -- 'ChatGPT', 'Claude'
    has_native_trees    boolean NOT NULL,          -- true for ChatGPT
    role_vocabulary     text[] NOT NULL,           -- ['user', 'assistant', ...]
    metadata            jsonb                       -- platform-specific config
);
```

**Design Notes:**
- `has_native_trees`: ChatGPT exports include branching; Claude exports are linear
- `role_vocabulary`: Validates message roles during import

#### `raw.dialogues`

Universal container for conversations from any source.

```sql
CREATE TABLE raw.dialogues (
    id                  uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    source              text NOT NULL REFERENCES raw.sources(id),
    source_id           text NOT NULL,             -- platform's conversation ID
    title               text,
    
    -- Source timestamps (from archive export)
    source_created_at   timestamptz,
    source_updated_at   timestamptz,
    
    source_json         jsonb NOT NULL,            -- complete original object
    
    -- Database timestamps
    created_at          timestamptz DEFAULT now(),
    updated_at          timestamptz DEFAULT now(),
    
    UNIQUE (source, source_id)
);
```

**Design Notes:**
- Dual timestamp strategy: `source_*` from export, `created_at/updated_at` for DB tracking
- `source_json` preserves complete original data for debugging/audit
- Unique constraint prevents duplicate imports

#### `raw.messages`

Universal message representation with tree structure support.

```sql
CREATE TABLE raw.messages (
    id                  uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    dialogue_id         uuid NOT NULL REFERENCES raw.dialogues ON DELETE CASCADE,
    source_id           text NOT NULL,             -- platform's message ID
    
    -- Tree structure
    parent_id           uuid REFERENCES raw.messages,
    
    -- Normalized fields
    role                text NOT NULL,             -- 'user', 'assistant', 'system', 'tool'
    author_id           text,                      -- for multi-user scenarios
    author_name         text,
    
    -- Source timestamps
    source_created_at   timestamptz,
    source_updated_at   timestamptz,
    
    -- Change tracking
    content_hash        text,                      -- SHA256 of content for diff
    deleted_at          timestamptz,               -- soft delete marker
    
    source_json         jsonb NOT NULL,
    
    -- Database timestamps
    created_at          timestamptz DEFAULT now(),
    updated_at          timestamptz DEFAULT now(),
    
    UNIQUE (dialogue_id, source_id)
);
```

**Design Notes:**
- Self-referential `parent_id` enables tree structure (for ChatGPT regenerations/edits)
- `content_hash` enables efficient change detection during incremental imports
- `deleted_at` tracks messages removed from source (vs. never existed)

#### `raw.content_parts`

Segmented content within a message (text, code, images, tool use).

```sql
CREATE TABLE raw.content_parts (
    id                  uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    message_id          uuid NOT NULL REFERENCES raw.messages ON DELETE CASCADE,
    sequence            int NOT NULL,              -- ordering within message
    
    part_type           text NOT NULL,             -- 'text', 'code', 'image', 'tool_use', 'tool_result'
    text_content        text,
    
    -- Code-specific
    language            text,                      -- 'python', 'javascript', etc.
    
    -- Media-specific
    media_type          text,                      -- 'image/png', 'audio/mp3'
    url                 text,
    
    -- Tool use (Claude)
    tool_name           text,
    tool_use_id         text,                      -- correlates tool_use with tool_result
    tool_input          jsonb,
    
    -- Timing
    started_at          timestamptz,
    ended_at            timestamptz,
    is_error            boolean DEFAULT false,
    
    source_json         jsonb NOT NULL,
    
    UNIQUE (message_id, sequence)
);
```

**Design Notes:**
- `part_type` discriminates content categories
- `tool_use_id` links tool calls to their results (important for Claude agentic flows)
- `sequence` maintains ordering for multi-part messages

### Platform Extension Tables

#### ChatGPT Extensions

ChatGPT exports include rich metadata about platform features:

```mermaid
erDiagram
    messages ||--o| chatgpt_message_meta : "1:1"
    messages ||--o{ chatgpt_search_groups : "1:N"
    messages ||--o{ chatgpt_code_executions : "1:N"
    messages ||--o{ chatgpt_canvas_docs : "1:N"
    content_parts ||--o{ chatgpt_dalle_generations : "1:N"
    chatgpt_search_groups ||--o{ chatgpt_search_entries : "1:N"
    chatgpt_code_executions ||--o{ chatgpt_code_outputs : "1:N"

    chatgpt_message_meta {
        uuid message_id PK_FK
        text model_slug
        text status
        boolean end_turn
        text gizmo_id
    }
    
    chatgpt_search_groups {
        uuid id PK
        uuid message_id FK
        text group_type
        text domain
    }
    
    chatgpt_code_executions {
        uuid id PK
        uuid message_id FK
        text run_id
        text status
        text code
        text final_output
        text exception_name
    }
```

| Table | Purpose | Key Fields |
|-------|---------|------------|
| `chatgpt_message_meta` | Message-level metadata | `model_slug`, `gizmo_id` |
| `chatgpt_search_groups` | Web search result groups | `domain`, `group_type` |
| `chatgpt_search_entries` | Individual search results | `url`, `title`, `snippet` |
| `chatgpt_code_executions` | Code interpreter runs | `code`, `status`, `exception_name` |
| `chatgpt_code_outputs` | Execution outputs | `output_type`, `text_content`, `image_url` |
| `chatgpt_dalle_generations` | Image generation metadata | `prompt`, `seed`, `edit_op` |
| `chatgpt_canvas_docs` | Canvas document operations | `textdoc_type`, `version` |

#### Claude Extensions

```sql
CREATE TABLE raw.claude_message_meta (
    message_id  uuid PRIMARY KEY REFERENCES raw.messages ON DELETE CASCADE,
    source_json jsonb NOT NULL    -- placeholder for future Claude-specific fields
);
```

**Design Notes:**
- Currently minimal; Claude exports have fewer platform-specific features
- `source_json` preserves any additional metadata for future extraction

---

#### `derived.annotator_cursors`

Tracks incremental processing state for each annotator.

```sql
CREATE TABLE derived.annotator_cursors (
    id                      uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    
    annotator_name          text NOT NULL,
    annotator_version       text NOT NULL,
    entity_type             text NOT NULL,
    
    -- High water mark: last processed entity timestamp
    high_water_mark         timestamptz NOT NULL,
    
    -- Statistics
    entities_processed      int NOT NULL DEFAULT 0,
    annotations_created     int NOT NULL DEFAULT 0,
    
    updated_at              timestamptz DEFAULT now(),
    
    UNIQUE (annotator_name, annotator_version, entity_type)
);
```

**Design Notes:**
- Each annotator+version+entity_type gets its own cursor
- Bumping VERSION in annotator code forces reprocessing

---

## Related Documentation

- [Architecture Overview](architecture.md)
- [Models](models.md) - SQLAlchemy ORM models
- [Extractors](extractors.md) - Data extraction system
- [Builders](builders.md) - Derived data construction



---
File: docs/testing.md
---
# docs/testing.md
# Testing Guide

## Overview

The test suite validates all components of LLM Archive across multiple layers:

- **Unit Tests**: Test individual functions and classes in isolation
- **Integration Tests**: Test component interactions with a real database

## Test Organization

```
tests/
â”œâ”€â”€ conftest.py                 # Shared fixtures
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ conftest.py             # Unit test fixtures
â”‚   â”œâ”€â”€ test_annotators.py      # Annotator logic tests
â”‚   â”œâ”€â”€ test_annotation_utils.py
â”‚   â”œâ”€â”€ test_cli.py             # CLI tests
â”‚   â”œâ”€â”€ test_content_classification.py
â”‚   â”œâ”€â”€ test_exchange_utils.py
â”‚   â”œâ”€â”€ test_extractor_utils.py
â”‚   â”œâ”€â”€ test_hash_utils.py
â”‚   â””â”€â”€ test_models.py          # Model tests
â””â”€â”€ integration/
    â”œâ”€â”€ conftest.py             # Database fixtures
    â”œâ”€â”€ test_annotators.py      # Annotator integration
    â”œâ”€â”€ test_builders.py        # Builder integration
    â”œâ”€â”€ test_extractors.py      # Extractor integration
    â”œâ”€â”€ test_idempotency.py     # Incremental import tests
    â””â”€â”€ test_models.py          # Model persistence tests
```

## Running Tests

### All Tests

```bash
# Run all tests
pytest

# With verbose output
pytest -v

# With coverage
pytest --cov=llm_archive --cov-report=html
```

### Unit Tests Only

```bash
# Unit tests don't require database
pytest tests/unit/ -v
```

### Integration Tests Only

```bash
# Requires PostgreSQL running
pytest tests/integration/ -v
```

### Specific Test File

```bash
pytest tests/unit/test_annotators.py -v
```

### Specific Test Class or Function

```bash
# Specific class
pytest tests/unit/test_annotators.py::TestCodeBlockAnnotator -v

# Specific test
pytest tests/unit/test_annotators.py::TestCodeBlockAnnotator::test_detects_code_block_with_language -v
```

---

## Unit Tests

Unit tests verify component logic without database dependencies.

### Annotator Tests

Tests validate the `annotate()` method by constructing data objects directly:

```python
# tests/unit/test_annotators.py

class TestCodeBlockAnnotator:
    """Test CodeBlockAnnotator (priority 90)."""
    
    def test_detects_code_block_with_language(self, message_id):
        """Should detect code blocks with language specification."""
        text = "```python\ndef hello():\n    print('world')\n```"
        data = make_message_data(text, message_id=message_id)
        
        # Create instance without session (we only test annotate())
        annotator = CodeBlockAnnotator.__new__(CodeBlockAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) >= 1
        assert 'python' in results[0].data['languages']
```

### Helper Functions

```python
def make_message_data(text: str, role: str = 'assistant', message_id=None) -> MessageTextData:
    """Helper to create MessageTextData for testing."""
    return MessageTextData(
        message_id=message_id or uuid4(),
        text=text,
        created_at=datetime.now(timezone.utc),
        role=role,
    )

def make_exchange_data(
    user_text: str | None = None,
    assistant_text: str | None = None,
) -> ExchangeData:
    """Helper to create ExchangeData for testing."""
    return ExchangeData(
        exchange_id=uuid4(),
        user_text=user_text,
        assistant_text=assistant_text,
        user_word_count=len(user_text.split()) if user_text else None,
        assistant_word_count=len(assistant_text.split()) if assistant_text else None,
        created_at=datetime.now(timezone.utc),
    )
```

### Model Tests

Test SQLAlchemy model instantiation and field defaults:

```python
class TestDialogueModel:
    def test_create_dialogue_instance(self):
        """Test creating a Dialogue model instance."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='test-123',
            title='Test Dialogue',
            source_json={'key': 'value'},
        )
        
        assert dialogue.source == 'chatgpt'
        assert dialogue.source_id == 'test-123'
        assert dialogue.id is None  # Set by database
```

---

## Integration Tests

Integration tests verify component interactions with a real PostgreSQL database.

### Database Fixtures

```python
# tests/integration/conftest.py

@pytest.fixture(scope='session')
def database():
    """Create test database."""
    # Create database
    create_test_database()
    
    yield
    
    # Cleanup
    drop_test_database()

@pytest.fixture
def session(database):
    """Create database session with rollback."""
    engine = create_engine(TEST_DATABASE_URL)
    Session = sessionmaker(bind=engine)
    session = Session()
    
    yield session
    
    session.rollback()
    session.close()
```

### Extractor Tests

```python
class TestChatGPTExtractor:
    def test_import_simple_conversation(self, session, sample_chatgpt_export):
        """Test importing a simple ChatGPT conversation."""
        extractor = ChatGPTExtractor(session)
        
        count = extractor.extract(sample_chatgpt_export)
        
        assert count == 1
        
        dialogue = session.query(Dialogue).first()
        assert dialogue is not None
        assert dialogue.source == 'chatgpt'
        assert len(dialogue.messages) > 0
```

### Builder Tests

```python
class TestTreeBuilder:
    def test_build_linear_tree(self, session, dialogue_with_messages):
        """Test tree analysis for linear conversation."""
        builder = TreeBuilder(session)
        
        results = builder.build(dialogue_with_messages.id)
        
        tree = session.query(DialogueTree).get(dialogue_with_messages.id)
        assert tree is not None
        assert tree.branch_count == 0  # Linear
        assert tree.is_linear  # Generated column
```

### Annotator Integration Tests

```python
class TestAnnotatorIntegration:
    def test_full_annotation_pipeline(self, session, populated_database):
        """Test running full annotation pipeline."""
        manager = AnnotationManager(session)
        manager.register(CodeBlockAnnotator)
        manager.register(WikiLinkAnnotator)
        manager.register(ExchangeTypeAnnotator)
        
        results = manager.run_all()
        
        assert all(count >= 0 for count in results.values())
        
        # Verify annotations were created
        annotations = session.query(Annotation).count()
        assert annotations > 0
```

---

## Test Fixtures

### Sample Data Fixtures

```python
# tests/conftest.py

@pytest.fixture
def sample_chatgpt_conversation():
    """Sample ChatGPT conversation data."""
    return {
        'id': 'test-conv-123',
        'title': 'Test Conversation',
        'create_time': 1699900000.0,
        'update_time': 1699900100.0,
        'mapping': {
            'root': {
                'id': 'root',
                'parent': None,
                'children': ['msg-1'],
                'message': {
                    'id': 'root-msg',
                    'author': {'role': 'system'},
                    'content': {'content_type': 'text', 'parts': ['']},
                }
            },
            'msg-1': {
                'id': 'msg-1',
                'parent': 'root',
                'children': ['msg-2'],
                'message': {
                    'id': 'user-msg-1',
                    'author': {'role': 'user'},
                    'content': {'content_type': 'text', 'parts': ['Hello!']},
                }
            },
            'msg-2': {
                'id': 'msg-2',
                'parent': 'msg-1',
                'children': [],
                'message': {
                    'id': 'asst-msg-1',
                    'author': {'role': 'assistant'},
                    'content': {'content_type': 'text', 'parts': ['Hi! How can I help?']},
                }
            },
        },
        'current_node': 'msg-2',
    }

@pytest.fixture
def sample_code_message():
    """Sample message with code block."""
    return MessageTextData(
        message_id=uuid4(),
        text='Here is the code:\n```python\nprint("hello")\n```',
        created_at=datetime.now(timezone.utc),
        role='assistant',
    )
```

### Database Population Fixtures

```python
@pytest.fixture
def populated_database(session):
    """Database with sample dialogues, messages, and exchanges."""
    # Create dialogue
    dialogue = Dialogue(
        source='chatgpt',
        source_id='test-1',
        title='Test',
        source_json={},
    )
    session.add(dialogue)
    session.flush()
    
    # Create messages
    messages = create_test_messages(dialogue.id, session)
    
    # Build derived structures
    tree_builder = TreeBuilder(session)
    tree_builder.build(dialogue.id)
    
    exchange_builder = ExchangeBuilder(session)
    exchange_builder.build(dialogue.id)
    
    session.commit()
    
    yield dialogue
```

---

## Test Patterns

### Testing Annotator Logic

```python
def test_annotator_logic():
    """Test annotator without database."""
    # 1. Create data object
    data = MessageTextData(...)
    
    # 2. Create annotator without session
    annotator = MyAnnotator.__new__(MyAnnotator)
    
    # 3. Call annotate directly
    results = annotator.annotate(data)
    
    # 4. Assert on results
    assert len(results) == 1
    assert results[0].value == 'expected'
```

### Testing Incremental Processing

```python
def test_incremental_import(session):
    """Test that incremental import updates correctly."""
    extractor = ChatGPTExtractor(session)
    
    # First import
    count1 = extractor.extract(file_path, mode='full')
    
    # Modify export file
    modify_export_file(file_path)
    
    # Incremental import
    count2 = extractor.extract(file_path, mode='incremental')
    
    # Should detect changes
    assert count2 > 0
```

### Testing Strategy Priority

```python
def test_strategy_priority():
    """Test that annotators run in priority order."""
    manager = AnnotationManager(session)
    
    # Register in random order
    manager.register(LowPriorityAnnotator)    # priority=30
    manager.register(HighPriorityAnnotator)   # priority=90
    manager.register(MedPriorityAnnotator)    # priority=50
    
    # Verify sorted order
    sorted_annotators = manager._sorted_annotators()
    priorities = [a.PRIORITY for a in sorted_annotators]
    
    assert priorities == sorted(priorities, reverse=True)
```

---

## Mocking

### Mock Database Session

```python
from unittest.mock import MagicMock, patch

def test_with_mock_session():
    """Test with mocked session."""
    mock_session = MagicMock()
    mock_session.query.return_value.filter.return_value.all.return_value = []
    
    annotator = MyAnnotator(mock_session)
    result = annotator.compute()
    
    assert result == 0
```

### Mock File System

```python
def test_import_with_mock_file(tmp_path):
    """Test import with temporary file."""
    # Create temp file
    export_file = tmp_path / "conversations.json"
    export_file.write_text(json.dumps([sample_conversation]))
    
    # Import
    extractor = ChatGPTExtractor(session)
    count = extractor.extract(export_file)
    
    assert count == 1
```

---

## Coverage

### Running with Coverage

```bash
# Generate coverage report
pytest --cov=llm_archive --cov-report=html

# View report
open htmlcov/index.html
```

### Coverage Requirements

- Unit tests: High coverage for business logic
- Integration tests: Cover happy paths and edge cases
- Minimum target: 80% line coverage

---

## Continuous Integration

### GitHub Actions Example

```yaml
# .github/workflows/test.yml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: pgvector/pgvector:pg16
        env:
          POSTGRES_DB: test_llm_archive
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        ports:
          - 5432:5432
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      
      - name: Install dependencies
        run: |
          pip install -e ".[dev]"
      
      - name: Run tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_llm_archive
        run: |
          pytest --cov=llm_archive --cov-report=xml
      
      - name: Upload coverage
        uses: codecov/codecov-action@v4
```

---

## Related Documentation

- [Architecture Overview](architecture.md)
- [Annotators](annotators.md) - Annotator testing patterns
- [CLI Reference](cli.md) - CLI testing


---
File: llm_archive/__init__.py
---
# llm_archive/__init__.py
"""LLM conversation archive ingestion and analysis."""

from llm_archive.config import DATABASE_URL

__version__ = "0.1.0"
__all__ = ["DATABASE_URL", "__version__"]


---
File: llm_archive/annotations/__init__.py
---
#from .core import AnnotationWriter, AnnotationReader, enums



---
File: llm_archive/annotations/core.py
---
# llm_archive/annotations/core.py
"""
Core annotation infrastructure for typed annotation tables.

This module provides:
- Enum types for annotation value types and entity types
- AnnotationWriter for inserting annotations during ingestion/processing
- Base classes for annotators that iterate over entities

The schema uses separate tables per (entity_type, value_type) combination:
- derived.{entity}_annotations_{value_type}
- e.g., derived.message_annotations_string, derived.prompt_response_annotations_flag
"""

from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any
from uuid import UUID

from sqlalchemy import text
from sqlalchemy.orm import Session
from loguru import logger


# ============================================================
# Enums
# ============================================================

class EntityType(str, Enum):
    """Supported entity types for annotations."""
    CONTENT_PART = 'content_part'
    MESSAGE = 'message'
    PROMPT_RESPONSE = 'prompt_response'
    DIALOGUE = 'dialogue'


class ValueType(str, Enum):
    """Annotation value types."""
    FLAG = 'flag'       # Key presence = true, no value
    STRING = 'string'   # Text value
    NUMERIC = 'numeric' # Numeric value
    JSON = 'json'       # JSONB value


# ============================================================
# Annotation Result (returned by annotators)
# ============================================================

@dataclass
class AnnotationResult:
    """
    Result from annotation logic.
    
    For FLAG annotations: only key is required
    For STRING/NUMERIC/JSON: key and value are required
    """
    key: str
    value: Any = None  # None for flags, typed for others
    value_type: ValueType = ValueType.STRING  # Inferred if not specified
    confidence: float | None = None
    reason: str | None = None
    source: str = 'heuristic'
    source_version: str | None = None
    
    def __eq__(self, other: object) -> bool:
        """Equality check - compares key, value, and value_type."""
        if not isinstance(other, AnnotationResult):
            return NotImplemented
        return (
            self.key == other.key 
            and self.value == other.value 
            and self.value_type == other.value_type
        )
    
    def __hash__(self) -> int:
        """Hash based on key, value, and value_type."""
        # Convert value to a hashable form if it's a dict/list
        value_hash = self.value
        if isinstance(self.value, dict):
            value_hash = tuple(sorted(self.value.items()))
        elif isinstance(self.value, list):
            value_hash = tuple(self.value)
        return hash((self.key, value_hash, self.value_type))
    
    def __repr__(self) -> str:
        """Compact string representation."""
        if self.value_type == ValueType.FLAG:
            return f"AnnotationResult({self.key!r}, FLAG)"
        return f"AnnotationResult({self.key!r}, {self.value!r}, {self.value_type.value})"


# ============================================================
# Annotation Writer (for ingestion and annotators)
# ============================================================

class AnnotationWriter:
    """
    Writes annotations to the appropriate typed tables.
    
    Used by:
    - Extractors during ingestion (source='ingestion')
    - Annotators during processing (source='heuristic', 'model', etc.)
    
    Handles table routing based on entity_type and value_type.
    Uses upsert semantics (ON CONFLICT DO NOTHING for multi-value,
    ON CONFLICT DO UPDATE for single-value tables).
    """
    
    # Table name templates
    TABLE_TEMPLATE = "derived.{entity}_annotations_{value_type}"
    
    # Tables where (entity_id, key) is unique (single value per key)
    SINGLE_VALUE_TABLES = {ValueType.FLAG, ValueType.JSON}
    
    def __init__(self, session: Session):
        self.session = session
        self._counts: dict[str, int] = {}
    
    def _table_name(self, entity_type: EntityType, value_type: ValueType) -> str:
        """Get the table name for an entity/value type combination."""
        return self.TABLE_TEMPLATE.format(
            entity=entity_type.value,
            value_type=value_type.value,
        )
    
    def write_flag(
        self,
        entity_type: EntityType,
        entity_id: UUID,
        key: str,
        confidence: float | None = None,
        reason: str | None = None,
        source: str = 'heuristic',
        source_version: str | None = None,
    ) -> bool:
        """Write a flag annotation (key presence = true)."""
        table = self._table_name(entity_type, ValueType.FLAG)
        
        result = self.session.execute(
            text(f"""
                INSERT INTO {table} 
                    (entity_id, annotation_key, confidence, reason, source, source_version)
                VALUES 
                    (:entity_id, :key, :confidence, :reason, :source, :source_version)
                ON CONFLICT (entity_id, annotation_key) DO NOTHING
                RETURNING id
            """),
            {
                'entity_id': entity_id,
                'key': key,
                'confidence': confidence,
                'reason': reason,
                'source': source,
                'source_version': source_version,
            }
        )
        created = result.scalar() is not None
        self._track(table, created)
        return created
    
    def write_string(
        self,
        entity_type: EntityType,
        entity_id: UUID,
        key: str,
        value: str,
        confidence: float | None = None,
        reason: str | None = None,
        source: str = 'heuristic',
        source_version: str | None = None,
    ) -> bool:
        """Write a string annotation."""
        table = self._table_name(entity_type, ValueType.STRING)
        
        result = self.session.execute(
            text(f"""
                INSERT INTO {table} 
                    (entity_id, annotation_key, annotation_value, confidence, reason, source, source_version)
                VALUES 
                    (:entity_id, :key, :value, :confidence, :reason, :source, :source_version)
                ON CONFLICT (entity_id, annotation_key, annotation_value) DO NOTHING
                RETURNING id
            """),
            {
                'entity_id': entity_id,
                'key': key,
                'value': value,
                'confidence': confidence,
                'reason': reason,
                'source': source,
                'source_version': source_version,
            }
        )
        created = result.scalar() is not None
        self._track(table, created)
        return created
    
    def write_numeric(
        self,
        entity_type: EntityType,
        entity_id: UUID,
        key: str,
        value: float | int,
        confidence: float | None = None,
        reason: str | None = None,
        source: str = 'heuristic',
        source_version: str | None = None,
    ) -> bool:
        """Write a numeric annotation."""
        table = self._table_name(entity_type, ValueType.NUMERIC)
        
        result = self.session.execute(
            text(f"""
                INSERT INTO {table} 
                    (entity_id, annotation_key, annotation_value, confidence, reason, source, source_version)
                VALUES 
                    (:entity_id, :key, :value, :confidence, :reason, :source, :source_version)
                ON CONFLICT (entity_id, annotation_key, annotation_value) DO NOTHING
                RETURNING id
            """),
            {
                'entity_id': entity_id,
                'key': key,
                'value': value,
                'confidence': confidence,
                'reason': reason,
                'source': source,
                'source_version': source_version,
            }
        )
        created = result.scalar() is not None
        self._track(table, created)
        return created
    
    def write_json(
        self,
        entity_type: EntityType,
        entity_id: UUID,
        key: str,
        value: dict | list,
        confidence: float | None = None,
        reason: str | None = None,
        source: str = 'heuristic',
        source_version: str | None = None,
    ) -> bool:
        """Write a JSON annotation (single value per key, upserts)."""
        import json
        table = self._table_name(entity_type, ValueType.JSON)
        
        result = self.session.execute(
            text(f"""
                INSERT INTO {table} 
                    (entity_id, annotation_key, annotation_value, confidence, reason, source, source_version)
                VALUES 
                    (:entity_id, :key, CAST(:value AS jsonb), :confidence, :reason, :source, :source_version)
                ON CONFLICT (entity_id, annotation_key) DO UPDATE SET
                    annotation_value = EXCLUDED.annotation_value,
                    confidence = EXCLUDED.confidence,
                    reason = EXCLUDED.reason,
                    source = EXCLUDED.source,
                    source_version = EXCLUDED.source_version,
                    created_at = now()
                RETURNING id
            """),
            {
                'entity_id': entity_id,
                'key': key,
                'value': json.dumps(value),
                'confidence': confidence,
                'reason': reason,
                'source': source,
                'source_version': source_version,
            }
        )
        created = result.scalar() is not None
        self._track(table, created)
        return created
    
    def write(self, entity_type: EntityType, entity_id: UUID, result: AnnotationResult) -> bool:
        """
        Write an annotation from an AnnotationResult.
        
        Dispatches to the appropriate typed write method.
        """
        if result.value_type == ValueType.FLAG:
            return self.write_flag(
                entity_type=entity_type,
                entity_id=entity_id,
                key=result.key,
                confidence=result.confidence,
                reason=result.reason,
                source=result.source,
                source_version=result.source_version,
            )
        elif result.value_type == ValueType.STRING:
            return self.write_string(
                entity_type=entity_type,
                entity_id=entity_id,
                key=result.key,
                value=str(result.value),
                confidence=result.confidence,
                reason=result.reason,
                source=result.source,
                source_version=result.source_version,
            )
        elif result.value_type == ValueType.NUMERIC:
            return self.write_numeric(
                entity_type=entity_type,
                entity_id=entity_id,
                key=result.key,
                value=float(result.value),
                confidence=result.confidence,
                reason=result.reason,
                source=result.source,
                source_version=result.source_version,
            )
        elif result.value_type == ValueType.JSON:
            return self.write_json(
                entity_type=entity_type,
                entity_id=entity_id,
                key=result.key,
                value=result.value,
                confidence=result.confidence,
                reason=result.reason,
                source=result.source,
                source_version=result.source_version,
            )
        else:
            raise ValueError(f"Unknown value type: {result.value_type}")
    
    def _track(self, table: str, created: bool):
        """Track annotation counts."""
        if table not in self._counts:
            self._counts[table] = 0
        if created:
            self._counts[table] += 1
    
    @property
    def counts(self) -> dict[str, int]:
        """Get annotation counts by table."""
        return self._counts.copy()


# ============================================================
# Annotation Reader (for querying)
# ============================================================

class AnnotationReader:
    """
    Reads annotations from the typed tables.
    
    Provides methods for:
    - Checking if an annotation exists
    - Getting annotation values
    - Filtering entities by annotations
    """
    
    TABLE_TEMPLATE = "derived.{entity}_annotations_{value_type}"
    
    def __init__(self, session: Session):
        self.session = session
    
    def _table_name(self, entity_type: EntityType, value_type: ValueType) -> str:
        return self.TABLE_TEMPLATE.format(
            entity=entity_type.value,
            value_type=value_type.value,
        )
    
    def has_flag(self, entity_type: EntityType, entity_id: UUID, key: str) -> bool:
        """Check if entity has a flag annotation."""
        table = self._table_name(entity_type, ValueType.FLAG)
        result = self.session.execute(
            text(f"SELECT 1 FROM {table} WHERE entity_id = :id AND annotation_key = :key"),
            {'id': entity_id, 'key': key}
        )
        return result.scalar() is not None
    
    def get_string(self, entity_type: EntityType, entity_id: UUID, key: str) -> list[str]:
        """Get all string values for a key (multi-value)."""
        table = self._table_name(entity_type, ValueType.STRING)
        result = self.session.execute(
            text(f"SELECT annotation_value FROM {table} WHERE entity_id = :id AND annotation_key = :key"),
            {'id': entity_id, 'key': key}
        )
        return [row[0] for row in result]
    
    def get_string_single(self, entity_type: EntityType, entity_id: UUID, key: str) -> str | None:
        """Get single string value (returns first if multiple)."""
        values = self.get_string(entity_type, entity_id, key)
        return values[0] if values else None
    
    def get_numeric(self, entity_type: EntityType, entity_id: UUID, key: str) -> list[float]:
        """Get all numeric values for a key."""
        table = self._table_name(entity_type, ValueType.NUMERIC)
        result = self.session.execute(
            text(f"SELECT annotation_value FROM {table} WHERE entity_id = :id AND annotation_key = :key"),
            {'id': entity_id, 'key': key}
        )
        return [float(row[0]) for row in result]
    
    def get_json(self, entity_type: EntityType, entity_id: UUID, key: str) -> dict | list | None:
        """Get JSON value for a key (single value)."""
        table = self._table_name(entity_type, ValueType.JSON)
        result = self.session.execute(
            text(f"SELECT annotation_value FROM {table} WHERE entity_id = :id AND annotation_key = :key"),
            {'id': entity_id, 'key': key}
        )
        row = result.fetchone()
        return row[0] if row else None
    
    def get_all_keys(self, entity_type: EntityType, entity_id: UUID) -> dict[str, list[Any]]:
        """Get all annotations for an entity, grouped by key."""
        all_table = f"derived.{entity_type.value}_annotations_all"
        result = self.session.execute(
            text(f"SELECT annotation_key, annotation_value, value_type FROM {all_table} WHERE entity_id = :id"),
            {'id': entity_id}
        )
        
        annotations: dict[str, list[Any]] = {}
        for key, value, value_type in result:
            if key not in annotations:
                annotations[key] = []
            if value_type == 'flag':
                annotations[key].append(True)
            else:
                annotations[key].append(value)
        
        return annotations
    
    def find_entities_with_flag(self, entity_type: EntityType, key: str) -> list[UUID]:
        """Find all entity IDs that have a specific flag."""
        table = self._table_name(entity_type, ValueType.FLAG)
        result = self.session.execute(
            text(f"SELECT entity_id FROM {table} WHERE annotation_key = :key"),
            {'key': key}
        )
        return [row[0] for row in result]
    
    def find_entities_with_string(
        self, 
        entity_type: EntityType, 
        key: str, 
        value: str | None = None
    ) -> list[UUID]:
        """Find entity IDs with a string annotation (optionally matching value)."""
        table = self._table_name(entity_type, ValueType.STRING)
        if value is not None:
            result = self.session.execute(
                text(f"SELECT entity_id FROM {table} WHERE annotation_key = :key AND annotation_value = :value"),
                {'key': key, 'value': value}
            )
        else:
            result = self.session.execute(
                text(f"SELECT DISTINCT entity_id FROM {table} WHERE annotation_key = :key"),
                {'key': key}
            )
        return [row[0] for row in result]



---
File: llm_archive/annotators/__init__.py
---
# llm_archive/annotators/__init__.py
"""Annotation infrastructure for entities.

Annotators analyze entities and produce annotations stored in derived.annotations.

Architecture Overview:
---------------------

**Annotation Keys vs Annotators (Strategy Pattern)**

An ANNOTATION_KEY identifies what we're trying to detect (e.g., 'code', 'latex').
Multiple annotators can target the same key using different strategies.
Higher PRIORITY annotators run first; lower-priority ones can be skipped
if the key is already satisfied.

Example: Detecting code in an exchange
  - ChatGPTCodeExecutionAnnotator (priority=100): Platform ground truth
  - CodeBlockAnnotator (priority=90): Explicit ``` blocks  
  - CodeStructureAnnotator (priority=70): Function/class patterns
  - CodeKeywordDensityAnnotator (priority=30): Keyword density

If code execution is detected (priority 100), lower-priority heuristics
can check has_annotation_key() to skip redundant work.

**Annotation Types**
- tag: For filtering (topic:physics, quality:high)
- feature: Detected features (has_code_blocks, has_latex)
- metadata: Structural data (dialogue_length, prompt_stats)
- title: Generated titles
- summary: Brief descriptions

**Entity Types**
- message: Individual messages
- exchange: User prompt + assistant response pair
- dialogue: Entire conversation

Creating Custom Annotators:
--------------------------

For MESSAGE annotations based on text content:

    class MyMessageAnnotator(MessageTextAnnotator):
        ANNOTATION_TYPE = 'feature'
        ANNOTATION_KEY = 'my_feature'  # What we're detecting
        PRIORITY = 50                   # When to run (higher = first)
        VERSION = '1.0'
        ROLE_FILTER = 'assistant'       # or 'user' or None for all
        
        def annotate(self, data: MessageTextData) -> list[AnnotationResult]:
            if 'keyword' in data.text:
                return [AnnotationResult(value='has_keyword', confidence=0.9)]
            return []

For EXCHANGE annotations based on content:

    class MyExchangeAnnotator(ExchangeAnnotator):
        ANNOTATION_TYPE = 'tag'
        ANNOTATION_KEY = 'my_tag'
        PRIORITY = 50
        VERSION = '1.0'
        
        def annotate(self, data: ExchangeData) -> list[AnnotationResult]:
            if (data.assistant_word_count or 0) > 1000:
                return [AnnotationResult(value='long_response', key='length')]
            return []

For EXCHANGE annotations based on platform features (e.g., ChatGPT):

    class MyChatGPTAnnotator(ExchangePlatformAnnotator):
        ANNOTATION_TYPE = 'feature'
        ANNOTATION_KEY = 'platform_feature'
        PRIORITY = 100  # Platform = ground truth
        VERSION = '1.0'
        
        def annotate(self, data: ExchangePlatformData) -> list[AnnotationResult]:
            # Query platform tables using data.message_ids
            ...

For DIALOGUE annotations with aggregate statistics:

    class MyDialogueAnnotator(DialogueAnnotator):
        ANNOTATION_TYPE = 'metadata'
        ANNOTATION_KEY = 'my_stats'
        PRIORITY = 50
        VERSION = '1.0'
        
        def annotate(self, data: DialogueData) -> list[AnnotationResult]:
            if data.exchange_count > 10:
                return [AnnotationResult(value='extended', key='length')]
            return []

Priority Guidelines:
- 100: Platform features (ground truth from database)
- 90: Explicit syntax (```, shebangs)
- 70: Structural patterns (function definitions)
- 50: Keyword detection (default)
- 30: Density/heuristic analysis

Bump VERSION to reprocess all entities with new logic.
"""

from llm_archive.annotators.prompt_response import (
    PromptResponseAnnotator,
    PromptResponseData,
    WikiCandidateAnnotator,
    NaiveTitleAnnotator,
)

__all__ = [

    # Prompt-response annotators
    "PromptResponseAnnotator",
    "PromptResponseData",
    "WikiCandidateAnnotator",
    "NaiveTitleAnnotator",
]



---
File: llm_archive/annotators/content_part.py
---
# llm_archive/annotators/content_part.py
"""
Content-part level annotators.

These annotators work on individual content_parts within messages,
detecting features like code blocks, LaTeX, and other content types.
"""

import re
from abc import abstractmethod
from dataclasses import dataclass
from datetime import datetime
from typing import Iterator
from uuid import UUID

from sqlalchemy import text
from sqlalchemy.orm import Session

from llm_archive.annotations.core import (
    AnnotationWriter,
    AnnotationReader,
    AnnotationResult,
    EntityType,
    ValueType,
)


@dataclass
class ContentPartData:
    """Data passed to content-part annotation logic."""
    content_part_id: UUID
    message_id: UUID
    dialogue_id: UUID
    sequence: int
    part_type: str
    text_content: str | None
    language: str | None
    role: str
    created_at: datetime | None


class ContentPartAnnotator:
    """
    Base class for annotating content parts.
    
    Iterates over raw.content_parts joined with raw.messages.
    Supports annotation prerequisites and skip conditions.
    """
    
    ENTITY_TYPE = EntityType.CONTENT_PART
    ANNOTATION_KEY: str = ''  # Subclass must define
    VALUE_TYPE: ValueType = ValueType.FLAG
    PRIORITY: int = 50  # Higher = runs first
    
    # Annotation filters
    REQUIRES_FLAGS: list[str] = []
    REQUIRES_STRINGS: list[tuple[str, str]] = []
    SKIP_IF_FLAGS: list[str] = []
    SKIP_IF_STRINGS: list[tuple[str, ...]] = []
    
    # Content filters
    PART_TYPE_FILTER: str | None = None  # Limit to specific part_type
    ROLE_FILTER: str | None = None  # Limit to 'user' or 'assistant'
    
    def __init__(self, session: Session):
        self.session = session
        self.writer = AnnotationWriter(session)
        self.reader = AnnotationReader(session)
    
    def compute(self) -> int:
        """Run annotation over content parts."""
        count = 0
        for data in self._iter_content_parts():
            results = self.annotate(data)
            for result in results:
                if self._write_result(data.content_part_id, result):
                    count += 1
        return count
    
    def _write_result(self, entity_id: UUID, result: AnnotationResult) -> bool:
        """Write an annotation result to the appropriate table."""
        return self.writer.write(EntityType.CONTENT_PART, entity_id, result)
    
    def _iter_content_parts(self) -> Iterator[ContentPartData]:
        """Iterate over content parts, respecting filters."""
        # Build base query
        query = """
            SELECT 
                cp.id as content_part_id,
                cp.message_id,
                m.dialogue_id,
                cp.sequence,
                cp.part_type,
                cp.text_content,
                cp.language,
                m.role,
                m.created_at
            FROM raw.content_parts cp
            JOIN raw.messages m ON m.id = cp.message_id
            WHERE m.deleted_at IS NULL
        """
        
        params = {}
        
        # Add part_type filter
        if self.PART_TYPE_FILTER:
            query += " AND cp.part_type = :part_type"
            params['part_type'] = self.PART_TYPE_FILTER
        
        # Add role filter
        if self.ROLE_FILTER:
            query += " AND m.role = :role"
            params['role'] = self.ROLE_FILTER
        
        query += " ORDER BY m.dialogue_id, m.created_at, cp.sequence"
        
        result = self.session.execute(text(query), params)
        
        for row in result:
            yield ContentPartData(
                content_part_id=row.content_part_id,
                message_id=row.message_id,
                dialogue_id=row.dialogue_id,
                sequence=row.sequence,
                part_type=row.part_type,
                text_content=row.text_content,
                language=row.language,
                role=row.role,
                created_at=row.created_at,
            )
    
    @abstractmethod
    def annotate(self, data: ContentPartData) -> list[AnnotationResult]:
        """
        Analyze content part and return annotations to create.
        
        Args:
            data: ContentPartData with content and metadata
            
        Returns:
            List of AnnotationResult objects (empty list if no match)
        """
        pass


# ============================================================
# Code Detection Annotators
# ============================================================

class CodeBlockAnnotator(ContentPartAnnotator):
    """
    Detect explicit code blocks (```) in text content parts.
    
    Highest priority code detector - explicit markdown code blocks
    are the most reliable signal.
    
    Produces:
    - has_code_block FLAG
    - code_block_count NUMERIC
    - code_languages STRING (multi-value)
    """
    
    ANNOTATION_KEY = 'has_code_block'
    VALUE_TYPE = ValueType.FLAG
    PRIORITY = 90
    PART_TYPE_FILTER = 'text'
    ROLE_FILTER = 'assistant'
    
    # Pattern to match complete code blocks: ```lang\n...content...```
    # Captures the optional language specifier
    CODE_BLOCK_PATTERN = re.compile(r'```(\w*)\n?[\s\S]*?```')
    
    def annotate(self, data: ContentPartData) -> list[AnnotationResult]:
        if not data.text_content:
            return []
        
        # Find all complete code blocks
        matches = list(self.CODE_BLOCK_PATTERN.finditer(data.text_content))
        
        if not matches:
            return []
        
        results = []
        
        # Flag annotation
        results.append(AnnotationResult(
            key='has_code_block',
            value_type=ValueType.FLAG,
            reason='markdown_code_block',
            confidence=1.0,
        ))
        
        # Count annotation
        block_count = len(matches)
        results.append(AnnotationResult(
            key='code_block_count',
            value=block_count,
            value_type=ValueType.NUMERIC,
        ))
        
        # Language annotations (multi-value)
        languages = set()
        for match in matches:
            lang = match.group(1).lower()
            if lang:  # Skip empty language specs
                languages.add(lang)
        
        for lang in languages:
            results.append(AnnotationResult(
                key='code_language',
                value=lang,
                value_type=ValueType.STRING,
                reason='code_block_language_spec',
            ))
        
        return results


class ScriptHeaderAnnotator(ContentPartAnnotator):
    """
    Detect script headers and system includes (strong code evidence).
    
    Shebangs and #include are unambiguous code markers.
    
    Produces:
    - has_script_header FLAG
    - script_type STRING
    """
    
    ANNOTATION_KEY = 'has_script_header'
    VALUE_TYPE = ValueType.FLAG
    PRIORITY = 85
    PART_TYPE_FILTER = 'text'
    
    SHEBANG_PATTERN = re.compile(r'^#!\s*/(?:usr/)?bin/(?:env\s+)?(\w+)', re.MULTILINE)
    INCLUDE_PATTERN = re.compile(r'^#include\s*[<"]', re.MULTILINE)
    PHP_PATTERN = re.compile(r'<\?php', re.IGNORECASE)
    
    def annotate(self, data: ContentPartData) -> list[AnnotationResult]:
        if not data.text_content:
            return []
        
        results = []
        
        # Check for shebang
        shebang_match = self.SHEBANG_PATTERN.search(data.text_content)
        if shebang_match:
            results.append(AnnotationResult(
                key='has_script_header',
                value_type=ValueType.FLAG,
                reason='shebang',
                confidence=1.0,
            ))
            results.append(AnnotationResult(
                key='script_type',
                value=shebang_match.group(1),
                value_type=ValueType.STRING,
                reason='shebang_interpreter',
            ))
            return results
        
        # Check for C/C++ includes
        if self.INCLUDE_PATTERN.search(data.text_content):
            results.append(AnnotationResult(
                key='has_script_header',
                value_type=ValueType.FLAG,
                reason='c_include',
                confidence=1.0,
            ))
            results.append(AnnotationResult(
                key='script_type',
                value='c',
                value_type=ValueType.STRING,
            ))
            return results
        
        # Check for PHP
        if self.PHP_PATTERN.search(data.text_content):
            results.append(AnnotationResult(
                key='has_script_header',
                value_type=ValueType.FLAG,
                reason='php_tag',
                confidence=1.0,
            ))
            results.append(AnnotationResult(
                key='script_type',
                value='php',
                value_type=ValueType.STRING,
            ))
            return results
        
        return results


# ============================================================
# Content Type Annotators
# ============================================================

class LatexContentAnnotator(ContentPartAnnotator):
    """
    Detect LaTeX/MathJax mathematical notation in content parts.
    
    Produces:
    - has_latex FLAG
    - latex_type STRING ('display', 'inline', 'commands')
    """
    
    ANNOTATION_KEY = 'has_latex'
    VALUE_TYPE = ValueType.FLAG
    PRIORITY = 70
    PART_TYPE_FILTER = 'text'
    ROLE_FILTER = 'assistant'
    
    # Display math: $$ ... $$ or \[ ... \]
    DISPLAY_MATH_PATTERN = re.compile(r'\$\$.+?\$\$|\\\[.+?\\\]', re.DOTALL)
    
    # Inline math: $ ... $ (but not $$)
    INLINE_MATH_PATTERN = re.compile(r'(?<!\$)\$(?!\$).+?(?<!\$)\$(?!\$)')
    
    # LaTeX commands: \frac, \sum, \int, etc.
    LATEX_COMMANDS_PATTERN = re.compile(
        r'\\(?:frac|sum|int|prod|lim|sqrt|begin|end|alpha|beta|gamma|'
        r'delta|epsilon|theta|lambda|sigma|omega|pi|infty|partial|nabla|'
        r'mathbb|mathcal|mathbf|mathrm|text|left|right|cdot|times|div)'
    )
    
    def annotate(self, data: ContentPartData) -> list[AnnotationResult]:
        if not data.text_content:
            return []
        
        results = []
        latex_types = set()
        
        if self.DISPLAY_MATH_PATTERN.search(data.text_content):
            latex_types.add('display')
        
        if self.INLINE_MATH_PATTERN.search(data.text_content):
            latex_types.add('inline')
        
        if self.LATEX_COMMANDS_PATTERN.search(data.text_content):
            latex_types.add('commands')
        
        if not latex_types:
            return []
        
        # Main flag
        results.append(AnnotationResult(
            key='has_latex',
            value_type=ValueType.FLAG,
            confidence=0.95 if 'display' in latex_types else 0.8,
            reason='latex_notation_detected',
        ))
        
        # Type annotations
        for latex_type in latex_types:
            results.append(AnnotationResult(
                key='latex_type',
                value=latex_type,
                value_type=ValueType.STRING,
            ))
        
        return results


class WikiLinkContentAnnotator(ContentPartAnnotator):
    """
    Detect Obsidian-style [[wiki links]] in content parts.
    
    This is a content-part level version for granular detection.
    The prompt-response level WikiCandidateAnnotator aggregates this.
    
    Produces:
    - has_wiki_links FLAG
    - wiki_link_count NUMERIC
    """
    
    ANNOTATION_KEY = 'has_wiki_links'
    VALUE_TYPE = ValueType.FLAG
    PRIORITY = 75
    PART_TYPE_FILTER = 'text'
    ROLE_FILTER = 'assistant'
    
    WIKI_LINK_PATTERN = re.compile(r'\[\[([^\]]+)\]\]')
    
    def annotate(self, data: ContentPartData) -> list[AnnotationResult]:
        if not data.text_content:
            return []
        
        matches = self.WIKI_LINK_PATTERN.findall(data.text_content)
        
        if not matches:
            return []
        
        return [
            AnnotationResult(
                key='has_wiki_links',
                value_type=ValueType.FLAG,
                confidence=1.0,
                reason='wiki_links_detected',
            ),
            AnnotationResult(
                key='wiki_link_count',
                value=len(matches),
                value_type=ValueType.NUMERIC,
            ),
        ]


# ============================================================
# Registry for running all annotators
# ============================================================

CONTENT_PART_ANNOTATORS = [
    CodeBlockAnnotator,
    ScriptHeaderAnnotator,
    LatexContentAnnotator,
    WikiLinkContentAnnotator,
]


def run_content_part_annotators(session: Session) -> dict[str, int]:
    """
    Run all content-part annotators in priority order.
    
    Returns dict mapping annotator name to annotation count.
    """
    # Sort by priority (descending)
    sorted_annotators = sorted(
        CONTENT_PART_ANNOTATORS,
        key=lambda cls: cls.PRIORITY,
        reverse=True,
    )
    
    results = {}
    for annotator_cls in sorted_annotators:
        annotator = annotator_cls(session)
        count = annotator.compute()
        results[annotator_cls.__name__] = count
    
    session.commit()
    return results



---
File: llm_archive/annotators/prompt_response.py
---
# llm_archive/annotators/prompt_response.py
"""Prompt-response level annotators.

These annotators work on the prompt_responses table, which is built
without tree dependency. They support:
- Wiki article candidate detection
- Naive title extraction
- Prerequisite/skip annotation filtering

Uses the new typed annotation tables (derived.prompt_response_annotations_*).
"""

from abc import abstractmethod
from dataclasses import dataclass
from datetime import datetime
from typing import Iterator
from uuid import UUID

from sqlalchemy import text
from sqlalchemy.orm import Session

from llm_archive.annotations.core import (
    AnnotationWriter, AnnotationReader, AnnotationResult,
    EntityType, ValueType,
)


# ============================================================
# Data Classes
# ============================================================

@dataclass
class PromptResponseData:
    """Data passed to prompt-response annotation logic."""
    prompt_response_id: UUID
    dialogue_id: UUID
    prompt_message_id: UUID
    response_message_id: UUID
    prompt_text: str | None
    response_text: str | None
    prompt_word_count: int | None
    response_word_count: int | None
    prompt_role: str
    response_role: str
    created_at: datetime | None


# ============================================================
# Base PromptResponse Annotator
# ============================================================

class PromptResponseAnnotator:
    """
    Base class for annotating prompt-response pairs.
    
    Iterates over derived.prompt_response_content_v (view that joins
    content from raw.content_parts - no denormalized storage).
    
    Supports annotation prerequisites and skip conditions using the
    new typed annotation tables:
    - REQUIRES_FLAGS: Only process entities with ALL of these flag annotations
    - REQUIRES_STRINGS: Only process entities with ALL of these (key, value) string annotations
    - SKIP_IF_FLAGS: Skip entities with ANY of these flag annotations
    - SKIP_IF_STRINGS: Skip entities with ANY of these (key,) or (key, value) string annotations
    
    Example:
        REQUIRES_STRINGS = [('exchange_type', 'wiki_article')]
        SKIP_IF_FLAGS = ['has_preamble']
    """
    
    ENTITY_TYPE = EntityType.PROMPT_RESPONSE
    
    # Annotator metadata
    ANNOTATION_KEY: str = None  # Required: the key this annotator produces
    VALUE_TYPE: ValueType = ValueType.STRING  # What type of annotation this produces
    PRIORITY: int = 50  # Higher runs first
    VERSION: str = '1.0'
    SOURCE: str = 'heuristic'
    
    # Filtering - override in subclass
    REQUIRES_FLAGS: list[str] = []
    REQUIRES_STRINGS: list[tuple[str, str]] = []  # (key, value) pairs
    SKIP_IF_FLAGS: list[str] = []
    SKIP_IF_STRINGS: list[tuple[str, ...]] = []  # (key,) or (key, value)
    
    def __init__(self, session: Session):
        self.session = session
        self.writer = AnnotationWriter(session)
        self.reader = AnnotationReader(session)
    
    def compute(self) -> int:
        """Run annotation over prompt-response pairs."""
        count = 0
        
        for data in self._iter_prompt_responses():
            results = self.annotate(data)
            for result in results:
                if self._write_result(data.prompt_response_id, result):
                    count += 1
        
        return count
    
    def _write_result(self, entity_id: UUID, result: AnnotationResult) -> bool:
        """Write an annotation result to the appropriate table."""
        # Use the result's value_type if specified, otherwise use class default
        value_type = result.value_type if result.value_type else self.VALUE_TYPE
        
        if value_type == ValueType.FLAG:
            return self.writer.write_flag(
                entity_type=self.ENTITY_TYPE,
                entity_id=entity_id,
                key=result.key,
                confidence=result.confidence,
                reason=result.reason,
                source=result.source or self.SOURCE,
                source_version=result.source_version or self.VERSION,
            )
        elif value_type == ValueType.STRING:
            return self.writer.write_string(
                entity_type=self.ENTITY_TYPE,
                entity_id=entity_id,
                key=result.key,
                value=str(result.value),
                confidence=result.confidence,
                reason=result.reason,
                source=result.source or self.SOURCE,
                source_version=result.source_version or self.VERSION,
            )
        elif value_type == ValueType.NUMERIC:
            return self.writer.write_numeric(
                entity_type=self.ENTITY_TYPE,
                entity_id=entity_id,
                key=result.key,
                value=float(result.value),
                confidence=result.confidence,
                reason=result.reason,
                source=result.source or self.SOURCE,
                source_version=result.source_version or self.VERSION,
            )
        elif value_type == ValueType.JSON:
            return self.writer.write_json(
                entity_type=self.ENTITY_TYPE,
                entity_id=entity_id,
                key=result.key,
                value=result.value,
                confidence=result.confidence,
                reason=result.reason,
                source=result.source or self.SOURCE,
                source_version=result.source_version or self.VERSION,
            )
        return False
    
    def _iter_prompt_responses(self) -> Iterator[PromptResponseData]:
        """Iterate over prompt-responses with content, respecting annotation filters."""
        # Base query uses the content view
        query_parts = ["""
            SELECT 
                prc.prompt_response_id,
                prc.dialogue_id,
                prc.prompt_message_id,
                prc.response_message_id,
                prc.prompt_text,
                prc.response_text,
                prc.prompt_word_count,
                prc.response_word_count,
                prc.prompt_role,
                prc.response_role,
                prc.created_at
            FROM derived.prompt_response_content_v prc
        """]
        
        params = {}
        join_idx = 0
        
        # Add REQUIRES_FLAGS joins (must have these flags)
        for flag_key in self.REQUIRES_FLAGS:
            alias = f"req_flag_{join_idx}"
            query_parts.append(f"""
                JOIN derived.prompt_response_annotations_flag {alias} ON 
                    {alias}.entity_id = prc.prompt_response_id
                    AND {alias}.annotation_key = :req_flag_key_{join_idx}
            """)
            params[f'req_flag_key_{join_idx}'] = flag_key
            join_idx += 1
        
        # Add REQUIRES_STRINGS joins (must have these key+value pairs)
        for key, value in self.REQUIRES_STRINGS:
            alias = f"req_str_{join_idx}"
            query_parts.append(f"""
                JOIN derived.prompt_response_annotations_string {alias} ON 
                    {alias}.entity_id = prc.prompt_response_id
                    AND {alias}.annotation_key = :req_str_key_{join_idx}
                    AND {alias}.annotation_value = :req_str_val_{join_idx}
            """)
            params[f'req_str_key_{join_idx}'] = key
            params[f'req_str_val_{join_idx}'] = value
            join_idx += 1
        
        # Add SKIP_IF_FLAGS exclusions
        skip_where_clauses = []
        for flag_key in self.SKIP_IF_FLAGS:
            alias = f"skip_flag_{join_idx}"
            query_parts.append(f"""
                LEFT JOIN derived.prompt_response_annotations_flag {alias} ON 
                    {alias}.entity_id = prc.prompt_response_id
                    AND {alias}.annotation_key = :skip_flag_key_{join_idx}
            """)
            params[f'skip_flag_key_{join_idx}'] = flag_key
            skip_where_clauses.append(f"{alias}.id IS NULL")
            join_idx += 1
        
        # Add SKIP_IF_STRINGS exclusions
        for skip_spec in self.SKIP_IF_STRINGS:
            alias = f"skip_str_{join_idx}"
            if len(skip_spec) == 1:
                # Skip if key exists (any value)
                query_parts.append(f"""
                    LEFT JOIN derived.prompt_response_annotations_string {alias} ON 
                        {alias}.entity_id = prc.prompt_response_id
                        AND {alias}.annotation_key = :skip_str_key_{join_idx}
                """)
                params[f'skip_str_key_{join_idx}'] = skip_spec[0]
            else:
                # Skip if key+value match
                query_parts.append(f"""
                    LEFT JOIN derived.prompt_response_annotations_string {alias} ON 
                        {alias}.entity_id = prc.prompt_response_id
                        AND {alias}.annotation_key = :skip_str_key_{join_idx}
                        AND {alias}.annotation_value = :skip_str_val_{join_idx}
                """)
                params[f'skip_str_key_{join_idx}'] = skip_spec[0]
                params[f'skip_str_val_{join_idx}'] = skip_spec[1]
            skip_where_clauses.append(f"{alias}.id IS NULL")
            join_idx += 1
        
        # WHERE clause for exclusions
        if skip_where_clauses:
            query_parts.append("WHERE " + " AND ".join(skip_where_clauses))
        
        query_parts.append("ORDER BY prc.created_at")
        
        query = text("\n".join(query_parts))
        
        for row in self.session.execute(query, params):
            yield PromptResponseData(
                prompt_response_id=row.prompt_response_id,
                dialogue_id=row.dialogue_id,
                prompt_message_id=row.prompt_message_id,
                response_message_id=row.response_message_id,
                prompt_text=row.prompt_text,
                response_text=row.response_text,
                prompt_word_count=row.prompt_word_count,
                response_word_count=row.response_word_count,
                prompt_role=row.prompt_role,
                response_role=row.response_role,
                created_at=row.created_at,
            )
    
    @abstractmethod
    def annotate(self, data: PromptResponseData) -> list[AnnotationResult]:
        """
        Analyze prompt-response pair and return annotations to create.
        
        Args:
            data: PromptResponseData with texts and metadata
            
        Returns:
            List of AnnotationResult objects (empty list if no match)
        """
        pass


# ============================================================
# Wiki Article Detection
# ============================================================

class WikiCandidateAnnotator(PromptResponseAnnotator):
    """
    Detect wiki-style article candidates.
    
    Looks for [[wiki links]] in assistant responses, which indicate
    the response was likely formatted as a wiki article.
    
    Produces a STRING annotation: exchange_type = 'wiki_article'
    """
    
    ANNOTATION_KEY = 'exchange_type'
    VALUE_TYPE = ValueType.STRING
    PRIORITY = 60
    VERSION = '1.0'
    
    def annotate(self, data: PromptResponseData) -> list[AnnotationResult]:
        if data.response_role != 'assistant':
            return []
        
        response = data.response_text or ''
        
        # Count wiki links
        wiki_link_count = response.count('[[')
        
        if wiki_link_count >= 1:
            # High confidence if multiple links
            confidence = 0.95 if wiki_link_count >= 3 else 0.8
            
            results = [
                # String annotation for exchange type
                AnnotationResult(
                    key='exchange_type',
                    value='wiki_article',
                    value_type=ValueType.STRING,
                    confidence=confidence,
                    reason='wiki_links_detected',
                ),
                # Numeric annotation for link count
                AnnotationResult(
                    key='wiki_link_count',
                    value=wiki_link_count,
                    value_type=ValueType.NUMERIC,
                    confidence=1.0,
                    reason='counted',
                ),
            ]
            return results
        
        return []


# ============================================================
# Title Extraction
# ============================================================

class NaiveTitleAnnotator(PromptResponseAnnotator):
    """
    Extract title from first line of response.
    
    Looks for:
    - Markdown headers: # Title
    - Bold headers: **Title**
    
    Should run AFTER wiki candidate detection.
    Only runs on wiki_article candidates.
    
    Produces a STRING annotation: proposed_title = '<extracted title>'
    """
    
    ANNOTATION_KEY = 'proposed_title'
    VALUE_TYPE = ValueType.STRING
    PRIORITY = 50
    VERSION = '1.0'
    
    # Only process wiki article candidates
    REQUIRES_STRINGS = [('exchange_type', 'wiki_article')]
    
    def annotate(self, data: PromptResponseData) -> list[AnnotationResult]:
        if data.response_role != 'assistant':
            return []
        
        response = data.response_text or ''
        title, reason = self._extract_title(response)
        
        if title:
            return [AnnotationResult(
                key='proposed_title',
                value=title,
                value_type=ValueType.STRING,
                confidence=0.8,
                reason=reason,
            )]
        
        return []
    
    def _extract_title(self, text: str) -> tuple[str | None, str | None]:
        """Extract title from first line of text. Returns (title, reason)."""
        lines = text.strip().split('\n')
        if not lines:
            return None, None
        
        first_line = lines[0].strip()
        
        # Markdown header: # Title or ## Title
        if first_line.startswith('#'):
            title = first_line.lstrip('#').strip()
            if title:
                return title, 'markdown_header'
        
        # Bold header: **Title**
        if first_line.startswith('**') and first_line.endswith('**'):
            title = first_line.strip('*').strip()
            if title:
                return title, 'bold_header'
        
        # Bold header with trailing content: **Title** - some subtitle
        if first_line.startswith('**') and '**' in first_line[2:]:
            end_idx = first_line.index('**', 2)
            title = first_line[2:end_idx].strip()
            if title:
                return title, 'bold_header_with_suffix'
        
        return None, None


# ============================================================
# Code Detection
# ============================================================

import re


class HasCodeAnnotator(PromptResponseAnnotator):
    """
    Detect if prompt-response pair involves code.
    
    Aggregates evidence from multiple sources:
    - Code blocks (```)
    - Script headers (shebang, #include)
    - Function definitions
    - Import statements
    
    Produces:
    - has_code FLAG
    - code_evidence STRING (multi-value)
    """
    
    ANNOTATION_KEY = 'has_code'
    VALUE_TYPE = ValueType.FLAG
    PRIORITY = 55
    VERSION = '1.0'
    
    SKIP_IF_FLAGS = ['has_code']  # Skip if already annotated
    
    def annotate(self, data: PromptResponseData) -> list[AnnotationResult]:
        if data.response_role != 'assistant':
            return []
        
        if not data.response_text:
            return []
        
        results = []
        evidence_types = set()
        
        # Check for code blocks
        if '```' in data.response_text:
            evidence_types.add('code_block')
        
        # Check for script headers
        if re.search(r'^#!\s*/(?:usr/)?bin/', data.response_text, re.MULTILINE):
            evidence_types.add('shebang')
        if re.search(r'^#include\s*[<"]', data.response_text, re.MULTILINE):
            evidence_types.add('c_include')
        
        # Check for function definitions
        if re.search(r'\bdef\s+\w+\s*\(', data.response_text):
            evidence_types.add('python_function')
        if re.search(r'function\s+\w+\s*\(', data.response_text):
            evidence_types.add('js_function')
        if re.search(r'const\s+\w+\s*=\s*\([^)]*\)\s*=>', data.response_text):
            evidence_types.add('arrow_function')
        
        # Check for import statements
        if re.search(r'^(?:import|from)\s+\w+', data.response_text, re.MULTILINE):
            evidence_types.add('python_import')
        if re.search(r'^(?:const|let|var)\s+.*=\s*require\s*\(', data.response_text, re.MULTILINE):
            evidence_types.add('js_require')
        
        if not evidence_types:
            return []
        
        # Main flag with confidence based on evidence strength
        strong_evidence = {'code_block', 'shebang', 'c_include'}
        is_strong = bool(evidence_types & strong_evidence)
        
        results.append(AnnotationResult(
            key='has_code',
            value_type=ValueType.FLAG,
            confidence=0.95 if is_strong else 0.75,
            reason=','.join(sorted(evidence_types)),
        ))
        
        # Evidence type annotations (multi-value)
        for evidence in evidence_types:
            results.append(AnnotationResult(
                key='code_evidence',
                value=evidence,
                value_type=ValueType.STRING,
            ))
        
        return results


# ============================================================
# LaTeX Detection
# ============================================================

class HasLatexAnnotator(PromptResponseAnnotator):
    """
    Detect if prompt-response pair contains LaTeX/math notation.
    
    Produces:
    - has_latex FLAG
    - latex_type STRING (multi-value: 'display', 'inline', 'commands')
    """
    
    ANNOTATION_KEY = 'has_latex'
    VALUE_TYPE = ValueType.FLAG
    PRIORITY = 54
    VERSION = '1.0'
    
    SKIP_IF_FLAGS = ['has_latex']
    
    # Patterns
    DISPLAY_MATH = re.compile(r'\$\$.+?\$\$|\\\[.+?\\\]', re.DOTALL)
    INLINE_MATH = re.compile(r'(?<!\$)\$(?!\$).+?(?<!\$)\$(?!\$)')
    LATEX_COMMANDS = re.compile(
        r'\\(?:frac|sum|int|prod|lim|sqrt|begin|end|alpha|beta|gamma|'
        r'delta|epsilon|theta|lambda|sigma|omega|pi|infty|partial|nabla|'
        r'mathbb|mathcal|mathbf|mathrm|text|left|right|cdot|times|div)'
    )
    
    def annotate(self, data: PromptResponseData) -> list[AnnotationResult]:
        if data.response_role != 'assistant':
            return []
        
        if not data.response_text:
            return []
        
        results = []
        latex_types = set()
        
        if self.DISPLAY_MATH.search(data.response_text):
            latex_types.add('display')
        
        if self.INLINE_MATH.search(data.response_text):
            latex_types.add('inline')
        
        if self.LATEX_COMMANDS.search(data.response_text):
            latex_types.add('commands')
        
        if not latex_types:
            return []
        
        # Main flag
        results.append(AnnotationResult(
            key='has_latex',
            value_type=ValueType.FLAG,
            confidence=0.95 if 'display' in latex_types else 0.8,
            reason='latex_detected',
        ))
        
        # Type annotations
        for latex_type in latex_types:
            results.append(AnnotationResult(
                key='latex_type',
                value=latex_type,
                value_type=ValueType.STRING,
            ))
        
        return results


# ============================================================
# Annotator Registry
# ============================================================

PROMPT_RESPONSE_ANNOTATORS = [
    WikiCandidateAnnotator,
    NaiveTitleAnnotator,
    HasCodeAnnotator,
    HasLatexAnnotator,
]


def run_prompt_response_annotators(session: Session) -> dict[str, int]:
    """
    Run all prompt-response annotators in priority order.
    
    Returns dict mapping annotator name to annotation count.
    """
    # Sort by priority (descending)
    sorted_annotators = sorted(
        PROMPT_RESPONSE_ANNOTATORS,
        key=lambda cls: cls.PRIORITY,
        reverse=True,
    )
    
    results = {}
    for annotator_cls in sorted_annotators:
        annotator = annotator_cls(session)
        count = annotator.compute()
        results[annotator_cls.__name__] = count
    
    session.commit()
    return results



---
File: llm_archive/builders/__init__.py
---
# llm_archive/builders/__init__.py
"""Derived data builders."""

from llm_archive.builders.prompt_response import PromptResponseBuilder

__all__ = ["PromptResponseBuilder"]



---
File: llm_archive/builders/prompt_response.py
---
# llm_archive/builders/prompt_response.py
"""Prompt-response pair building - direct message associations without tree dependency."""

from uuid import UUID

from sqlalchemy.orm import Session
from sqlalchemy import text
from loguru import logger

from llm_archive.models import Dialogue, Message, ContentPart


class PromptResponseBuilder:
    """
    Builds prompt-response pairs directly from messages.
    
    Unlike ExchangeBuilder (which depends on tree analysis), this uses:
    1. parent_id relationship when available (ChatGPT)
    2. Sequential fallback (Claude, or when parent_id missing)
    
    Result: Each non-user message is paired with its eliciting user prompt.
    """
    
    def __init__(self, session: Session):
        self.session = session
    
    def build_all(self) -> dict[str, int]:
        """Build prompt-response pairs for all dialogues."""
        dialogues = self.session.query(Dialogue).all()
        
        counts = {
            'dialogues': 0,
            'prompt_responses': 0,
            'content_records': 0,
        }
        
        for dialogue in dialogues:
            try:
                result = self.build_for_dialogue(dialogue.id)
                counts['dialogues'] += 1
                counts['prompt_responses'] += result['prompt_responses']
                counts['content_records'] += result['content_records']
            except Exception as e:
                logger.error(f"Failed to build prompt-responses for {dialogue.id}: {e}")
                self.session.rollback()
        
        self.session.commit()
        logger.info(f"Prompt-response building complete: {counts}")
        return counts
    
    def build_for_dialogue(self, dialogue_id: UUID) -> dict[str, int]:
        """Build prompt-response pairs for a single dialogue."""
        # Clear existing data
        self._clear_existing(dialogue_id)
        
        # Get messages ordered by created_at (with fallback to id for stable ordering)
        messages = (
            self.session.query(Message)
            .filter(Message.dialogue_id == dialogue_id)
            .filter(Message.deleted_at.is_(None))
            .order_by(Message.created_at.nulls_first(), Message.id)
            .all()
        )
        
        if not messages:
            return {'prompt_responses': 0, 'content_records': 0}
        
        # Build lookup by ID
        msg_by_id = {m.id: m for m in messages}
        position_by_id = {m.id: i for i, m in enumerate(messages)}
        
        # Track most recent user message for sequential fallback
        last_user_msg: Message | None = None
        
        pr_count = 0
        for msg in messages:
            if msg.role == 'user':
                last_user_msg = msg
                continue
            
            # Find the prompt for this response
            prompt_msg = self._find_prompt(msg, msg_by_id, last_user_msg)
            
            if prompt_msg is None:
                # Response without a prompt (e.g., system greeting)
                continue
            
            # Create prompt-response record
            pr_id = self._create_prompt_response(
                dialogue_id=dialogue_id,
                prompt_msg=prompt_msg,
                response_msg=msg,
                prompt_position=position_by_id[prompt_msg.id],
                response_position=position_by_id[msg.id],
            )
            pr_count += 1
        
        # Build content records
        content_count = self._build_content(dialogue_id)
        
        self.session.flush()
        
        return {
            'prompt_responses': pr_count,
            'content_records': content_count,
        }
    
    def _find_prompt(
        self,
        response_msg: Message,
        msg_by_id: dict[UUID, Message],
        last_user_msg: Message | None,
    ) -> Message | None:
        """Find the user prompt that elicited this response."""
        # Strategy 1: Use parent_id if it points to a user message
        if response_msg.parent_id and response_msg.parent_id in msg_by_id:
            parent = msg_by_id[response_msg.parent_id]
            if parent.role == 'user':
                return parent
            # Parent exists but isn't user - walk up to find user
            # (handles cases like assistant -> tool_result -> assistant)
            current = parent
            visited = {response_msg.id}
            while current and current.id not in visited:
                visited.add(current.id)
                if current.role == 'user':
                    return current
                if current.parent_id and current.parent_id in msg_by_id:
                    current = msg_by_id[current.parent_id]
                else:
                    break
        
        # Strategy 2: Fall back to most recent user message
        return last_user_msg
    
    def _create_prompt_response(
        self,
        dialogue_id: UUID,
        prompt_msg: Message,
        response_msg: Message,
        prompt_position: int,
        response_position: int,
    ) -> UUID:
        """Insert a prompt_response record and return its ID."""
        result = self.session.execute(
            text("""
                INSERT INTO derived.prompt_responses 
                    (dialogue_id, prompt_message_id, response_message_id, 
                     prompt_position, response_position, prompt_role, response_role)
                VALUES 
                    (:dialogue_id, :prompt_id, :response_id,
                     :prompt_pos, :response_pos, :prompt_role, :response_role)
                RETURNING id
            """),
            {
                'dialogue_id': dialogue_id,
                'prompt_id': prompt_msg.id,
                'response_id': response_msg.id,
                'prompt_pos': prompt_position,
                'response_pos': response_position,
                'prompt_role': prompt_msg.role,
                'response_role': response_msg.role,
            }
        )
        return result.scalar_one()
    
    def _build_content(self, dialogue_id: UUID) -> int:
        """Build content records for all prompt-responses in a dialogue."""
        # Use SQL to aggregate text content efficiently
        result = self.session.execute(
            text("""
                INSERT INTO derived.prompt_response_content 
                    (prompt_response_id, prompt_text, response_text, 
                     prompt_word_count, response_word_count)
                SELECT 
                    pr.id,
                    prompt_content.text_content as prompt_text,
                    response_content.text_content as response_text,
                    COALESCE(array_length(regexp_split_to_array(prompt_content.text_content, '\\s+'), 1), 0),
                    COALESCE(array_length(regexp_split_to_array(response_content.text_content, '\\s+'), 1), 0)
                FROM derived.prompt_responses pr
                LEFT JOIN LATERAL (
                    SELECT string_agg(cp.text_content, E'\\n' ORDER BY cp.sequence) as text_content
                    FROM raw.content_parts cp
                    WHERE cp.message_id = pr.prompt_message_id
                      AND cp.part_type = 'text'
                ) prompt_content ON true
                LEFT JOIN LATERAL (
                    SELECT string_agg(cp.text_content, E'\\n' ORDER BY cp.sequence) as text_content
                    FROM raw.content_parts cp
                    WHERE cp.message_id = pr.response_message_id
                      AND cp.part_type = 'text'
                ) response_content ON true
                WHERE pr.dialogue_id = :dialogue_id
                ON CONFLICT (prompt_response_id) DO UPDATE SET
                    prompt_text = EXCLUDED.prompt_text,
                    response_text = EXCLUDED.response_text,
                    prompt_word_count = EXCLUDED.prompt_word_count,
                    response_word_count = EXCLUDED.response_word_count
            """),
            {'dialogue_id': dialogue_id}
        )
        return result.rowcount
    
    def _clear_existing(self, dialogue_id: UUID):
        """Clear existing prompt-response data for a dialogue."""
        self.session.execute(
            text("""
                DELETE FROM derived.prompt_responses 
                WHERE dialogue_id = :dialogue_id
            """),
            {'dialogue_id': dialogue_id}
        )



---
File: llm_archive/cli.py
---
# llm_archive/cli.py
"""Command-line interface for LLM archive operations."""

import json
from pathlib import Path

import fire
from loguru import logger

from llm_archive.config import DATABASE_URL
from llm_archive.db import get_session, init_schema, reset_schema
from llm_archive.extractors import ChatGPTExtractor, ClaudeExtractor
from llm_archive.builders import PromptResponseBuilder
from llm_archive.annotators import (
    WikiCandidateAnnotator,
    NaiveTitleAnnotator,
)


class CLI:
    """LLM Archive - Conversation ingestion and analysis."""
    
    def __init__(self, db_url: str | None = None):
        self.db_url = db_url or DATABASE_URL
    
    # ================================================================
    # Schema Management
    # ================================================================
    
    def init(self, schema_dir: str = "schema"):
        """Initialize database schema."""
        init_schema(self.db_url, Path(schema_dir))
        logger.info("Schema initialized")
    
    def reset(self, confirm: bool = False, schema_dir: str = "schema"):
        """Reset database (drops and recreates schema)."""
        if not confirm:
            logger.warning("Pass --confirm to reset database")
            return
        reset_schema(self.db_url, Path(schema_dir))
        logger.info("Database reset")
    
    # ================================================================
    # Import
    # ================================================================
    
    def import_chatgpt(
        self, 
        path: str,
        assume_immutable: bool = False,
        incremental: bool = False,
    ):
        """Import ChatGPT conversations.json export.
        
        Args:
            path: Path to conversations.json file
            assume_immutable: Skip content hash checks for existing messages.
                Faster, but won't detect in-place message edits. Use when
                the provider treats messages as immutable (edits create new IDs).
            incremental: Don't soft-delete messages missing from this import.
                Use when importing partial/delta exports.
        """
        data = self._load_json(path)
        
        with get_session(self.db_url) as session:
            extractor = ChatGPTExtractor(
                session,
                assume_immutable=assume_immutable,
                incremental=incremental,
            )
            counts = extractor.extract_all(data)
        
        return counts
    
    def import_claude(
        self, 
        path: str,
        assume_immutable: bool = False,
        incremental: bool = False,
    ):
        """Import Claude conversations.json export.
        
        Args:
            path: Path to conversations.json file
            assume_immutable: Skip content hash checks for existing messages.
                Faster, but won't detect in-place message edits. Use when
                the provider treats messages as immutable (edits create new IDs).
            incremental: Don't soft-delete messages missing from this import.
                Use when importing partial/delta exports.
        """
        data = self._load_json(path)
        
        with get_session(self.db_url) as session:
            extractor = ClaudeExtractor(
                session,
                assume_immutable=assume_immutable,
                incremental=incremental,
            )
            counts = extractor.extract_all(data)
        
        return counts
    
    def import_all(
        self,
        chatgpt_path: str | None = None,
        claude_path: str | None = None,
        assume_immutable: bool = False,
        incremental: bool = False,
    ):
        """Import from multiple sources.
        
        Args:
            chatgpt_path: Path to ChatGPT conversations.json
            claude_path: Path to Claude conversations.json
            assume_immutable: Skip content hash checks for existing messages
            incremental: Don't soft-delete messages missing from this import
        """
        results = {}
        
        if chatgpt_path:
            results['chatgpt'] = self.import_chatgpt(
                chatgpt_path,
                assume_immutable=assume_immutable,
                incremental=incremental,
            )
        
        if claude_path:
            results['claude'] = self.import_claude(
                claude_path,
                assume_immutable=assume_immutable,
                incremental=incremental,
            )
        
        return results
    
    # ================================================================
    # Build Derived Structures
    # ================================================================
    

    def build_prompt_responses(self):
        """Build prompt-response pairs (no tree dependency)."""
        with get_session(self.db_url) as session:
            builder = PromptResponseBuilder(session)
            counts = builder.build_all()
        return counts

    def build_all(self):
        """Build all derived structures."""
        results = {}
        results['prompt-responses'] = self.build_prompt_responses()
        return results
    
    # ================================================================
    # Annotations
    # ================================================================
    
    def annotate(self):
        """Run all annotators."""
        results = {}
        with get_session(self.db_url) as session:
            results['wiki candidates'] = WikiCandidateAnnotator(session).compute()
            results['naive titles'] = NaiveTitleAnnotator(session).compute()
        
        return results
    
    # ================================================================
    # Analysis
    # ================================================================
    
    def stats(self):
        """Show database statistics."""
        from sqlalchemy import text
        
        with get_session(self.db_url) as session:
            stats = {}
            
            # Raw counts
            stats['dialogues'] = session.execute(
                text("SELECT COUNT(*) FROM raw.dialogues")
            ).scalar()
            
            stats['messages'] = session.execute(
                text("SELECT COUNT(*) FROM raw.messages")
            ).scalar()
            
            stats['content_parts'] = session.execute(
                text("SELECT COUNT(*) FROM raw.content_parts")
            ).scalar()
            
            # By source
            sources = session.execute(
                text("SELECT source, COUNT(*) FROM raw.dialogues GROUP BY source")
            ).fetchall()
            stats['by_source'] = {s: c for s, c in sources}
            
            stats['annotations'] = session.execute(
                text("SELECT COUNT(*) FROM derived.annotations WHERE superseded_at IS NULL")
            ).scalar()
            
        
        # Print nicely
        print("\n=== LLM Archive Statistics ===\n")
        print("Raw Data:")
        print(f"  Dialogues: {stats['dialogues']}")
        print(f"  Messages: {stats['messages']}")
        print(f"  Content Parts: {stats['content_parts']}")
        print(f"  By Source: {stats['by_source']}")
        
        print("\nDerived Data:")
        print(f"  Annotations: {stats['annotations']}")
        
        return stats
    
    # ================================================================
    # Full Pipeline
    # ================================================================
    
    def run(
        self,
        chatgpt_path: str | None = None,
        claude_path: str | None = None,
        init_db: bool = False,
        schema_dir: str = "schema",
        assume_immutable: bool = False,
        incremental: bool = False,
    ):
        """Run full pipeline: import, build, annotate.
        
        Args:
            chatgpt_path: Path to ChatGPT conversations.json
            claude_path: Path to Claude conversations.json
            init_db: Initialize database schema before import
            schema_dir: Directory containing schema files
            assume_immutable: Skip content hash checks for existing messages
            incremental: Don't soft-delete messages missing from this import
        """
        results = {}
        
        if init_db:
            self.init(schema_dir)
        
        # Import
        if chatgpt_path or claude_path:
            results['import'] = self.import_all(
                chatgpt_path=chatgpt_path,
                claude_path=claude_path,
                assume_immutable=assume_immutable,
                incremental=incremental,
            )
        
        # Build
        results['build'] = self.build_all()
        
        # Annotate
        results['annotate'] = self.annotate()
        
        # Stats
        self.stats()
        
        return results
    
    # ================================================================
    # Helpers
    # ================================================================
    
    def _load_json(self, path: str) -> list[dict]:
        """Load JSON file."""
        p = Path(path)
        if not p.exists():
            raise FileNotFoundError(f"File not found: {path}")
        
        logger.info(f"Loading {path}")
        with p.open() as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            raise ValueError("Expected JSON array")
        
        logger.info(f"Loaded {len(data)} items")
        return data


def main():
    """Entry point."""
    fire.Fire(CLI)


if __name__ == "__main__":
    main()



---
File: llm_archive/config.py
---
# llm_archive/config.py
"""Configuration management via environment variables."""

import os
from pathlib import Path

from dotenv import load_dotenv

# Load .env from project root
_env_path = Path(__file__).parent.parent / ".env"
if _env_path.exists():
    load_dotenv(_env_path)


def get_database_url() -> str:
    """Construct database URL from environment variables."""
    host = os.getenv("POSTGRES_HOST", "localhost")
    port = os.getenv("POSTGRES_PORT", "5432")
    db = os.getenv("POSTGRES_DB", "llm_archive")
    user = os.getenv("POSTGRES_USER", "postgres")
    password = os.getenv("POSTGRES_PASSWORD", "postgres")
    
    return f"postgresql://{user}:{password}@{host}:{port}/{db}"


# Default URL for imports
DATABASE_URL = get_database_url()


---
File: llm_archive/db.py
---
# llm_archive/db.py
"""Database connection and session management."""

from contextlib import contextmanager
from pathlib import Path

from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker, Session
from loguru import logger


def get_engine(db_url: str):
    """Create database engine."""
    return create_engine(db_url, echo=False)


def get_session_factory(db_url: str) -> sessionmaker:
    """Create a session factory."""
    engine = get_engine(db_url)
    return sessionmaker(bind=engine)


@contextmanager
def get_session(db_url: str):
    """Context manager for database sessions."""
    factory = get_session_factory(db_url)
    session = factory()
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()


def init_schema(db_url: str, schema_dir: Path | str):
    """Initialize database schema from SQL files."""
    schema_dir = Path(schema_dir)
    engine = get_engine(db_url)
    
    sql_files = sorted(schema_dir.glob("*.sql"))
    
    if not sql_files:
        logger.warning(f"No SQL files found in {schema_dir}")
        return
    
    with engine.connect() as conn:
        for sql_file in sql_files:
            logger.info(f"Executing {sql_file.name}")
            sql = sql_file.read_text()
            
            statements = [s.strip() for s in sql.split(';') if s.strip()]
            for stmt in statements:
                try:
                    conn.execute(text(stmt))
                except Exception as e:
                    logger.debug(f"Statement note: {e}")
            
            conn.commit()
    
    logger.info("Schema initialization complete")


def reset_schema(db_url: str, schema_dir: Path | str | None = None):
    """Drop and recreate schemas (destructive!)."""
    engine = get_engine(db_url)
    
    with engine.connect() as conn:
        conn.execute(text("DROP SCHEMA IF EXISTS derived CASCADE"))
        conn.execute(text("DROP SCHEMA IF EXISTS raw CASCADE"))
        conn.commit()
    
    logger.info("Schemas dropped")
    
    if schema_dir:
        init_schema(db_url, schema_dir)


---
File: llm_archive/extractors/__init__.py
---
# llm_archive/extractors/__init__.py
"""Source-specific extractors for importing dialogue data."""

from llm_archive.extractors.base import BaseExtractor, parse_timestamp, normalize_role
from llm_archive.extractors.chatgpt import ChatGPTExtractor
from llm_archive.extractors.claude import ClaudeExtractor

__all__ = [
    "BaseExtractor",
    "parse_timestamp",
    "normalize_role",
    "ChatGPTExtractor",
    "ClaudeExtractor",
]


---
File: llm_archive/extractors/base.py
---
# llm_archive/extractors/base.py
"""Shared extraction utilities and base classes."""

import hashlib
import json
from abc import ABC, abstractmethod
from datetime import datetime, timezone
from typing import Any
from uuid import UUID

from sqlalchemy.orm import Session
from loguru import logger

from llm_archive.models import Dialogue, Message, ContentPart


def parse_timestamp(value: int | float | str | None) -> datetime | None:
    """
    Parse timestamp from various formats to timezone-aware datetime.
    
    Handles:
    - Epoch floats/ints (ChatGPT)
    - ISO 8601 strings (Claude)
    """
    if value is None:
        return None
    
    if isinstance(value, str):
        # ISO 8601 format
        try:
            dt = datetime.fromisoformat(value.replace('Z', '+00:00'))
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt
        except ValueError:
            return None
    
    if isinstance(value, (int, float)):
        # Epoch seconds
        try:
            return datetime.fromtimestamp(value, tz=timezone.utc)
        except (ValueError, OSError):
            return None
    
    return None


def normalize_role(role: str, source: str) -> str:
    """
    Normalize role/sender to standard vocabulary.
    
    Standard roles: 'user', 'assistant', 'system', 'tool'
    """
    if role is None:
        return 'unknown'
    
    role_lower = role.lower()
    
    # Claude uses 'human' instead of 'user'
    if role_lower == 'human':
        return 'user'
    
    return role_lower


def safe_get(data: dict[str, Any], *keys: str, default: Any = None) -> Any:
    """Safely traverse nested dict."""
    current = data
    for key in keys:
        if not isinstance(current, dict):
            return default
        current = current.get(key, default)
        if current is None:
            return default
    return current


def compute_content_hash(source_json: dict | list | str) -> str:
    """Compute a stable hash of message content for change detection."""
    # Serialize to JSON with sorted keys for stability
    if isinstance(source_json, str):
        content = source_json
    else:
        content = json.dumps(source_json, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(content.encode()).hexdigest()


class BaseExtractor(ABC):
    """
    Base class for source extractors.
    
    Supports idempotent ingestion with incremental updates:
    - Skip dialogues that haven't changed (by updated_at timestamp)
    - Preserve message UUIDs for unchanged messages
    - Soft-delete messages removed from source (unless incremental=True)
    - Only rebuild content_parts for actually changed messages
    
    Args:
        session: SQLAlchemy session
        assume_immutable: If True, assume message content never changes once created.
            This skips content hash comparison for existing messages, which is faster
            but won't detect in-place edits. Use for providers where edits create new
            message UUIDs rather than modifying existing ones. Default: False.
        incremental: If True, treat the import as a delta/partial update. Messages
            not present in the current import will NOT be soft-deleted. Use when
            importing partial exports or streaming updates. Default: False.
    """
    
    SOURCE_ID: str = None  # Override in subclass
    
    def __init__(
        self, 
        session: Session, 
        assume_immutable: bool = False,
        incremental: bool = False,
    ):
        self.session = session
        self.assume_immutable = assume_immutable
        self.incremental = incremental
        self._message_id_map: dict[str, UUID] = {}  # source_id -> native UUID
        self.counts: dict[str, int] = {}  # Populated by extract_all
    
    def _increment_count(self, key: str, amount: int = 1):
        """Safely increment a count (no-op if counts not initialized)."""
        if key in self.counts:
            self.counts[key] += amount
    
    @abstractmethod
    def extract_dialogue(self, raw: dict[str, Any]) -> str | None:
        """
        Extract a single dialogue and all its contents.
        
        Returns:
            'new' - new dialogue created
            'updated' - existing dialogue updated  
            'skipped' - existing dialogue unchanged
            None - extraction failed
        """
        pass
    
    def extract_all(self, data: list[dict[str, Any]]) -> dict[str, int]:
        """Extract all dialogues from a data list."""
        self.counts = {
            'dialogues_new': 0,
            'dialogues_updated': 0,
            'dialogues_skipped': 0,
            'messages_new': 0,
            'messages_updated': 0,
            'messages_unchanged': 0,
            'messages_restored': 0,
            'messages_soft_deleted': 0,
            'content_parts': 0,
            'failed': 0,
        }
        
        for i, raw in enumerate(data):
            try:
                result = self.extract_dialogue(raw)
                if result == 'new':
                    self.counts['dialogues_new'] += 1
                elif result == 'updated':
                    self.counts['dialogues_updated'] += 1
                elif result == 'skipped':
                    self.counts['dialogues_skipped'] += 1
                elif result is None:
                    self.counts['failed'] += 1
            except Exception as e:
                logger.error(f"Failed to extract dialogue {i}: {e}")
                self.counts['failed'] += 1
                self.session.rollback()
        
        self.session.commit()
        total = self.counts['dialogues_new'] + self.counts['dialogues_updated']
        logger.info(f"{self.SOURCE_ID} extraction complete: {total} processed ({self.counts})")
        return self.counts
    
    def get_existing_dialogue(self, source_id: str) -> Dialogue | None:
        """Check if dialogue already exists."""
        return (
            self.session.query(Dialogue)
            .filter(Dialogue.source == self.SOURCE_ID)
            .filter(Dialogue.source_id == source_id)
            .first()
        )
    
    def get_existing_messages(self, dialogue_id: UUID) -> dict[str, Message]:
        """Get all existing messages for a dialogue, keyed by source_id."""
        messages = (
            self.session.query(Message)
            .filter(Message.dialogue_id == dialogue_id)
            .all()
        )
        return {m.source_id: m for m in messages}
    
    def should_update(self, existing: Dialogue, new_updated_at: datetime | None) -> bool:
        """Determine if existing dialogue should be updated."""
        if new_updated_at is None:
            return False
        if existing.updated_at is None:
            return True
        return new_updated_at > existing.updated_at
    
    def register_message_id(self, source_id: str, native_id: UUID):
        """Register a mapping from source message ID to native UUID."""
        self._message_id_map[source_id] = native_id
    
    def resolve_message_id(self, source_id: str | None) -> UUID | None:
        """Resolve a source message ID to native UUID."""
        if source_id is None:
            return None
        return self._message_id_map.get(source_id)
    
    def _delete_message_content(self, message_id: UUID):
        """Delete content parts and related data for a message."""
        # Content parts cascade delete citations
        self.session.query(ContentPart).filter(
            ContentPart.message_id == message_id
        ).delete()
    
    def _soft_delete_messages(self, messages: list[Message]) -> int:
        """Soft delete messages that are no longer in source."""
        now = datetime.now(timezone.utc)
        count = 0
        for msg in messages:
            if msg.deleted_at is None:
                msg.deleted_at = now
                count += 1
        return count
    
    def _restore_message(self, message: Message):
        """Restore a soft-deleted message."""
        message.deleted_at = None


---
File: llm_archive/extractors/chatgpt.py
---
# llm_archive/extractors/chatgpt.py
"""ChatGPT conversation extractor."""

from datetime import datetime, timezone
from typing import Any
from uuid import UUID

from sqlalchemy import text
from sqlalchemy.orm import Session
from loguru import logger

from llm_archive.models import (
    Dialogue, Message, ContentPart, Citation, Attachment,
    ChatGPTMessageMeta, ChatGPTSearchGroup, ChatGPTSearchEntry,
    ChatGPTCodeExecution, ChatGPTCodeOutput, ChatGPTDalleGeneration,
    ChatGPTCanvasDoc,
)
from llm_archive.extractors.base import (
    BaseExtractor, parse_timestamp, normalize_role, safe_get, compute_content_hash
)
from llm_archive.annotations.core import AnnotationWriter, EntityType


class ChatGPTExtractor(BaseExtractor):
    """Extracts ChatGPT conversations into the raw schema."""
    
    SOURCE_ID = 'chatgpt'
    
    def __init__(
        self, 
        session: Session, 
        assume_immutable: bool = False,
        incremental: bool = False,
    ):
        super().__init__(session, assume_immutable=assume_immutable, incremental=incremental)
        self.annotation_writer = AnnotationWriter(session)
    
    def extract_dialogue(self, raw: dict[str, Any]) -> str | None:
        """
        Extract a complete ChatGPT conversation with incremental updates.
        
        Returns:
            'new' - new dialogue created
            'updated' - existing dialogue updated
            'skipped' - existing dialogue unchanged
            None - extraction failed
        """
        source_id = raw.get('conversation_id') or raw.get('id')
        if not source_id:
            logger.warning("Conversation missing ID, skipping")
            return None
        
        updated_at = parse_timestamp(raw.get('update_time'))
        
        # Check for existing dialogue
        existing = self.get_existing_dialogue(source_id)
        
        if existing:
            if self.should_update(existing, updated_at):
                # Update existing dialogue metadata
                logger.debug(f"Updating dialogue {source_id}")
                existing.title = raw.get('title')
                existing.updated_at = updated_at
                existing.source_json = raw
                dialogue_id = existing.id
                
                # Incremental message sync
                self._message_id_map = {}
                mapping = raw.get('mapping', {})
                self._sync_messages(dialogue_id, mapping)
                
                return 'updated'
            else:
                # Skip - no changes
                logger.debug(f"Skipping unchanged dialogue {source_id}")
                return 'skipped'
        else:
            # Create new dialogue
            dialogue = Dialogue(
                source=self.SOURCE_ID,
                source_id=source_id,
                title=raw.get('title'),
                created_at=parse_timestamp(raw.get('create_time')),
                updated_at=updated_at,
                source_json=raw,
            )
            self.session.add(dialogue)
            self.session.flush()
            dialogue_id = dialogue.id
            
            # Clear message ID map and extract all messages
            self._message_id_map = {}
            mapping = raw.get('mapping', {})
            self._extract_messages_new(dialogue_id, mapping)
            
            return 'new'
    
    def _sync_messages(self, dialogue_id: UUID, mapping: dict[str, Any]):
        """
        Incrementally sync messages - preserve UUIDs for unchanged messages.
        
        This method:
        1. Compares existing messages with new data (unless assume_immutable=True)
        2. Updates changed messages in place
        3. Creates new messages
        4. Soft-deletes messages removed from source
        
        When assume_immutable=True, existing messages are assumed unchanged and
        skipped without hash comparison. This is faster but won't detect edits.
        """
        existing_messages = self.get_existing_messages(dialogue_id)
        seen_source_ids = set()
        
        # First pass: collect message data
        message_data = {}
        for node_id, node in mapping.items():
            msg_data = node.get('message')
            if not msg_data:
                continue
            
            msg_source_id = msg_data.get('id')
            if not msg_source_id:
                continue
            
            message_data[msg_source_id] = {
                'node': node,
                'msg_data': msg_data,
            }
            seen_source_ids.add(msg_source_id)
        
        # Second pass: sync each message
        for source_id, data in message_data.items():
            msg_data = data['msg_data']
            
            if source_id in existing_messages:
                existing = existing_messages[source_id]
                
                if self.assume_immutable:
                    # Fast path: assume content unchanged, just restore if deleted
                    if existing.deleted_at is not None:
                        existing.deleted_at = None
                        logger.debug(f"Restored message {source_id}")
                        self._increment_count('messages_restored')
                    else:
                        self._increment_count('messages_unchanged')
                    self.register_message_id(source_id, existing.id)
                else:
                    # Full check: compute hash and compare
                    new_hash = compute_content_hash(msg_data)
                    
                    if existing.content_hash == new_hash and existing.deleted_at is None:
                        # Unchanged - just register the ID mapping
                        self.register_message_id(source_id, existing.id)
                        self._increment_count('messages_unchanged')
                    else:
                        # Changed or was soft-deleted - update in place
                        was_deleted = existing.deleted_at is not None
                        self._update_message(existing, msg_data, new_hash)
                        self.register_message_id(source_id, existing.id)
                        if was_deleted:
                            self._increment_count('messages_restored')
                        else:
                            self._increment_count('messages_updated')
            else:
                # New message - always compute hash for storage
                new_hash = compute_content_hash(msg_data)
                msg_id = self._create_message(dialogue_id, msg_data, new_hash)
                if msg_id:
                    self.register_message_id(source_id, msg_id)
                    self._increment_count('messages_new')
        
        # Third pass: update parent links (now that all messages exist)
        for source_id, data in message_data.items():
            node = data['node']
            parent_node_id = node.get('parent')
            
            # Look up parent node and get its message ID
            if parent_node_id and parent_node_id in mapping:
                parent_node = mapping[parent_node_id]
                parent_msg_data = parent_node.get('message')
                
                if parent_msg_data:
                    parent_source_id = parent_msg_data.get('id')
                    
                    native_id = self.resolve_message_id(source_id)
                    parent_native_id = self.resolve_message_id(parent_source_id)
                    
                    if native_id:
                        msg = self.session.get(Message, native_id)
                        if msg and msg.parent_id != parent_native_id:
                            msg.parent_id = parent_native_id
        
        # Fourth pass: soft-delete messages no longer in source (unless incremental mode)
        if not self.incremental:
            for source_id, existing in existing_messages.items():
                if source_id not in seen_source_ids and existing.deleted_at is None:
                    existing.deleted_at = datetime.now(timezone.utc)
                    logger.debug(f"Soft-deleted message {source_id}")
                    self._increment_count('messages_soft_deleted')
    
    def _update_message(self, message: Message, msg_data: dict[str, Any], content_hash: str):
        """Update an existing message in place."""
        # Update message fields
        message.role = normalize_role(safe_get(msg_data, 'author', 'role'), self.SOURCE_ID)
        message.author_id = safe_get(msg_data, 'author', 'metadata', 'user_id')
        message.author_name = safe_get(msg_data, 'author', 'name')
        message.created_at = parse_timestamp(msg_data.get('create_time'))
        message.updated_at = parse_timestamp(msg_data.get('update_time'))
        message.content_hash = content_hash
        message.source_json = msg_data
        
        # Restore if was soft-deleted
        if message.deleted_at is not None:
            message.deleted_at = None
            logger.debug(f"Restored message {message.source_id}")
        
        # Delete related data before re-extracting
        self._delete_message_content(message.id)
        self._delete_message_metadata(message.id)
        self._delete_message_annotations(message.id)
        
        # Re-extract related data
        self._extract_content_parts(message.id, msg_data)
        self._extract_attachments(message.id, msg_data)
        self._extract_chatgpt_meta(message.id, msg_data)
    
    def _delete_message_metadata(self, message_id: UUID):
        """Delete ChatGPT-specific metadata for a message."""
        self.session.query(ChatGPTMessageMeta).filter(
            ChatGPTMessageMeta.message_id == message_id
        ).delete()
        self.session.query(Attachment).filter(
            Attachment.message_id == message_id
        ).delete()
    
    def _delete_message_annotations(self, message_id: UUID):
        """Delete annotations for a message (for re-extraction)."""
        # Delete from all message annotation tables
        for value_type in ['flag', 'string', 'numeric', 'json']:
            self.session.execute(
                text(f"DELETE FROM derived.message_annotations_{value_type} WHERE entity_id = :id"),
                {'id': message_id}
            )
    
    def _create_message(self, dialogue_id: UUID, msg_data: dict[str, Any], content_hash: str) -> UUID | None:
        """Create a new message."""
        source_id = msg_data.get('id')
        if not source_id:
            return None
        
        message = Message(
            dialogue_id=dialogue_id,
            source_id=source_id,
            parent_id=None,  # Set in later pass
            role=normalize_role(safe_get(msg_data, 'author', 'role'), self.SOURCE_ID),
            author_id=safe_get(msg_data, 'author', 'metadata', 'user_id'),
            author_name=safe_get(msg_data, 'author', 'name'),
            created_at=parse_timestamp(msg_data.get('create_time')),
            updated_at=parse_timestamp(msg_data.get('update_time')),
            content_hash=content_hash,
            source_json=msg_data,
        )
        self.session.add(message)
        self.session.flush()
        
        # Extract content parts and metadata
        self._extract_content_parts(message.id, msg_data)
        self._extract_attachments(message.id, msg_data)
        self._extract_chatgpt_meta(message.id, msg_data)
        
        return message.id
    
    def _extract_messages_new(self, dialogue_id: UUID, mapping: dict[str, Any]):
        """Extract all messages for a new dialogue."""
        # First pass: create all messages without parent links
        for node_id, node in mapping.items():
            msg_data = node.get('message')
            if not msg_data:
                continue
            
            content_hash = compute_content_hash(msg_data)
            msg_id = self._create_message(dialogue_id, msg_data, content_hash)
            if msg_id:
                self.register_message_id(msg_data.get('id'), msg_id)
                self._increment_count('messages_new')
        
        # Second pass: set parent links
        for node_id, node in mapping.items():
            msg_data = node.get('message')
            if not msg_data:
                continue
            
            source_id = msg_data.get('id')
            parent_node_id = node.get('parent')
            
            # Look up parent node and get its message ID
            if parent_node_id and parent_node_id in mapping:
                parent_node = mapping[parent_node_id]
                parent_msg_data = parent_node.get('message')
                
                if parent_msg_data:
                    parent_source_id = parent_msg_data.get('id')
                    
                    native_id = self.resolve_message_id(source_id)
                    parent_native_id = self.resolve_message_id(parent_source_id)
                    
                    if native_id and parent_native_id:
                        msg = self.session.get(Message, native_id)
                        if msg:
                            msg.parent_id = parent_native_id
    
    def _extract_content_parts(self, message_id: UUID, msg_data: dict[str, Any]):
        """Extract content parts from a message."""
        content = msg_data.get('content', {})
        parts = content.get('parts', [])
        
        for seq, part in enumerate(parts):
            part_info = self._classify_content_part(part)
            
            content_part = ContentPart(
                message_id=message_id,
                sequence=seq,
                part_type=part_info.get('part_type', 'unknown'),
                text_content=part_info.get('text_content'),
                language=part_info.get('language'),
                media_type=part_info.get('media_type'),
                url=part_info.get('url'),
                source_json=part_info.get('source_json', {}),
            )
            self.session.add(content_part)
            self.session.flush()
            self._increment_count('content_parts')
            
            # Extract DALL-E generations if present
            if isinstance(part, dict):
                self._extract_dalle_generation(content_part.id, part)
        
        # Extract citations from metadata
        metadata = msg_data.get('metadata', {})
        citations = metadata.get('citations', [])
        
        # Link citations to first text content part (if any)
        if citations and parts:
            first_part = self.session.query(ContentPart).filter(
                ContentPart.message_id == message_id,
                ContentPart.sequence == 0
            ).first()
            
            if first_part:
                self._extract_citations(first_part.id, citations)
    
    def _classify_content_part(self, part: str | dict[str, Any]) -> dict[str, Any]:
        """
        Classify a content part and extract all relevant fields.
        
        Returns dict with: part_type, text_content, language, media_type, url, source_json
        """
        if isinstance(part, str):
            return {
                'part_type': 'text',
                'text_content': part,
                'source_json': {'text': part},
            }
        
        if not isinstance(part, dict):
            return {
                'part_type': 'unknown',
                'source_json': {'raw': str(part)},
            }
        
        content_type = part.get('content_type', '')
        result = {'source_json': part}
        
        # Image content
        if 'image' in content_type:
            result['part_type'] = 'image'
            result['media_type'] = part.get('content_type')
            
            # Try to find URL in various places
            asset_pointer = part.get('asset_pointer', '')
            if asset_pointer and asset_pointer.startswith('file-service://'):
                result['url'] = asset_pointer
            elif part.get('url'):
                result['url'] = part.get('url')
            
            return result
        
        # Audio content
        if 'audio' in content_type:
            result['part_type'] = 'audio'
            result['media_type'] = part.get('content_type')
            result['url'] = part.get('url') or part.get('asset_pointer')
            return result
        
        # Video content
        if 'video' in content_type:
            result['part_type'] = 'video'
            result['media_type'] = part.get('content_type')
            result['url'] = part.get('url') or part.get('asset_pointer')
            return result
        
        # Code content (from code interpreter)
        if content_type == 'code' or part.get('language'):
            result['part_type'] = 'code'
            result['language'] = part.get('language')
            result['text_content'] = part.get('text') or part.get('code')
            return result
        
        # Text content - might be in various places
        text = part.get('text') or part.get('result') or part.get('content')
        if text and isinstance(text, str):
            result['part_type'] = 'text'
            result['text_content'] = text
            return result
        
        # Fallback
        result['part_type'] = content_type or 'unknown'
        return result
    
    def _extract_citations(self, content_part_id: UUID, citations: list[dict[str, Any]]):
        """Extract citations from metadata."""
        for cit in citations:
            meta = cit.get('metadata', {})
            
            citation = Citation(
                content_part_id=content_part_id,
                source_id=None,
                url=meta.get('url'),
                title=meta.get('title'),
                snippet=meta.get('text'),
                published_at=parse_timestamp(meta.get('pub_date')),
                start_index=cit.get('start_ix'),
                end_index=cit.get('end_ix'),
                citation_type=meta.get('type'),
                source_json=cit,
            )
            self.session.add(citation)
    
    def _extract_attachments(self, message_id: UUID, msg_data: dict[str, Any]):
        """Extract attachments from message metadata."""
        metadata = msg_data.get('metadata', {})
        attachments = metadata.get('attachments', [])
        
        for att in attachments:
            attachment = Attachment(
                message_id=message_id,
                file_name=att.get('name'),
                file_type=att.get('mime_type') or att.get('mimeType'),
                file_size=att.get('size'),
                extracted_text=None,  # ChatGPT doesn't provide this
                source_json=att,
            )
            self.session.add(attachment)
    
    def _extract_chatgpt_meta(self, message_id: UUID, msg_data: dict[str, Any]):
        """
        Extract ChatGPT-specific metadata and write annotations.
        
        Writes:
        - gizmo_id as message string annotation
        - has_gizmo as message flag annotation
        - model_slug as message string annotation
        
        Also creates ChatGPTMessageMeta record for backwards compatibility.
        """
        metadata = msg_data.get('metadata', {})
        
        # Write gizmo_id as message annotation (if present)
        gizmo_id = metadata.get('gizmo_id')
        if gizmo_id:
            self.annotation_writer.write_string(
                entity_type=EntityType.MESSAGE,
                entity_id=message_id,
                key='gizmo_id',
                value=gizmo_id,
                source='ingestion',
                source_version='chatgpt_extractor_1.0',
            )
            # Also write a flag for "has gizmo" for easier filtering
            self.annotation_writer.write_flag(
                entity_type=EntityType.MESSAGE,
                entity_id=message_id,
                key='has_gizmo',
                source='ingestion',
            )
        
        # Write model_slug as annotation (useful for filtering by model)
        model_slug = metadata.get('model_slug')
        if model_slug:
            self.annotation_writer.write_string(
                entity_type=EntityType.MESSAGE,
                entity_id=message_id,
                key='model_slug',
                value=model_slug,
                source='ingestion',
            )
        
        # Create ChatGPTMessageMeta record (backwards compatibility)
        meta = ChatGPTMessageMeta(
            message_id=message_id,
            model_slug=model_slug,
            status=msg_data.get('status'),
            end_turn=msg_data.get('end_turn'),
            gizmo_id=gizmo_id,
            source_json=metadata,
        )
        self.session.add(meta)
        
        # Search result groups
        search_groups = metadata.get('search_result_groups', [])
        for group_data in search_groups:
            self._extract_search_group(message_id, group_data)
        
        # Code executions
        agg_result = metadata.get('aggregate_result')
        if agg_result:
            self._extract_code_execution(message_id, agg_result)
        
        # Canvas documents
        canvas = metadata.get('canvas')
        if canvas:
            self._extract_canvas_doc(message_id, canvas)
    
    def _extract_search_group(self, message_id: UUID, group_data: dict[str, Any]):
        """Extract a search result group and its entries."""
        group = ChatGPTSearchGroup(
            message_id=message_id,
            group_type=group_data.get('type'),
            domain=group_data.get('domain'),
            source_json=group_data,
        )
        self.session.add(group)
        self.session.flush()
        
        entries = group_data.get('entries', [])
        for seq, entry_data in enumerate(entries):
            entry = ChatGPTSearchEntry(
                group_id=group.id,
                sequence=seq,
                url=entry_data.get('url'),
                title=entry_data.get('title'),
                snippet=entry_data.get('snippet'),
                published_at=parse_timestamp(entry_data.get('pub_date')),
                attribution=entry_data.get('attribution'),
                source_json=entry_data,
            )
            self.session.add(entry)
    
    def _extract_code_execution(self, message_id: UUID, agg_result: dict[str, Any]):
        """Extract code execution data."""
        exception = agg_result.get('in_kernel_exception') or {}
        
        execution = ChatGPTCodeExecution(
            message_id=message_id,
            run_id=agg_result.get('run_id'),
            status=agg_result.get('status'),
            code=agg_result.get('code'),
            started_at=parse_timestamp(agg_result.get('start_time')),
            ended_at=parse_timestamp(agg_result.get('end_time')),
            final_output=agg_result.get('final_expression_output'),
            exception_name=exception.get('name'),
            exception_traceback='\n'.join(exception.get('traceback', [])) or None,
            source_json=agg_result,
        )
        self.session.add(execution)
        self.session.flush()
        
        # Extract outputs
        messages = agg_result.get('messages', [])
        for seq, msg in enumerate(messages):
            output = ChatGPTCodeOutput(
                execution_id=execution.id,
                sequence=seq,
                output_type=msg.get('message_type'),
                stream_name=msg.get('stream_name'),
                text_content=msg.get('text'),
                image_url=msg.get('image_url'),
                source_json=msg,
            )
            self.session.add(output)
    
    def _extract_dalle_generation(self, content_part_id: UUID, part: dict[str, Any]):
        """Extract DALL-E generation data from a content part."""
        metadata = part.get('metadata') or {}
        dalle = metadata.get('dalle') or metadata.get('generation')
        
        if not dalle:
            return
        
        generation = ChatGPTDalleGeneration(
            content_part_id=content_part_id,
            gen_id=dalle.get('gen_id'),
            prompt=dalle.get('prompt'),
            seed=dalle.get('seed'),
            parent_gen_id=dalle.get('parent_gen_id'),
            edit_op=dalle.get('edit_op'),
            width=dalle.get('width') or part.get('width'),
            height=dalle.get('height') or part.get('height'),
            source_json=dalle,
        )
        self.session.add(generation)
    
    def _extract_canvas_doc(self, message_id: UUID, canvas: dict[str, Any]):
        """
        Extract canvas document as content_part + annotations.
        
        Creates:
        - content_part with part_type='canvas' containing the canvas content
        - String annotation for canvas title
        - String annotation for textdoc_id (for version tracking)
        - Numeric annotation for version number
        - ChatGPTCanvasDoc record for backwards compatibility
        """
        textdoc_id = canvas.get('textdoc_id')
        version = canvas.get('version')
        title = canvas.get('title')
        
        # Get canvas content (may be in different fields depending on export format)
        canvas_content = canvas.get('content') or canvas.get('textdoc_content')
        
        # Get current max sequence for this message
        max_seq_result = self.session.execute(
            text("""
                SELECT COALESCE(MAX(sequence), -1) 
                FROM raw.content_parts 
                WHERE message_id = :msg_id
            """),
            {'msg_id': message_id}
        )
        max_seq = max_seq_result.scalar() or -1
        
        # Create content_part for canvas
        content_part = ContentPart(
            message_id=message_id,
            sequence=max_seq + 1,
            part_type='canvas',
            text_content=canvas_content,
            source_json=canvas,
        )
        self.session.add(content_part)
        self.session.flush()
        
        # Write canvas title as content_part annotation
        if title:
            self.annotation_writer.write_string(
                entity_type=EntityType.CONTENT_PART,
                entity_id=content_part.id,
                key='title',
                value=title,
                source='ingestion',
                reason='canvas_metadata',
            )
        
        # Write textdoc_id for version tracking
        if textdoc_id:
            self.annotation_writer.write_string(
                entity_type=EntityType.CONTENT_PART,
                entity_id=content_part.id,
                key='textdoc_id',
                value=textdoc_id,
                source='ingestion',
            )
        
        # Write version number
        if version is not None:
            self.annotation_writer.write_numeric(
                entity_type=EntityType.CONTENT_PART,
                entity_id=content_part.id,
                key='canvas_version',
                value=version,
                source='ingestion',
            )
        
        # Write document type
        textdoc_type = canvas.get('textdoc_type')
        if textdoc_type:
            self.annotation_writer.write_string(
                entity_type=EntityType.CONTENT_PART,
                entity_id=content_part.id,
                key='textdoc_type',
                value=textdoc_type,
                source='ingestion',
            )
        
        # Create ChatGPTCanvasDoc record for backwards compatibility
        doc = ChatGPTCanvasDoc(
            message_id=message_id,
            textdoc_id=textdoc_id,
            textdoc_type=textdoc_type,
            version=version,
            title=title,
            from_version=canvas.get('from_version'),
            content_length=canvas.get('textdoc_content_length'),
            has_user_edit=canvas.get('has_user_edit'),
            source_json=canvas,
        )
        self.session.add(doc)


# ============================================================
# Post-extraction utilities
# ============================================================

def mark_latest_canvas_versions(session: Session) -> int:
    """
    Mark the latest version of each canvas document.
    
    Run this after extraction is complete. For each textdoc_id,
    finds the content_part with the highest version number and
    marks it with 'is_latest_canvas_version' flag.
    
    Returns:
        Count of canvas documents marked as latest.
    
    Usage:
        extractor.extract_all(data)
        session.commit()
        mark_latest_canvas_versions(session)
    """
    writer = AnnotationWriter(session)
    
    # Find latest version for each textdoc_id
    latest_versions = session.execute(
        text("""
            WITH canvas_versions AS (
                SELECT 
                    cp.id as content_part_id,
                    s.annotation_value as textdoc_id,
                    n.annotation_value as version_num,
                    ROW_NUMBER() OVER (
                        PARTITION BY s.annotation_value 
                        ORDER BY n.annotation_value DESC
                    ) as rn
                FROM raw.content_parts cp
                JOIN derived.content_part_annotations_string s 
                    ON s.entity_id = cp.id AND s.annotation_key = 'textdoc_id'
                JOIN derived.content_part_annotations_numeric n
                    ON n.entity_id = cp.id AND n.annotation_key = 'canvas_version'
            )
            SELECT content_part_id, textdoc_id
            FROM canvas_versions
            WHERE rn = 1
        """)
    )
    
    count = 0
    for row in latest_versions:
        writer.write_flag(
            entity_type=EntityType.CONTENT_PART,
            entity_id=row.content_part_id,
            key='is_latest_canvas_version',
            source='ingestion',
            reason='highest_version_number',
        )
        count += 1
    
    session.commit()
    return count


def find_wiki_gizmo_messages(session: Session, gizmo_id: str) -> list[UUID]:
    """
    Find all message IDs that used a specific gizmo.
    
    Useful for identifying likely wiki article candidates
    if you know which gizmo was used to generate them.
    
    Args:
        session: Database session
        gizmo_id: The gizmo ID to search for (e.g., 'g-xxxxx')
        
    Returns:
        List of message UUIDs
    """
    result = session.execute(
        text("""
            SELECT entity_id 
            FROM derived.message_annotations_string 
            WHERE annotation_key = 'gizmo_id' 
              AND annotation_value = :gizmo_id
        """),
        {'gizmo_id': gizmo_id}
    )
    return [row[0] for row in result]



---
File: llm_archive/extractors/claude.py
---
# llm_archive/extractors/claude.py
"""Claude conversation extractor."""

from datetime import datetime, timezone
from typing import Any
from uuid import UUID

from sqlalchemy.orm import Session
from loguru import logger

from llm_archive.models import (
    Dialogue, Message, ContentPart, Citation, Attachment,
    ClaudeMessageMeta,
)
from llm_archive.extractors.base import (
    BaseExtractor, parse_timestamp, normalize_role, safe_get, compute_content_hash
)


class ClaudeExtractor(BaseExtractor):
    """Extracts Claude conversations into the raw schema."""
    
    SOURCE_ID = 'claude'
    
    def __init__(
        self, 
        session: Session, 
        assume_immutable: bool = False,
        incremental: bool = False,
    ):
        super().__init__(session, assume_immutable=assume_immutable, incremental=incremental)
    
    def extract_dialogue(self, raw: dict[str, Any]) -> str | None:
        """
        Extract a complete Claude conversation with incremental updates.
        
        Returns:
            'new' - new dialogue created
            'updated' - existing dialogue updated
            'skipped' - existing dialogue unchanged
            None - extraction failed
        """
        source_id = raw.get('uuid')
        if not source_id:
            logger.warning("Conversation missing UUID, skipping")
            return None
        
        updated_at = parse_timestamp(raw.get('updated_at'))
        
        # Check for existing dialogue
        existing = self.get_existing_dialogue(source_id)
        
        if existing:
            if self.should_update(existing, updated_at):
                # Update existing dialogue metadata
                logger.debug(f"Updating dialogue {source_id}")
                existing.title = raw.get('name')
                existing.updated_at = updated_at
                existing.source_json = raw
                dialogue_id = existing.id
                
                # Incremental message sync
                self._message_id_map = {}
                chat_messages = raw.get('chat_messages', [])
                self._sync_messages(dialogue_id, chat_messages)
                
                return 'updated'
            else:
                # Skip - no changes
                logger.debug(f"Skipping unchanged dialogue {source_id}")
                return 'skipped'
        else:
            # Create new dialogue
            dialogue = Dialogue(
                source=self.SOURCE_ID,
                source_id=source_id,
                title=raw.get('name'),
                created_at=parse_timestamp(raw.get('created_at')),
                updated_at=updated_at,
                source_json=raw,
            )
            self.session.add(dialogue)
            self.session.flush()
            dialogue_id = dialogue.id
            
            # Clear message ID map and extract all messages
            self._message_id_map = {}
            chat_messages = raw.get('chat_messages', [])
            self._extract_messages_new(dialogue_id, chat_messages)
            
            return 'new'
    
    def _sync_messages(self, dialogue_id: UUID, chat_messages: list[dict[str, Any]]):
        """
        Incrementally sync messages - preserve UUIDs for unchanged messages.
        
        Claude conversations are linear, so we maintain the chain structure.
        
        When assume_immutable=True, existing messages are assumed unchanged and
        skipped without hash comparison. This is faster but won't detect edits.
        """
        existing_messages = self.get_existing_messages(dialogue_id)
        seen_source_ids = set()
        
        prev_message_id = None
        
        for msg_data in chat_messages:
            source_id = msg_data.get('uuid')
            if not source_id:
                continue
            
            seen_source_ids.add(source_id)
            
            if source_id in existing_messages:
                existing = existing_messages[source_id]
                
                if self.assume_immutable:
                    # Fast path: assume content unchanged, just restore if deleted
                    if existing.deleted_at is not None:
                        existing.deleted_at = None
                        logger.debug(f"Restored message {source_id}")
                        self._increment_count('messages_restored')
                    else:
                        self._increment_count('messages_unchanged')
                    # Update parent link if needed (chain structure may have changed)
                    if existing.parent_id != prev_message_id:
                        existing.parent_id = prev_message_id
                    self.register_message_id(source_id, existing.id)
                    prev_message_id = existing.id
                else:
                    # Full check: compute hash and compare
                    content_hash = compute_content_hash(msg_data)
                    
                    if existing.content_hash == content_hash and existing.deleted_at is None:
                        # Unchanged - just update parent link if needed and register
                        if existing.parent_id != prev_message_id:
                            existing.parent_id = prev_message_id
                        self.register_message_id(source_id, existing.id)
                        prev_message_id = existing.id
                        self._increment_count('messages_unchanged')
                    else:
                        # Changed or was soft-deleted - update in place
                        was_deleted = existing.deleted_at is not None
                        self._update_message(existing, msg_data, content_hash, prev_message_id)
                        self.register_message_id(source_id, existing.id)
                        prev_message_id = existing.id
                        if was_deleted:
                            self._increment_count('messages_restored')
                        else:
                            self._increment_count('messages_updated')
            else:
                # New message - always compute hash for storage
                content_hash = compute_content_hash(msg_data)
                msg_id = self._create_message(dialogue_id, msg_data, content_hash, prev_message_id)
                if msg_id:
                    self.register_message_id(source_id, msg_id)
                    prev_message_id = msg_id
                    self._increment_count('messages_new')
        
        # Soft-delete messages no longer in source (unless incremental mode)
        if not self.incremental:
            for source_id, existing in existing_messages.items():
                if source_id not in seen_source_ids and existing.deleted_at is None:
                    existing.deleted_at = datetime.now(timezone.utc)
                    logger.debug(f"Soft-deleted message {source_id}")
                    self._increment_count('messages_soft_deleted')
    
    def _update_message(
        self, 
        message: Message, 
        msg_data: dict[str, Any], 
        content_hash: str,
        parent_id: UUID | None
    ):
        """Update an existing message in place."""
        sender = msg_data.get('sender', 'unknown')
        
        message.parent_id = parent_id
        message.role = normalize_role(sender, self.SOURCE_ID)
        message.created_at = parse_timestamp(msg_data.get('created_at'))
        message.updated_at = parse_timestamp(msg_data.get('updated_at'))
        message.content_hash = content_hash
        message.source_json = msg_data
        
        # Restore if was soft-deleted
        if message.deleted_at is not None:
            message.deleted_at = None
            logger.debug(f"Restored message {message.source_id}")
        
        # Delete related data before re-extracting
        self._delete_message_content(message.id)
        self._delete_message_metadata(message.id)
        
        # Re-extract related data
        self._extract_content_parts(message.id, msg_data)
        self._extract_attachments(message.id, msg_data)
        self._extract_claude_meta(message.id, msg_data)
    
    def _delete_message_metadata(self, message_id: UUID):
        """Delete Claude-specific metadata for a message."""
        self.session.query(ClaudeMessageMeta).filter(
            ClaudeMessageMeta.message_id == message_id
        ).delete()
        self.session.query(Attachment).filter(
            Attachment.message_id == message_id
        ).delete()
    
    def _create_message(
        self, 
        dialogue_id: UUID, 
        msg_data: dict[str, Any],
        content_hash: str,
        parent_id: UUID | None
    ) -> UUID | None:
        """Create a new message."""
        source_id = msg_data.get('uuid')
        if not source_id:
            return None
        
        sender = msg_data.get('sender', 'unknown')
        
        message = Message(
            dialogue_id=dialogue_id,
            source_id=source_id,
            parent_id=parent_id,
            role=normalize_role(sender, self.SOURCE_ID),
            author_id=None,
            author_name=None,
            created_at=parse_timestamp(msg_data.get('created_at')),
            updated_at=parse_timestamp(msg_data.get('updated_at')),
            content_hash=content_hash,
            source_json=msg_data,
        )
        self.session.add(message)
        self.session.flush()
        
        self.register_message_id(source_id, message.id)
        
        # Extract content parts and metadata
        self._extract_content_parts(message.id, msg_data)
        self._extract_attachments(message.id, msg_data)
        self._extract_claude_meta(message.id, msg_data)
        
        return message.id
    
    def _extract_messages_new(self, dialogue_id: UUID, chat_messages: list[dict[str, Any]]):
        """Extract all messages for a new dialogue."""
        prev_message_id = None
        
        for msg_data in chat_messages:
            content_hash = compute_content_hash(msg_data)
            message_id = self._create_message(dialogue_id, msg_data, content_hash, prev_message_id)
            if message_id:
                prev_message_id = message_id
                self._increment_count('messages_new')
    
    def _extract_content_parts(self, message_id: UUID, msg_data: dict[str, Any]):
        """Extract content parts from a Claude message."""
        # Claude has top-level 'text' field and structured 'content' array
        
        # First, add the main text as a content part
        main_text = msg_data.get('text')
        content_array = msg_data.get('content', [])
        
        # If there's structured content, use that
        if content_array:
            for seq, part in enumerate(content_array):
                part_info = self._classify_content_part(part)
                
                content_part = ContentPart(
                    message_id=message_id,
                    sequence=seq,
                    part_type=part_info.get('part_type', 'unknown'),
                    text_content=part_info.get('text_content'),
                    language=part_info.get('language'),
                    media_type=part_info.get('media_type'),
                    url=part_info.get('url'),
                    tool_name=part_info.get('tool_name'),
                    tool_use_id=part_info.get('tool_use_id'),
                    tool_input=part_info.get('tool_input'),
                    started_at=parse_timestamp(part.get('start_timestamp')),
                    ended_at=parse_timestamp(part.get('stop_timestamp')),
                    is_error=part_info.get('is_error') or part.get('is_error', False),
                    source_json=part,
                )
                self.session.add(content_part)
                self.session.flush()
                self._increment_count('content_parts')
                
                # Extract citations within this content part
                citations = part.get('citations', [])
                if citations:
                    self._extract_citations(content_part.id, citations)
        
        elif main_text:
            # Fall back to main text field
            content_part = ContentPart(
                message_id=message_id,
                sequence=0,
                part_type='text',
                text_content=main_text,
                source_json={'text': main_text},
            )
            self.session.add(content_part)
            self._increment_count('content_parts')
    
    def _classify_content_part(self, part: dict[str, Any]) -> dict[str, Any]:
        """
        Classify a Claude content part and extract all relevant fields.
        
        Returns dict with: part_type, text_content, tool_name, tool_use_id, tool_input, media_type, url
        """
        part_type = part.get('type', 'unknown').lower()
        
        # Map Claude types to our taxonomy
        type_map = {
            'text': 'text',
            'tool_use': 'tool_use',
            'tool_result': 'tool_result',
            'thinking': 'thinking',
            'image': 'image',
        }
        
        result = {
            'part_type': type_map.get(part_type, part_type),
        }
        
        # Extract fields based on type
        if part_type == 'text':
            result['text_content'] = part.get('text')
        
        elif part_type == 'thinking':
            result['text_content'] = part.get('thinking')
        
        elif part_type == 'tool_use':
            result['tool_name'] = part.get('name')
            result['tool_use_id'] = part.get('id')
            result['tool_input'] = part.get('input')
            # Some tools have text output
            if isinstance(part.get('input'), dict):
                # Try to capture any query or text input
                result['text_content'] = part['input'].get('query') or part['input'].get('text')
        
        elif part_type == 'tool_result':
            result['tool_use_id'] = part.get('tool_use_id')
            result['is_error'] = part.get('is_error', False)
            
            # Tool results might have text in various places
            content = part.get('content')
            if isinstance(content, str):
                result['text_content'] = content
            elif isinstance(content, list):
                # Concatenate text from nested content
                texts = []
                for item in content:
                    if isinstance(item, dict) and item.get('text'):
                        texts.append(item['text'])
                    elif isinstance(item, str):
                        texts.append(item)
                result['text_content'] = '\n'.join(texts) if texts else None
        
        elif part_type == 'image':
            result['media_type'] = part.get('media_type')
            # Claude images might have URL or base64 source
            source = part.get('source', {})
            if source.get('type') == 'url':
                result['url'] = source.get('url')
            # base64 data stays in source_json
        
        return result
    
    def _extract_citations(self, content_part_id: UUID, citations: list[dict[str, Any]]):
        """Extract citations from a content part."""
        for cit in citations:
            details = cit.get('details', {})
            
            citation = Citation(
                content_part_id=content_part_id,
                source_id=cit.get('uuid'),
                url=details.get('url'),
                title=None,  # Claude citations don't include title
                snippet=None,
                published_at=None,
                start_index=cit.get('start_index'),
                end_index=cit.get('end_index'),
                citation_type=details.get('type'),
                source_json=cit,
            )
            self.session.add(citation)
    
    def _extract_attachments(self, message_id: UUID, msg_data: dict[str, Any]):
        """Extract attachments from a Claude message."""
        attachments = msg_data.get('attachments', [])
        
        for att in attachments:
            attachment = Attachment(
                message_id=message_id,
                file_name=att.get('file_name'),
                file_type=att.get('file_type'),
                file_size=att.get('file_size'),
                extracted_text=att.get('extracted_content'),
                source_json=att,
            )
            self.session.add(attachment)
        
        # Also check 'files' array
        files = msg_data.get('files', [])
        for f in files:
            # Files array is simpler, just has file_name
            if isinstance(f, dict) and f.get('file_name'):
                attachment = Attachment(
                    message_id=message_id,
                    file_name=f.get('file_name'),
                    source_json=f,
                )
                self.session.add(attachment)
    
    def _extract_claude_meta(self, message_id: UUID, msg_data: dict[str, Any]):
        """Extract Claude-specific metadata."""
        # For now, just store the raw message data
        # We can add specific fields as we discover what's useful
        meta = ClaudeMessageMeta(
            message_id=message_id,
            source_json=msg_data,
        )
        self.session.add(meta)


---
File: llm_archive/models/__init__.py
---
# llm_archive/models/__init__.py
"""SQLAlchemy models for raw and derived schemas."""

from llm_archive.models.raw import (
    Base,
    Source,
    Dialogue,
    Message,
    ContentPart,
    Citation,
    Attachment,
    ChatGPTMessageMeta,
    ChatGPTSearchGroup,
    ChatGPTSearchEntry,
    ChatGPTCodeExecution,
    ChatGPTCodeOutput,
    ChatGPTDalleGeneration,
    ChatGPTCanvasDoc,
    ClaudeMessageMeta,
)

from llm_archive.models.derived import (
    PromptResponse,
)

__all__ = [
    # Base
    "Base",
    # Raw core
    "Source",
    "Dialogue",
    "Message",
    "ContentPart",
    "Citation",
    "Attachment",
    # Raw ChatGPT extensions
    "ChatGPTMessageMeta",
    "ChatGPTSearchGroup",
    "ChatGPTSearchEntry",
    "ChatGPTCodeExecution",
    "ChatGPTCodeOutput",
    "ChatGPTDalleGeneration",
    "ChatGPTCanvasDoc",
    # Raw Claude extensions
    "ClaudeMessageMeta",
    # Derived
    "PromptResponse",
    "PromptResponseContent",
]



---
File: llm_archive/models/derived.py
---
# llm_archive/models/derived.py
"""SQLAlchemy models for derived.* schema tables."""

from sqlalchemy import (
    Column, String, Integer, Boolean, DateTime, Float, ForeignKey, Text,
    ARRAY, func
)
from sqlalchemy.dialects.postgresql import UUID as PG_UUID, JSONB
from sqlalchemy.orm import relationship

from llm_archive.models.raw import Base


# ----------------------------------------------------------------------
# Prompt-Response Pairs (unique by response)
# ----------------------------------------------------------------------

class PromptResponse(Base):
    """
    Direct prompt-response association without tree dependency.
    
    Each record pairs a user prompt with one of its responses.
    A prompt can have multiple responses (regenerations).
    Each response appears in exactly one record.
    """
    __tablename__ = "prompt_responses"
    __table_args__ = {"schema": "derived"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid())
    dialogue_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.dialogues.id", ondelete="CASCADE"), nullable=False)
    
    prompt_message_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.messages.id"), nullable=False)
    response_message_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.messages.id"), nullable=False)
    
    prompt_position = Column(Integer, nullable=False)
    response_position = Column(Integer, nullable=False)
    
    prompt_role = Column(String, nullable=False)
    response_role = Column(String, nullable=False)
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())



---
File: llm_archive/models/raw.py
---
# llm_archive/models/raw.py
"""SQLAlchemy models for raw.* schema tables."""

from datetime import datetime
from uuid import UUID

from sqlalchemy import (
    Column, String, Integer, Boolean, DateTime, ForeignKey, Text, BigInteger,
    ARRAY, func
)
from sqlalchemy.dialects.postgresql import UUID as PG_UUID, JSONB
from sqlalchemy.orm import declarative_base, relationship

Base = declarative_base()


# ============================================================
# Core Tables
# ============================================================

class Source(Base):
    """Registry of dialogue sources."""
    __tablename__ = "sources"
    __table_args__ = {"schema": "raw"}
    
    id = Column(String, primary_key=True)
    display_name = Column(String, nullable=False)
    has_native_trees = Column(Boolean, nullable=False)
    role_vocabulary = Column(ARRAY(String), nullable=False)
    source_metadata = Column(JSONB, name="metadata")  # 'metadata' reserved by SQLAlchemy
    
    dialogues = relationship("Dialogue", back_populates="source_rel")


class Dialogue(Base):
    """Universal dialogue container."""
    __tablename__ = "dialogues"
    __table_args__ = {"schema": "raw"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid())
    source = Column(String, ForeignKey("raw.sources.id"), nullable=False)
    source_id = Column(String, nullable=False)
    
    title = Column(String)
    
    # Source timestamps (from archive)
    source_created_at = Column(DateTime(timezone=True))
    source_updated_at = Column(DateTime(timezone=True))
    
    source_json = Column(JSONB, nullable=False)
    
    # DB timestamps
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now())
    
    source_rel = relationship("Source", back_populates="dialogues")
    messages = relationship("Message", back_populates="dialogue", cascade="all, delete-orphan")


class Message(Base):
    """Universal message with tree structure support."""
    __tablename__ = "messages"
    __table_args__ = {"schema": "raw"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid())
    dialogue_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.dialogues.id", ondelete="CASCADE"), nullable=False)
    source_id = Column(String, nullable=False)
    
    parent_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.messages.id"))
    
    role = Column(String, nullable=False)
    author_id = Column(String)
    author_name = Column(String)
    
    # Source timestamps (from archive)
    source_created_at = Column(DateTime(timezone=True))
    source_updated_at = Column(DateTime(timezone=True))
    
    # Change tracking
    content_hash = Column(String)  # hash of content for change detection
    deleted_at = Column(DateTime(timezone=True))  # soft delete (removed from source)
    
    source_json = Column(JSONB, nullable=False)
    
    # DB timestamps
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now())
    
    dialogue = relationship("Dialogue", back_populates="messages")
    parent = relationship("Message", remote_side=[id], backref="children")
    content_parts = relationship("ContentPart", back_populates="message", cascade="all, delete-orphan")
    attachments = relationship("Attachment", back_populates="message", cascade="all, delete-orphan")


class ContentPart(Base):
    """Content segments within a message."""
    __tablename__ = "content_parts"
    __table_args__ = {"schema": "raw"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid())
    message_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.messages.id", ondelete="CASCADE"), nullable=False)
    sequence = Column(Integer, nullable=False)
    
    part_type = Column(String, nullable=False)
    text_content = Column(Text)
    
    # Code-specific fields
    language = Column(String)  # programming language for code blocks
    
    # Media fields
    media_type = Column(String)  # MIME type (image/png, audio/mp3, etc.)
    url = Column(String)  # URL for external resources
    
    # Tool use fields (Claude)
    tool_name = Column(String)  # name of tool being used
    tool_use_id = Column(String)  # links tool_result back to tool_use
    tool_input = Column(JSONB)  # tool input parameters
    
    # Timing and status
    started_at = Column(DateTime(timezone=True))
    ended_at = Column(DateTime(timezone=True))
    is_error = Column(Boolean, default=False)
    
    source_json = Column(JSONB, nullable=False)
    
    message = relationship("Message", back_populates="content_parts")
    citations = relationship("Citation", back_populates="content_part", cascade="all, delete-orphan")


class Citation(Base):
    """Citations within content parts."""
    __tablename__ = "citations"
    __table_args__ = {"schema": "raw"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid())
    content_part_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.content_parts.id", ondelete="CASCADE"), nullable=False)
    source_id = Column(String)
    
    url = Column(String)
    title = Column(String)
    snippet = Column(Text)
    published_at = Column(DateTime(timezone=True))
    
    start_index = Column(Integer)
    end_index = Column(Integer)
    citation_type = Column(String)
    
    source_json = Column(JSONB)
    
    content_part = relationship("ContentPart", back_populates="citations")


class Attachment(Base):
    """File attachments on messages."""
    __tablename__ = "attachments"
    __table_args__ = {"schema": "raw"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid())
    message_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.messages.id", ondelete="CASCADE"), nullable=False)
    
    file_name = Column(String)
    file_type = Column(String)
    file_size = Column(Integer)
    extracted_text = Column(Text)
    
    source_json = Column(JSONB)
    
    message = relationship("Message", back_populates="attachments")


# ============================================================
# ChatGPT Extensions
# ============================================================

class ChatGPTMessageMeta(Base):
    """ChatGPT-specific message metadata."""
    __tablename__ = "chatgpt_message_meta"
    __table_args__ = {"schema": "raw"}
    
    message_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.messages.id", ondelete="CASCADE"), primary_key=True)
    model_slug = Column(String)
    status = Column(String)
    end_turn = Column(Boolean)
    gizmo_id = Column(String)
    source_json = Column(JSONB, nullable=False)


class ChatGPTSearchGroup(Base):
    """ChatGPT search result groups."""
    __tablename__ = "chatgpt_search_groups"
    __table_args__ = {"schema": "raw"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid())
    message_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.messages.id", ondelete="CASCADE"), nullable=False)
    group_type = Column(String)
    domain = Column(String)
    source_json = Column(JSONB, nullable=False)
    
    entries = relationship("ChatGPTSearchEntry", back_populates="group", cascade="all, delete-orphan")


class ChatGPTSearchEntry(Base):
    """ChatGPT search result entries."""
    __tablename__ = "chatgpt_search_entries"
    __table_args__ = {"schema": "raw"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid())
    group_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.chatgpt_search_groups.id", ondelete="CASCADE"), nullable=False)
    sequence = Column(Integer, nullable=False)
    url = Column(String)
    title = Column(String)
    snippet = Column(Text)
    published_at = Column(DateTime(timezone=True))
    attribution = Column(String)
    source_json = Column(JSONB, nullable=False)
    
    group = relationship("ChatGPTSearchGroup", back_populates="entries")


class ChatGPTCodeExecution(Base):
    """ChatGPT code execution records."""
    __tablename__ = "chatgpt_code_executions"
    __table_args__ = {"schema": "raw"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid())
    message_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.messages.id", ondelete="CASCADE"), nullable=False)
    run_id = Column(String)
    status = Column(String)
    code = Column(Text)
    started_at = Column(DateTime(timezone=True))
    ended_at = Column(DateTime(timezone=True))
    final_output = Column(Text)
    exception_name = Column(String)
    exception_traceback = Column(Text)
    source_json = Column(JSONB, nullable=False)
    
    outputs = relationship("ChatGPTCodeOutput", back_populates="execution", cascade="all, delete-orphan")


class ChatGPTCodeOutput(Base):
    """ChatGPT code execution outputs."""
    __tablename__ = "chatgpt_code_outputs"
    __table_args__ = {"schema": "raw"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid())
    execution_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.chatgpt_code_executions.id", ondelete="CASCADE"), nullable=False)
    sequence = Column(Integer, nullable=False)
    output_type = Column(String)
    stream_name = Column(String)
    text_content = Column(Text)
    image_url = Column(String)
    source_json = Column(JSONB, nullable=False)
    
    execution = relationship("ChatGPTCodeExecution", back_populates="outputs")


class ChatGPTDalleGeneration(Base):
    """ChatGPT DALL-E image generations."""
    __tablename__ = "chatgpt_dalle_generations"
    __table_args__ = {"schema": "raw"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid())
    content_part_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.content_parts.id", ondelete="CASCADE"), nullable=False)
    gen_id = Column(String)
    prompt = Column(Text)
    seed = Column(BigInteger)
    parent_gen_id = Column(String)
    edit_op = Column(String)
    width = Column(Integer)
    height = Column(Integer)
    source_json = Column(JSONB, nullable=False)


class ChatGPTCanvasDoc(Base):
    """ChatGPT canvas document operations."""
    __tablename__ = "chatgpt_canvas_docs"
    __table_args__ = {"schema": "raw"}
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid())
    message_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.messages.id", ondelete="CASCADE"), nullable=False)
    textdoc_id = Column(String)
    textdoc_type = Column(String)
    version = Column(Integer)
    title = Column(String)
    from_version = Column(Integer)
    content_length = Column(Integer)
    has_user_edit = Column(Boolean)
    source_json = Column(JSONB, nullable=False)


# ============================================================
# Claude Extensions
# ============================================================

class ClaudeMessageMeta(Base):
    """Claude-specific message metadata."""
    __tablename__ = "claude_message_meta"
    __table_args__ = {"schema": "raw"}
    
    message_id = Column(PG_UUID(as_uuid=True), ForeignKey("raw.messages.id", ondelete="CASCADE"), primary_key=True)
    source_json = Column(JSONB, nullable=False)


---
File: pyproject.toml
---
# pyproject.toml

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "llm_archive"
version = "0.1.0"
description = "LLM conversation archive ingestion and analysis"
requires-python = ">=3.11"
dependencies = [
    "sqlalchemy>=2.0",
    "psycopg2-binary>=2.9",
    "loguru>=0.7",
    "fire>=0.7.1",
    "python-dotenv>=1.2.1",
    "omegaconf>=2.3.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "pytest-cov>=4.0",
    "pytest-xdist>=3.0",
    "ruff>=0.4",
]

[project.scripts]
llm-archive = "llm_archive.cli:main"

[tool.setuptools.packages.find]
where = ["."]
include = ["llm_archive"]
exclude = ["data"]

[tool.pytest.ini_options]
testpaths = ["tests"]
# python_files = ["test_*.py"]
# python_functions = ["test_*"]
addopts = "-v --tb=short"
filterwarnings = [
    "ignore::DeprecationWarning",
]
markers = [
    "integration: marks tests as requiring database",
]

[tool.coverage.run]
source = ["llm_archive"]
branch = true

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
]

[tool.ruff]
target-version = "py311"
line-length = 100

[tool.ruff.lint]
select = [
    "E",      # pycodestyle errors
    "W",      # pycodestyle warnings
    "F",      # Pyflakes
    "I",      # isort
    "B",      # flake8-bugbear
    "C4",     # flake8-comprehensions
    "UP",     # pyupgrade
]
ignore = [
    "E501",   # line too long (handled by formatter)
    "B008",   # do not perform function calls in argument defaults
    "B905",   # zip without strict parameter
]

[tool.ruff.lint.isort]
known-first-party = ["llm_archive"]



---
File: requirements.txt
---



---
File: tests/__init__.py
---
# tests/__init__.py
"""LLM Archive test suite."""



---
File: tests/conftest.py
---
# tests/conftest.py
"""Root pytest configuration."""

import pytest


def pytest_configure(config):
    """Configure pytest markers."""
    config.addinivalue_line(
        "markers", "integration: marks tests as requiring database (deselect with '-m \"not integration\"')"
    )


def pytest_collection_modifyitems(config, items):
    """Automatically mark tests in integration folder."""
    for item in items:
        if "integration" in str(item.fspath):
            item.add_marker(pytest.mark.integration)



---
File: tests/integration/__init__.py
---




---
File: tests/integration/conftest.py
---
# tests/integration/conftest.py
"""Pytest configuration for integration tests - CORRECTED FIXTURES."""

import os
from typing import Generator

import pytest
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine
from sqlalchemy.orm import Session, sessionmaker


def get_test_db_url() -> str:
    """Get test database URL from environment."""
    url = os.getenv('TEST_DATABASE_URL', 'postgresql://localhost:5432/llm_archive_test')
    return url


@pytest.fixture(scope="session")
def db_engine() -> Generator[Engine, None, None]:
    """Create database engine for tests."""
    url = get_test_db_url()
    engine = create_engine(url, echo=False)
    yield engine
    engine.dispose()


@pytest.fixture(scope="session")
def setup_schemas(db_engine):
    """Initialize schemas once per test session."""
    from pathlib import Path
    
    # Find schema directory relative to this file
    tests_dir = Path(__file__).parent.parent
    project_dir = tests_dir.parent
    schema_dir = project_dir / "schema"
    
    with db_engine.connect() as conn:
        # Drop and recreate schemas
        conn.execute(text("DROP SCHEMA IF EXISTS derived CASCADE"))
        conn.execute(text("DROP SCHEMA IF EXISTS raw CASCADE"))
        conn.commit()
        
        # Execute schema files in order
        for sql_file in sorted(schema_dir.glob("*.sql")):
            print(f"Executing {sql_file.name}")
            sql = sql_file.read_text()
            
            try:
                conn.execute(text(sql))
                conn.commit()
            except Exception as e:
                if "already exists" in str(e).lower():
                    conn.rollback()
                    print(f"Note: {sql_file.name} - {e}")
                else:
                    print(f"ERROR in {sql_file.name}: {e}")
                    conn.rollback()
                    raise
    
    yield
    
    # Cleanup
    with db_engine.connect() as conn:
        conn.execute(text("DROP SCHEMA IF EXISTS derived CASCADE"))
        conn.execute(text("DROP SCHEMA IF EXISTS raw CASCADE"))
        conn.commit()


@pytest.fixture
def db_session(db_engine, setup_schemas) -> Generator[Session, None, None]:
    """Create a database session with transaction rollback."""
    connection = db_engine.connect()
    transaction = connection.begin()
    SessionFactory = sessionmaker(bind=connection)
    session = SessionFactory()
    
    yield session
    
    session.close()
    transaction.rollback()
    connection.close()


@pytest.fixture
def clean_db_session(db_session) -> Session:
    """Alias for db_session."""
    return db_session


# ============================================================
# ChatGPT Test Fixtures
# ============================================================

@pytest.fixture
def chatgpt_simple_conversation() -> dict:
    """Simple linear ChatGPT conversation."""
    return {
        "conversation_id": "conv-simple-001",
        "title": "Simple Test Conversation",
        "create_time": 1700000000.0,
        "update_time": 1700001000.0,
        "mapping": {
            "root": {
                "id": "root",
                "parent": None,
                "children": ["node-1"],
                "message": None,
            },
            "node-1": {
                "id": "node-1",
                "parent": "root",
                "children": ["node-2"],
                "message": {
                    "id": "msg-1",
                    "author": {"role": "user"},
                    "create_time": 1700000100.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Hello, how are you?"]
                    }
                }
            },
            "node-2": {
                "id": "node-2",
                "parent": "node-1",
                "children": ["node-3"],
                "message": {
                    "id": "msg-2",
                    "author": {"role": "assistant"},
                    "create_time": 1700000200.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["I'm doing well, thank you!"]
                    }
                }
            },
            "node-3": {
                "id": "node-3",
                "parent": "node-2",
                "children": ["node-4"],
                "message": {
                    "id": "msg-3",
                    "author": {"role": "user"},
                    "create_time": 1700000300.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["What's the weather like?"]
                    }
                }
            },
            "node-4": {
                "id": "node-4",
                "parent": "node-3",
                "children": [],
                "message": {
                    "id": "msg-4",
                    "author": {"role": "assistant"},
                    "create_time": 1700000400.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["It's sunny and warm today."]
                    }
                }
            }
        }
    }


@pytest.fixture
def chatgpt_branched_conversation() -> dict:
    """ChatGPT conversation: 1 user message with 2 assistant responses that each have continuation messages."""
    return {
        "conversation_id": "conv-branched-001",
        "title": "Branched Test Conversation",
        "create_time": 1700000000.0,
        "update_time": 1700002000.0,
        "mapping": {
            "root": {
                "id": "root",
                "parent": None,
                "children": ["user1"],
                "message": None,
            },
            # THE ONLY USER MESSAGE - has 2 assistant children (regenerations)
            "user1": {
                "id": "user1",
                "parent": "root",
                "children": ["asst1a", "asst1b"],  # BRANCH POINT
                "message": {
                    "id": "msg-user1",
                    "author": {"role": "user"},
                    "create_time": 1700000100.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Tell me a story"]
                    }
                }
            },
            # First branch
            "asst1a": {
                "id": "asst1a",
                "parent": "user1",
                "children": ["asst2a"],
                "message": {
                    "id": "msg-asst1a",
                    "author": {"role": "assistant"},
                    "create_time": 1700000200.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Once upon a time..."]
                    }
                }
            },
            "asst2a": {
                "id": "asst2a",
                "parent": "asst1a",
                "children": [],
                "message": {
                    "id": "msg-asst2a",
                    "author": {"role": "assistant"},
                    "create_time": 1700000300.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["There was a brave knight..."]
                    }
                }
            },
            # Second branch  
            "asst1b": {
                "id": "asst1b",
                "parent": "user1",
                "children": ["asst2b"],
                "message": {
                    "id": "msg-asst1b",
                    "author": {"role": "assistant"},
                    "create_time": 1700000250.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["In a galaxy far away..."]
                    }
                }
            },
            "asst2b": {
                "id": "asst2b",
                "parent": "asst1b",
                "children": [],
                "message": {
                    "id": "msg-asst2b",
                    "author": {"role": "assistant"},
                    "create_time": 1700000350.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["A spaceship landed..."]
                    }
                }
            }
        }
    }


@pytest.fixture
def chatgpt_conversation_with_code() -> dict:
    """ChatGPT conversation with code content - uses nested parts structure."""
    return {
        "conversation_id": "conv-code-001",
        "title": "Code Example",
        "create_time": 1700000000.0,
        "update_time": 1700001000.0,
        "mapping": {
            "root": {
                "id": "root",
                "parent": None,
                "children": ["node-1"],
                "message": None,
            },
            "node-1": {
                "id": "node-1",
                "parent": "root",
                "children": ["node-2"],
                "message": {
                    "id": "msg-1",
                    "author": {"role": "user"},
                    "create_time": 1700000100.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Write a Python function to calculate fibonacci numbers"]
                    }
                }
            },
            "node-2": {
                "id": "node-2",
                "parent": "node-1",
                "children": [],
                "message": {
                    "id": "msg-2",
                    "author": {"role": "assistant"},
                    "create_time": 1700000200.0,
                    "content": {
                        "content_type": "text",
                        "parts": [
                            "Here's a Python function:",
                            {
                                "content_type": "code",
                                "language": "python",
                                "text": "def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)"
                            },
                            "This is a recursive implementation."
                        ]
                    }
                }
            }
        }
    }


@pytest.fixture
def chatgpt_conversation_with_image() -> dict:
    """ChatGPT conversation with image content - uses nested parts structure."""
    return {
        "conversation_id": "conv-image-001",
        "title": "Image Example",
        "create_time": 1700000000.0,
        "update_time": 1700001000.0,
        "mapping": {
            "root": {
                "id": "root",
                "parent": None,
                "children": ["node-1"],
                "message": None,
            },
            "node-1": {
                "id": "node-1",
                "parent": "root",
                "children": ["node-2"],
                "message": {
                    "id": "msg-1",
                    "author": {"role": "user"},
                    "create_time": 1700000100.0,
                    "content": {
                        "content_type": "multimodal_text",
                        "parts": [
                            "What's in this image?",
                            {
                                "content_type": "image/png",
                                "asset_pointer": "file-service://dalle-gen-abc123"
                            }
                        ]
                    }
                }
            },
            "node-2": {
                "id": "node-2",
                "parent": "node-1",
                "children": [],
                "message": {
                    "id": "msg-2",
                    "author": {"role": "assistant"},
                    "create_time": 1700000200.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["This image shows a cat."]
                    }
                }
            }
        }
    }


@pytest.fixture
def chatgpt_conversations(
    chatgpt_simple_conversation,
    chatgpt_branched_conversation,
) -> list[dict]:
    """List of ChatGPT test conversations."""
    third_conversation = {
        "conversation_id": "conv-third-001",
        "title": "Third Conversation",
        "create_time": 1700003000.0,
        "update_time": 1700003000.0,
        "mapping": {
            "root": {
                "id": "root",
                "parent": None,
                "children": ["node-1"],
                "message": None,
            },
            "node-1": {
                "id": "node-1",
                "parent": "root",
                "children": [],
                "message": {
                    "id": "msg-third-1",
                    "author": {"role": "user"},
                    "create_time": 1700003100.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Hello"]
                    }
                }
            }
        }
    }
    return [
        chatgpt_simple_conversation,
        chatgpt_branched_conversation,
        third_conversation,
    ]


# ============================================================
# Claude Test Fixtures
# ============================================================

@pytest.fixture
def claude_simple_conversation() -> dict:
    """Simple Claude conversation."""
    return {
        "uuid": "claude-conv-001",
        "name": "Claude Test Conversation",
        "created_at": "2024-01-15T10:00:00Z",
        "updated_at": "2024-01-15T10:30:00Z",
        "chat_messages": [
            {
                "uuid": "claude-msg-001",
                "sender": "human",
                "created_at": "2024-01-15T10:00:00Z",
                "content": [
                    {"type": "text", "text": "Hello Claude"}
                ]
            },
            {
                "uuid": "claude-msg-002",
                "sender": "assistant",
                "created_at": "2024-01-15T10:01:00Z",
                "content": [
                    {"type": "text", "text": "Hello! How can I help you today?"}
                ]
            },
            {
                "uuid": "claude-msg-003",
                "sender": "human",
                "created_at": "2024-01-15T10:05:00Z",
                "content": [
                    {"type": "text", "text": "What's 5 + 3?"}
                ]
            },
            {
                "uuid": "claude-msg-004",
                "sender": "assistant",
                "created_at": "2024-01-15T10:06:00Z",
                "content": [
                    {"type": "text", "text": "5 + 3 = 8"}
                ]
            }
        ]
    }


@pytest.fixture
def claude_conversation_with_thinking() -> dict:
    """Claude conversation with thinking blocks."""
    return {
        "uuid": "claude-conv-002",
        "name": "Claude Thinking Test",
        "created_at": "2024-01-15T11:00:00Z",
        "updated_at": "2024-01-15T11:05:00Z",
        "chat_messages": [
            {
                "uuid": "claude-msg-005",
                "sender": "human",
                "created_at": "2024-01-15T11:00:00Z",
                "content": [
                    {"type": "text", "text": "What is 15 * 23?"}
                ]
            },
            {
                "uuid": "claude-msg-006",
                "sender": "assistant",
                "created_at": "2024-01-15T11:01:00Z",
                "content": [
                    {"type": "thinking", "thinking": "Let me calculate: 15 * 23 = 345"},
                    {"type": "text", "text": "15 multiplied by 23 equals 345."}
                ]
            }
        ]
    }


@pytest.fixture
def claude_conversation_with_tool_use() -> dict:
    """Claude conversation with tool use."""
    return {
        "uuid": "claude-conv-003",
        "name": "Claude Tool Use Test",
        "created_at": "2024-01-15T12:00:00Z",
        "updated_at": "2024-01-15T12:10:00Z",
        "chat_messages": [
            {
                "uuid": "claude-msg-007",
                "sender": "human",
                "created_at": "2024-01-15T12:00:00Z",
                "content": [
                    {"type": "text", "text": "Search for recent news about AI."}
                ]
            },
            {
                "uuid": "claude-msg-008",
                "sender": "assistant",
                "created_at": "2024-01-15T12:01:00Z",
                "content": [
                    {
                        "type": "tool_use",
                        "id": "tool-001",
                        "name": "web_search",
                        "input": {"query": "recent AI news 2024"}
                    }
                ]
            },
            {
                "uuid": "claude-msg-009",
                "sender": "human",
                "created_at": "2024-01-15T12:02:00Z",
                "content": [
                    {
                        "type": "tool_result",
                        "tool_use_id": "tool-001",
                        "content": "AI advances in 2024 include..."
                    }
                ]
            },
            {
                "uuid": "claude-msg-010",
                "sender": "assistant",
                "created_at": "2024-01-15T12:03:00Z",
                "content": [
                    {"type": "text", "text": "Based on my search, here are the recent developments in AI..."}
                ]
            }
        ]
    }


@pytest.fixture
def claude_conversations(
    claude_simple_conversation,
    claude_conversation_with_thinking,
    claude_conversation_with_tool_use,
) -> list[dict]:
    """List of Claude test conversations."""
    return [
        claude_simple_conversation,
        claude_conversation_with_thinking,
        claude_conversation_with_tool_use,
    ]


# ============================================================
# Populated Database Fixtures
# ============================================================

@pytest.fixture
def populated_chatgpt_db(clean_db_session, chatgpt_simple_conversation):
    """Database with a single ChatGPT conversation imported."""
    from llm_archive.extractors import ChatGPTExtractor
    
    extractor = ChatGPTExtractor(clean_db_session)
    extractor.extract_dialogue(chatgpt_simple_conversation)
    clean_db_session.commit()
    
    return clean_db_session


@pytest.fixture
def populated_claude_db(clean_db_session, claude_simple_conversation):
    """Database with a single Claude conversation imported."""
    from llm_archive.extractors import ClaudeExtractor
    
    extractor = ClaudeExtractor(clean_db_session)
    extractor.extract_dialogue(claude_simple_conversation)
    clean_db_session.commit()
    
    return clean_db_session


@pytest.fixture
def fully_populated_db(
    clean_db_session,
    chatgpt_simple_conversation,
    chatgpt_branched_conversation,
    claude_simple_conversation,
):
    """Database with multiple conversations and derived data."""
    from llm_archive.extractors import ChatGPTExtractor, ClaudeExtractor
    from llm_archive.builders.prompt_response import PromptResponseBuilder
    
    chatgpt_extractor = ChatGPTExtractor(clean_db_session)
    chatgpt_extractor.extract_dialogue(chatgpt_simple_conversation)
    chatgpt_extractor.extract_dialogue(chatgpt_branched_conversation)
    
    claude_extractor = ClaudeExtractor(clean_db_session)
    claude_extractor.extract_dialogue(claude_simple_conversation)
    
    clean_db_session.commit()
    
    pr_builder = PromptResponseBuilder(clean_db_session)
    pr_builder.build_all()
    
    clean_db_session.commit()
    
    return clean_db_session



---
File: tests/integration/test_annotations.py
---
# tests/integration/test_annotations.py
"""Integration tests for typed annotation system."""

import pytest
from uuid import uuid4

from llm_archive.annotations.core import (
    AnnotationWriter,
    AnnotationReader,
    EntityType,
    ValueType,
    AnnotationResult,
)
from llm_archive.extractors.chatgpt import ChatGPTExtractor
from llm_archive.builders.prompt_response import PromptResponseBuilder
from llm_archive.annotators.prompt_response import (
    WikiCandidateAnnotator,
    NaiveTitleAnnotator,
)
from llm_archive.models import Message, PromptResponse


class TestAnnotationWriterIntegration:
    """Integration tests for AnnotationWriter."""
    
    def test_write_flag_creates_record(self, clean_db_session, chatgpt_simple_conversation):
        """Writing a flag creates a record in flag table."""
        # Get a real message ID
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        writer = AnnotationWriter(clean_db_session)
        result = writer.write_flag(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='test_flag',
            source='test',
        )
        clean_db_session.commit()
        
        assert result is True
        
        # Verify record exists
        reader = AnnotationReader(clean_db_session)
        assert reader.has_flag(EntityType.MESSAGE, message.id, 'test_flag')
    
    def test_write_string_creates_record(self, clean_db_session, chatgpt_simple_conversation):
        """Writing a string creates a record in string table."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        writer = AnnotationWriter(clean_db_session)
        result = writer.write_string(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='category',
            value='greeting',
            source='test',
        )
        clean_db_session.commit()
        
        assert result is True
        
        reader = AnnotationReader(clean_db_session)
        values = reader.get_string(EntityType.MESSAGE, message.id, 'category')
        assert 'greeting' in values
    
    def test_write_numeric_creates_record(self, clean_db_session, chatgpt_simple_conversation):
        """Writing a numeric creates a record in numeric table."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        writer = AnnotationWriter(clean_db_session)
        result = writer.write_numeric(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='word_count',
            value=42,
            source='test',
        )
        clean_db_session.commit()
        
        assert result is True
        
        reader = AnnotationReader(clean_db_session)
        values = reader.get_numeric(EntityType.MESSAGE, message.id, 'word_count')
        assert 42 in values
    
    def test_write_json_creates_record(self, clean_db_session, chatgpt_simple_conversation):
        """Writing JSON creates a record in json table."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        writer = AnnotationWriter(clean_db_session)
        result = writer.write_json(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='metadata',
            value={'tags': ['test', 'example'], 'score': 0.95},
            source='test',
        )
        clean_db_session.commit()
        
        assert result is True
        
        reader = AnnotationReader(clean_db_session)
        value = reader.get_json(EntityType.MESSAGE, message.id, 'metadata')
        assert value == {'tags': ['test', 'example'], 'score': 0.95}
    
    def test_write_duplicate_flag_returns_false(self, clean_db_session, chatgpt_simple_conversation):
        """Writing duplicate flag returns False (no new record)."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        writer = AnnotationWriter(clean_db_session)
        
        # First write succeeds
        result1 = writer.write_flag(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='test_flag',
            source='test',
        )
        clean_db_session.commit()
        assert result1 is True
        
        # Duplicate returns False
        result2 = writer.write_flag(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='test_flag',
            source='test',
        )
        assert result2 is False
    
    def test_write_multi_value_string(self, clean_db_session, chatgpt_simple_conversation):
        """Can write multiple values for same string key."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        writer = AnnotationWriter(clean_db_session)
        writer.write_string(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='tag',
            value='coding',
            source='test',
        )
        writer.write_string(
            entity_type=EntityType.MESSAGE,
            entity_id=message.id,
            key='tag',
            value='python',
            source='test',
        )
        clean_db_session.commit()
        
        reader = AnnotationReader(clean_db_session)
        values = reader.get_string(EntityType.MESSAGE, message.id, 'tag')
        assert set(values) == {'coding', 'python'}
    
    def test_write_from_annotation_result(self, clean_db_session, chatgpt_simple_conversation):
        """Can write from AnnotationResult object."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        result = AnnotationResult(
            key='exchange_type',
            value='wiki_article',
            value_type=ValueType.STRING,
            confidence=0.9,
            reason='wiki_links_detected',
        )
        
        writer = AnnotationWriter(clean_db_session)
        written = writer.write(EntityType.MESSAGE, message.id, result)
        clean_db_session.commit()
        
        assert written is True
        
        reader = AnnotationReader(clean_db_session)
        values = reader.get_string(EntityType.MESSAGE, message.id, 'exchange_type')
        assert 'wiki_article' in values


class TestAnnotationReaderIntegration:
    """Integration tests for AnnotationReader."""
    
    def test_find_entities_with_flag(self, clean_db_session, chatgpt_simple_conversation):
        """Can find all entities with a specific flag."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        messages = clean_db_session.query(Message).all()
        assert len(messages) >= 2
        
        writer = AnnotationWriter(clean_db_session)
        
        # Flag first two messages with 'has_code'
        writer.write_flag(EntityType.MESSAGE, messages[0].id, 'has_code', source='test')
        writer.write_flag(EntityType.MESSAGE, messages[1].id, 'has_code', source='test')
        
        # Flag third message with something else (if exists)
        if len(messages) > 2:
            writer.write_flag(EntityType.MESSAGE, messages[2].id, 'has_attachment', source='test')
        
        clean_db_session.commit()
        
        reader = AnnotationReader(clean_db_session)
        results = reader.find_entities_with_flag(EntityType.MESSAGE, 'has_code')
        
        assert messages[0].id in results
        assert messages[1].id in results
        if len(messages) > 2:
            assert messages[2].id not in results
    
    def test_find_entities_with_string_value(self, clean_db_session, chatgpt_simple_conversation):
        """Can find entities with specific string value."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        messages = clean_db_session.query(Message).all()
        
        writer = AnnotationWriter(clean_db_session)
        writer.write_string(EntityType.MESSAGE, messages[0].id, 'topic', 'coding', source='test')
        writer.write_string(EntityType.MESSAGE, messages[1].id, 'topic', 'general', source='test')
        clean_db_session.commit()
        
        reader = AnnotationReader(clean_db_session)
        
        # Find by specific value
        coding_results = reader.find_entities_with_string(EntityType.MESSAGE, 'topic', 'coding')
        assert messages[0].id in coding_results
        assert messages[1].id not in coding_results
        
        # Find by key only (any value)
        all_results = reader.find_entities_with_string(EntityType.MESSAGE, 'topic', None)
        assert messages[0].id in all_results
        assert messages[1].id in all_results
    
    def test_get_all_keys(self, clean_db_session, chatgpt_simple_conversation):
        """Can get all annotations for an entity."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        
        writer = AnnotationWriter(clean_db_session)
        writer.write_flag(EntityType.MESSAGE, message.id, 'has_code', source='test')
        writer.write_string(EntityType.MESSAGE, message.id, 'language', 'python', source='test')
        writer.write_numeric(EntityType.MESSAGE, message.id, 'line_count', 50, source='test')
        clean_db_session.commit()
        
        reader = AnnotationReader(clean_db_session)
        all_keys = reader.get_all_keys(EntityType.MESSAGE, message.id)
        
        assert 'has_code' in all_keys
        assert 'language' in all_keys
        assert 'line_count' in all_keys


class TestPromptResponseAnnotatorIntegration:
    """Integration tests for prompt-response annotators."""
    
    @pytest.fixture
    def wiki_conversation(self):
        """Conversation with wiki-style content."""
        return {
            'conversation_id': 'conv-wiki',
            'title': 'Wiki Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-user': {
                    'id': 'node-user',
                    'message': {
                        'id': 'msg-user',
                        'author': {'role': 'user'},
                        'content': {'parts': ['Write about cats']},
                        'create_time': 1700000000,
                    },
                    'parent': None,
                },
                'node-asst': {
                    'id': 'node-asst',
                    'message': {
                        'id': 'msg-asst',
                        'author': {'role': 'assistant'},
                        'content': {'parts': [
                            '# The Domestic Cat\n\n'
                            '[[Cats]] are [[mammals]] that belong to the family [[Felidae]]. '
                            'They are known for their [[hunting]] abilities.'
                        ]},
                        'create_time': 1700000001,
                    },
                    'parent': 'node-user',
                },
            },
        }
    
    def test_wiki_candidate_annotator_end_to_end(self, clean_db_session, wiki_conversation):
        """Test WikiCandidateAnnotator with real database."""
        # Import and build
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(wiki_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        builder.build_all()
        clean_db_session.commit()
        
        # Run annotator
        annotator = WikiCandidateAnnotator(clean_db_session)
        count = annotator.compute()
        clean_db_session.commit()
        
        assert count > 0
        
        # Verify annotations exist
        reader = AnnotationReader(clean_db_session)
        pr = clean_db_session.query(PromptResponse).first()
        
        values = reader.get_string(EntityType.PROMPT_RESPONSE, pr.id, 'exchange_type')
        assert 'wiki_article' in values
        
        counts = reader.get_numeric(EntityType.PROMPT_RESPONSE, pr.id, 'wiki_link_count')
        assert len(counts) > 0
        assert counts[0] >= 4  # At least 4 wiki links in our test data
    
    def test_naive_title_annotator_end_to_end(self, clean_db_session, wiki_conversation):
        """Test NaiveTitleAnnotator with real database."""
        # Import and build
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(wiki_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        builder.build_all()
        clean_db_session.commit()
        
        # Run wiki candidate annotator first (prerequisite)
        wiki_annotator = WikiCandidateAnnotator(clean_db_session)
        wiki_annotator.compute()
        clean_db_session.commit()
        
        # Run title annotator
        title_annotator = NaiveTitleAnnotator(clean_db_session)
        count = title_annotator.compute()
        clean_db_session.commit()
        
        assert count > 0
        
        # Verify title was extracted
        reader = AnnotationReader(clean_db_session)
        pr = clean_db_session.query(PromptResponse).first()
        
        values = reader.get_string(EntityType.PROMPT_RESPONSE, pr.id, 'proposed_title')
        assert 'The Domestic Cat' in values
    
    def test_annotator_prerequisite_filtering(self, clean_db_session, chatgpt_simple_conversation):
        """Test that NaiveTitleAnnotator respects REQUIRES_STRINGS."""
        # Import conversation that won't be marked as wiki
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        builder.build_all()
        clean_db_session.commit()
        
        # Skip wiki annotator - no wiki_article annotations will exist
        
        # Run title annotator
        title_annotator = NaiveTitleAnnotator(clean_db_session)
        count = title_annotator.compute()
        clean_db_session.commit()
        
        # Should process nothing because prerequisite not met
        assert count == 0


class TestGizmoAnnotationIntegration:
    """Integration tests for gizmo annotation writing during extraction."""
    
    def test_gizmo_annotation_written_during_extraction(self, clean_db_session):
        """Test that gizmo_id is written as annotation during extraction."""
        conversation = {
            'conversation_id': 'conv-gizmo',
            'title': 'Gizmo Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': {
                        'id': 'msg-1',
                        'author': {'role': 'assistant'},
                        'content': {'parts': ['Response from custom GPT']},
                        'metadata': {
                            'gizmo_id': 'g-wiki-generator',
                            'model_slug': 'gpt-4',
                        },
                        'create_time': 1700000000,
                    },
                    'parent': None,
                },
            },
        }
        
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(conversation)
        clean_db_session.commit()
        
        message = clean_db_session.query(Message).first()
        reader = AnnotationReader(clean_db_session)
        
        # Check gizmo_id annotation
        gizmo_values = reader.get_string(EntityType.MESSAGE, message.id, 'gizmo_id')
        assert 'g-wiki-generator' in gizmo_values
        
        # Check has_gizmo flag
        assert reader.has_flag(EntityType.MESSAGE, message.id, 'has_gizmo')
        
        # Check model_slug annotation
        model_values = reader.get_string(EntityType.MESSAGE, message.id, 'model_slug')
        assert 'gpt-4' in model_values



---
File: tests/integration/test_extraction_diagnostics.py
---
# tests/integration/test_extraction_diagnostics.py
"""Diagnostic tests to understand what's happening during extraction."""

import pytest
from llm_archive.extractors import ChatGPTExtractor
from llm_archive.models import Message


def test_branched_conversation_diagnostic(db_session, chatgpt_branched_conversation):
    """Diagnostic test to see what's actually extracted."""
    extractor = ChatGPTExtractor(db_session)
    result = extractor.extract_dialogue(chatgpt_branched_conversation)
    
    print(f"\n=== EXTRACTION RESULT: {result} ===")
    
    # Get all messages
    messages = db_session.query(Message).order_by(Message.created_at).all()
    
    print(f"\n=== TOTAL MESSAGES: {len(messages)} ===")
    for i, msg in enumerate(messages):
        print(f"{i+1}. source_id={msg.source_id}, role={msg.role}, parent_id={msg.parent_id}")
    
    # Check the mapping structure
    mapping = chatgpt_branched_conversation['mapping']
    print(f"\n=== MAPPING STRUCTURE ===")
    for node_id, node in mapping.items():
        msg_data = node.get('message')
        if msg_data:
            msg_id = msg_data.get('id')
            role = msg_data.get('author', {}).get('role')
            parent_node = node.get('parent')
            children = node.get('children', [])
            print(f"Node: {node_id}, msg_id: {msg_id}, role: {role}, parent: {parent_node}, children: {children}")
    
    # Find the user message
    user_msg = db_session.query(Message).filter(Message.role == 'user').first()
    print(f"\n=== USER MESSAGE ===")
    print(f"source_id: {user_msg.source_id}")
    print(f"parent_id: {user_msg.parent_id}")
    
    # Find children
    children = db_session.query(Message).filter(Message.parent_id == user_msg.id).all()
    print(f"\n=== CHILDREN OF USER MESSAGE ===")
    print(f"Count: {len(children)}")
    for child in children:
        print(f"  - source_id={child.source_id}, role={child.role}")
    
    # Check if parent_id is set at all
    messages_with_parent = db_session.query(Message).filter(Message.parent_id.isnot(None)).all()
    print(f"\n=== MESSAGES WITH PARENT_ID SET ===")
    print(f"Count: {len(messages_with_parent)}")
    for msg in messages_with_parent:
        parent = db_session.get(Message, msg.parent_id)
        parent_source = parent.source_id if parent else "NOT FOUND"
        print(f"  - {msg.source_id} -> parent: {parent_source}")
    
    # The actual assertion
    assert len(children) == 2, f"Expected 2 children, found {len(children)}"


def test_simple_parent_child_relationship(db_session):
    """Test if parent-child relationships work at all."""
    simple_conv = {
        "conversation_id": "test-parent-child",
        "title": "Parent Child Test",
        "create_time": 1700000000.0,
        "update_time": 1700000000.0,
        "mapping": {
            "root": {
                "id": "root",
                "parent": None,
                "children": ["node1"],
                "message": None
            },
            "node1": {
                "id": "node1", 
                "parent": "root",
                "children": ["node2"],
                "message": {
                    "id": "msg1",
                    "author": {"role": "user"},
                    "create_time": 1700000100.0,
                    "content": {"content_type": "text", "parts": ["Hello"]}
                }
            },
            "node2": {
                "id": "node2",
                "parent": "node1",
                "children": [],
                "message": {
                    "id": "msg2",
                    "author": {"role": "assistant"},
                    "create_time": 1700000200.0,
                    "content": {"content_type": "text", "parts": ["Hi"]}
                }
            }
        }
    }
    
    extractor = ChatGPTExtractor(db_session)
    extractor.extract_dialogue(simple_conv)
    
    msg1 = db_session.query(Message).filter(Message.source_id == "msg1").first()
    msg2 = db_session.query(Message).filter(Message.source_id == "msg2").first()
    
    print(f"\n=== SIMPLE TEST ===")
    print(f"msg1 (user): source_id={msg1.source_id}, id={msg1.id}, parent_id={msg1.parent_id}")
    print(f"msg2 (asst): source_id={msg2.source_id}, id={msg2.id}, parent_id={msg2.parent_id}")
    
    # Check if msg2's parent_id points to msg1's id
    if msg2.parent_id:
        print(f"msg2.parent_id == msg1.id? {msg2.parent_id == msg1.id}")
    else:
        print("msg2.parent_id is None!")
    
    assert msg2.parent_id == msg1.id, "Child should have parent_id pointing to parent"


def test_branched_extraction_step_by_step(db_session):
    """Test extraction with a known branched structure."""
    conv = {
        "conversation_id": "test-branch",
        "title": "Branch Test",
        "create_time": 1700000000.0,
        "update_time": 1700000000.0,
        "mapping": {
            "root": {
                "id": "root",
                "parent": None,
                "children": ["user1"],
                "message": None
            },
            "user1": {
                "id": "user1",
                "parent": "root", 
                "children": ["asst1", "asst2"],  # TWO CHILDREN
                "message": {
                    "id": "msg-user1",
                    "author": {"role": "user"},
                    "create_time": 1700000100.0,
                    "content": {"content_type": "text", "parts": ["Question"]}
                }
            },
            "asst1": {
                "id": "asst1",
                "parent": "user1",
                "children": [],
                "message": {
                    "id": "msg-asst1",
                    "author": {"role": "assistant"},
                    "create_time": 1700000200.0,
                    "content": {"content_type": "text", "parts": ["Answer 1"]}
                }
            },
            "asst2": {
                "id": "asst2",
                "parent": "user1",
                "children": [],
                "message": {
                    "id": "msg-asst2",
                    "author": {"role": "assistant"},
                    "create_time": 1700000250.0,
                    "content": {"content_type": "text", "parts": ["Answer 2"]}
                }
            }
        }
    }
    
    extractor = ChatGPTExtractor(db_session)
    extractor.extract_dialogue(conv)
    
    user_msg = db_session.query(Message).filter(Message.source_id == "msg-user1").first()
    asst1 = db_session.query(Message).filter(Message.source_id == "msg-asst1").first()
    asst2 = db_session.query(Message).filter(Message.source_id == "msg-asst2").first()
    
    print(f"\n=== BRANCHED TEST ===")
    print(f"user: id={user_msg.id}, source_id={user_msg.source_id}")
    print(f"asst1: id={asst1.id}, parent_id={asst1.parent_id}")
    print(f"asst2: id={asst2.id}, parent_id={asst2.parent_id}")
    
    # Both assistants should have user as parent
    assert asst1.parent_id == user_msg.id, "asst1 parent should be user"
    assert asst2.parent_id == user_msg.id, "asst2 parent should be user"
    
    # User should have 2 children
    children = db_session.query(Message).filter(Message.parent_id == user_msg.id).all()
    print(f"Children count: {len(children)}")
    
    assert len(children) == 2, f"User should have 2 children, got {len(children)}"



---
File: tests/integration/test_extractors.py
---
# tests/integration/test_extractors.py
"""Tests for conversation extractors."""

import pytest
from uuid import UUID

from llm_archive.extractors import ChatGPTExtractor, ClaudeExtractor
from llm_archive.models import Dialogue, Message, ContentPart


class TestChatGPTExtractor:
    """Tests for ChatGPT extractor."""
    
    def test_extract_simple_conversation(self, db_session, chatgpt_simple_conversation):
        """Test extracting a simple linear conversation."""
        extractor = ChatGPTExtractor(db_session)
        result = extractor.extract_dialogue(chatgpt_simple_conversation)
        
        assert result == 'new'
        
        # Check dialogue was created
        dialogue = db_session.query(Dialogue).filter(
            Dialogue.source_id == "conv-simple-001"
        ).first()
        
        assert dialogue is not None
        assert dialogue.source == 'chatgpt'
        assert dialogue.title == "Simple Test Conversation"
    
    def test_extract_messages(self, db_session, chatgpt_simple_conversation):
        """Test that messages are extracted correctly."""
        extractor = ChatGPTExtractor(db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        
        dialogue = db_session.query(Dialogue).filter(
            Dialogue.source_id == "conv-simple-001"
        ).first()
        
        messages = db_session.query(Message).filter(
            Message.dialogue_id == dialogue.id
        ).all()
        
        # Should have 4 messages (excluding root node which has no message)
        assert len(messages) == 4
        
        # Check roles
        roles = sorted([m.role for m in messages])
        assert roles == ['assistant', 'assistant', 'user', 'user']
    
    def test_extract_content_parts(self, db_session, chatgpt_simple_conversation):
        """Test that content parts are extracted."""
        extractor = ChatGPTExtractor(db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        
        # Get a user message
        message = db_session.query(Message).filter(
            Message.role == 'user'
        ).first()
        
        parts = db_session.query(ContentPart).filter(
            ContentPart.message_id == message.id
        ).all()
        
        assert len(parts) >= 1
        assert parts[0].part_type == 'text'
        assert parts[0].text_content is not None
    
    def test_extract_branched_conversation(self, db_session, chatgpt_branched_conversation):
        """Test extracting a conversation with branches."""
        extractor = ChatGPTExtractor(db_session)
        result = extractor.extract_dialogue(chatgpt_branched_conversation)
        
        assert result == 'new'
        
        dialogue = db_session.query(Dialogue).filter(
            Dialogue.source_id == "conv-branched-001"
        ).first()
        
        messages = db_session.query(Message).filter(
            Message.dialogue_id == dialogue.id
        ).all()
        
        # Should have 5 messages (including both branches)
        assert len(messages) == 5
        
        # Check for branch point (message with multiple children)
        user_msg = db_session.query(Message).filter(
            Message.dialogue_id == dialogue.id,
            Message.role == 'user'
        ).first()
        
        # Count children
        children = db_session.query(Message).filter(
            Message.parent_id == user_msg.id
        ).all()
        
        # First user message should have 2 children (regeneration)
        assert len(children) == 2
    
    def test_extract_code_content(self, db_session, chatgpt_conversation_with_code):
        """Test extracting code execution content with language."""
        extractor = ChatGPTExtractor(db_session)
        extractor.extract_dialogue(chatgpt_conversation_with_code)
        
        dialogue = db_session.query(Dialogue).filter(
            Dialogue.source_id == "conv-code-001"
        ).first()
        
        assert dialogue is not None
        
        # Check for code content part with language
        code_parts = db_session.query(ContentPart).filter(
            ContentPart.part_type == 'code'
        ).all()
        
        assert len(code_parts) >= 1
        
        code_part = code_parts[0]
        assert code_part.language == 'python'
        assert 'fibonacci' in code_part.text_content
    
    def test_extract_image_content(self, db_session, chatgpt_conversation_with_image):
        """Test extracting image content with media type and URL."""
        extractor = ChatGPTExtractor(db_session)
        extractor.extract_dialogue(chatgpt_conversation_with_image)
        
        dialogue = db_session.query(Dialogue).filter(
            Dialogue.source_id == "conv-image-001"
        ).first()
        
        assert dialogue is not None
        
        # Check for image content part
        image_parts = db_session.query(ContentPart).filter(
            ContentPart.part_type == 'image'
        ).all()
        
        assert len(image_parts) >= 1
        
        image_part = image_parts[0]
        assert image_part.media_type == 'image/png'
        assert image_part.url is not None
        assert 'dalle-gen-abc123' in image_part.url or 'example.com' in image_part.url
    
    def test_missing_conversation_id(self, db_session):
        """Test handling of conversation without ID."""
        extractor = ChatGPTExtractor(db_session)
        result = extractor.extract_dialogue({"title": "No ID"})
        
        assert result is None
    
    def test_extract_all(self, db_session, chatgpt_conversations):
        """Test extracting multiple conversations."""
        extractor = ChatGPTExtractor(db_session)
        counts = extractor.extract_all(chatgpt_conversations)
        
        assert counts['dialogues_new'] == 3
        assert counts['failed'] == 0


class TestClaudeExtractor:
    """Tests for Claude extractor."""
    
    def test_extract_simple_conversation(self, db_session, claude_simple_conversation):
        """Test extracting a simple Claude conversation."""
        extractor = ClaudeExtractor(db_session)
        result = extractor.extract_dialogue(claude_simple_conversation)
        
        assert result == 'new'
        
        dialogue = db_session.query(Dialogue).filter(
            Dialogue.source_id == "claude-conv-001"
        ).first()
        
        assert dialogue is not None
        assert dialogue.source == 'claude'
        assert dialogue.title == "Claude Test Conversation"
    
    def test_extract_messages_linear(self, db_session, claude_simple_conversation):
        """Test that Claude messages form a linear chain."""
        extractor = ClaudeExtractor(db_session)
        extractor.extract_dialogue(claude_simple_conversation)
        
        dialogue = db_session.query(Dialogue).filter(
            Dialogue.source_id == "claude-conv-001"
        ).first()
        
        messages = db_session.query(Message).filter(
            Message.dialogue_id == dialogue.id
        ).order_by(Message.created_at).all()
        
        assert len(messages) == 4
        
        # Check linear structure
        for i in range(1, len(messages)):
            assert messages[i].parent_id == messages[i-1].id
    
    def test_role_normalization(self, db_session, claude_simple_conversation):
        """Test that 'human' role is normalized to 'user'."""
        extractor = ClaudeExtractor(db_session)
        extractor.extract_dialogue(claude_simple_conversation)
        
        messages = db_session.query(Message).all()
        roles = set(m.role for m in messages)
        
        assert 'human' not in roles
        assert 'user' in roles
        assert 'assistant' in roles
    
    def test_extract_thinking_blocks(self, db_session, claude_conversation_with_thinking):
        """Test extracting thinking content."""
        extractor = ClaudeExtractor(db_session)
        extractor.extract_dialogue(claude_conversation_with_thinking)
        
        # Check for thinking content part
        thinking_parts = db_session.query(ContentPart).filter(
            ContentPart.part_type == 'thinking'
        ).all()
        
        assert len(thinking_parts) >= 1
        assert thinking_parts[0].text_content is not None
    
    def test_extract_tool_use(self, db_session, claude_conversation_with_tool_use):
        """Test extracting tool use content with all fields."""
        extractor = ClaudeExtractor(db_session)
        extractor.extract_dialogue(claude_conversation_with_tool_use)
        
        tool_use_parts = db_session.query(ContentPart).filter(
            ContentPart.part_type == 'tool_use'
        ).all()
        
        assert len(tool_use_parts) >= 1
        
        # Verify tool use fields are extracted
        tool_use = tool_use_parts[0]
        assert tool_use.tool_name == 'web_search'
        assert tool_use.tool_use_id == 'tool-001'
        assert tool_use.tool_input == {'query': 'recent AI news 2024'}
        assert tool_use.text_content == 'recent AI news 2024'  # Extracted from input.query
    
    def test_extract_tool_result(self, db_session, claude_conversation_with_tool_use):
        """Test extracting tool result content with linked tool_use_id."""
        extractor = ClaudeExtractor(db_session)
        extractor.extract_dialogue(claude_conversation_with_tool_use)
        
        tool_result_parts = db_session.query(ContentPart).filter(
            ContentPart.part_type == 'tool_result'
        ).all()
        
        assert len(tool_result_parts) >= 1
        
        # Verify tool result fields
        tool_result = tool_result_parts[0]
        assert tool_result.tool_use_id == 'tool-001'  # Links back to tool_use
        assert tool_result.text_content == 'AI advances in 2024 include...'
    
    def test_missing_uuid(self, db_session):
        """Test handling of conversation without UUID."""
        extractor = ClaudeExtractor(db_session)
        result = extractor.extract_dialogue({"name": "No UUID"})
        
        assert result is None
    
    def test_extract_all(self, db_session, claude_conversations):
        """Test extracting multiple Claude conversations."""
        extractor = ClaudeExtractor(db_session)
        counts = extractor.extract_all(claude_conversations)
        
        assert counts['dialogues_new'] == 3
        assert counts['failed'] == 0


class TestExtractorTimestamps:
    """Tests for timestamp parsing."""
    
    def test_chatgpt_epoch_timestamps(self, db_session, chatgpt_simple_conversation):
        """Test parsing ChatGPT epoch timestamps."""
        extractor = ChatGPTExtractor(db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        
        dialogue = db_session.query(Dialogue).first()
        
        assert dialogue.created_at is not None
        assert dialogue.updated_at is not None
        assert dialogue.created_at.tzinfo is not None  # Timezone aware
    
    def test_claude_iso_timestamps(self, db_session, claude_simple_conversation):
        """Test parsing Claude ISO timestamps."""
        extractor = ClaudeExtractor(db_session)
        extractor.extract_dialogue(claude_simple_conversation)
        
        dialogue = db_session.query(Dialogue).first()
        
        assert dialogue.created_at is not None
        assert dialogue.updated_at is not None
        assert dialogue.created_at.tzinfo is not None  # Timezone aware



---
File: tests/integration/test_idempotency.py
---
# tests/integration/test_idempotency.py
"""Tests for idempotent import behavior with incremental updates."""

import copy
import pytest
from datetime import datetime, timezone

from llm_archive.extractors import ChatGPTExtractor, ClaudeExtractor
from llm_archive.models import Dialogue, Message


class TestChatGPTIdempotency:
    """Tests for ChatGPT idempotent import."""
    
    def test_reimport_unchanged_skips(self, clean_db_session, chatgpt_simple_conversation):
        """Test that reimporting unchanged conversation is skipped."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        result1 = extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        assert result1 == 'new'
        
        # Second import - same data
        result2 = extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        assert result2 == 'skipped'
        
        # Should still have only one dialogue
        count = clean_db_session.query(Dialogue).count()
        assert count == 1
    
    def test_reimport_updated_updates(self, clean_db_session, chatgpt_simple_conversation):
        """Test that reimporting updated conversation updates it."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        result1 = extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        assert result1 == 'new'
        
        original_title = clean_db_session.query(Dialogue).first().title
        
        # Modify and reimport
        updated = copy.deepcopy(chatgpt_simple_conversation)
        updated['update_time'] = 1700002000.0  # Later timestamp
        updated['title'] = "Updated Title"
        
        result2 = extractor.extract_dialogue(updated)
        clean_db_session.commit()
        assert result2 == 'updated'
        
        # Should still have only one dialogue
        count = clean_db_session.query(Dialogue).count()
        assert count == 1
        
        # Title should be updated
        dialogue = clean_db_session.query(Dialogue).first()
        assert dialogue.title == "Updated Title"
    
    def test_reimport_messages_refreshed(self, clean_db_session, chatgpt_simple_conversation):
        """Test that messages are refreshed on update."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_msg_count = clean_db_session.query(Message).count()
        
        # Modify and reimport
        updated = copy.deepcopy(chatgpt_simple_conversation)
        updated['update_time'] = 1700002000.0
        
        extractor.extract_dialogue(updated)
        clean_db_session.commit()
        
        # Message count should be the same (refreshed, not duplicated)
        new_msg_count = clean_db_session.query(Message).count()
        assert new_msg_count == original_msg_count
    
    def test_extract_all_mixed_results(self, clean_db_session, chatgpt_simple_conversation):
        """Test extract_all with mix of new, updated, and skipped."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import one conversation
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Create variations
        unchanged = copy.deepcopy(chatgpt_simple_conversation)
        
        updated = copy.deepcopy(chatgpt_simple_conversation)
        updated['update_time'] = 1700005000.0
        updated['title'] = "Updated"
        
        new_conv = copy.deepcopy(chatgpt_simple_conversation)
        new_conv['conversation_id'] = "conv-new-001"
        
        # Extract all
        counts = extractor.extract_all([unchanged, updated, new_conv])
        
        # Note: after the first was already imported, we have:
        # - unchanged: skipped (no update since update_time same)
        # - updated: but we already updated it above with unchanged, 
        #   so this depends on whether unchanged came first
        # Let's just check we have correct totals
        assert counts['dialogues_skipped'] + counts['dialogues_new'] + counts['dialogues_updated'] == 3


class TestClaudeIdempotency:
    """Tests for Claude idempotent import."""
    
    def test_reimport_unchanged_skips(self, clean_db_session, claude_simple_conversation):
        """Test that reimporting unchanged conversation is skipped."""
        extractor = ClaudeExtractor(clean_db_session)
        
        # First import
        result1 = extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        assert result1 == 'new'
        
        # Second import - same data
        result2 = extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        assert result2 == 'skipped'
    
    def test_reimport_updated_updates(self, clean_db_session, claude_simple_conversation):
        """Test that reimporting updated conversation updates it."""
        extractor = ClaudeExtractor(clean_db_session)
        
        # First import
        result1 = extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        
        # Modify and reimport
        updated = copy.deepcopy(claude_simple_conversation)
        updated['updated_at'] = "2024-01-16T10:00:00Z"  # Later timestamp
        updated['name'] = "Updated Title"
        
        result2 = extractor.extract_dialogue(updated)
        clean_db_session.commit()
        assert result2 == 'updated'
        
        # Title should be updated
        dialogue = clean_db_session.query(Dialogue).first()
        assert dialogue.title == "Updated Title"


class TestCrossSourceIdempotency:
    """Tests for idempotency across sources."""
    
    def test_same_content_different_sources(self, clean_db_session):
        """Test that same content from different sources creates separate records."""
        chatgpt_conv = {
            "conversation_id": "cross-001",
            "title": "Cross Source Test",
            "create_time": 1700000000.0,
            "update_time": 1700001000.0,
            "mapping": {
                "root": {"id": "root", "parent": None, "children": ["m1"], "message": None},
                "m1": {
                    "id": "m1",
                    "parent": "root",
                    "children": [],
                    "message": {
                        "id": "m1",
                        "author": {"role": "user"},
                        "create_time": 1700000100.0,
                        "content": {"content_type": "text", "parts": ["Hello"]}
                    }
                }
            }
        }
        
        claude_conv = {
            "uuid": "cross-001",  # Same ID!
            "name": "Cross Source Test",
            "created_at": "2024-01-15T10:00:00Z",
            "updated_at": "2024-01-15T10:30:00Z",
            "chat_messages": [
                {
                    "uuid": "msg-001",
                    "sender": "human",
                    "created_at": "2024-01-15T10:00:00Z",
                    "content": [{"type": "text", "text": "Hello"}]
                }
            ]
        }
        
        # Import both
        ChatGPTExtractor(clean_db_session).extract_dialogue(chatgpt_conv)
        ClaudeExtractor(clean_db_session).extract_dialogue(claude_conv)
        clean_db_session.commit()
        
        # Should have two dialogues (different sources)
        dialogues = clean_db_session.query(Dialogue).all()
        assert len(dialogues) == 2
        
        sources = set(d.source for d in dialogues)
        assert sources == {'chatgpt', 'claude'}


class TestPartialUpdate:
    """Tests for partial update scenarios."""
    
    def test_conversation_extended(self, clean_db_session, chatgpt_simple_conversation):
        """Test handling of conversation that has been extended."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # Import original
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_msg_count = clean_db_session.query(Message).count()
        
        # Extend conversation (add new messages)
        extended = copy.deepcopy(chatgpt_simple_conversation)
        extended['update_time'] = 1700005000.0
        
        # Add new message to mapping
        new_msg_id = "new-msg-001"
        last_msg_id = list(extended['mapping'].keys())[-1]
        
        extended['mapping'][new_msg_id] = {
            "id": new_msg_id,
            "parent": last_msg_id,
            "children": [],
            "message": {
                "id": new_msg_id,
                "author": {"role": "user"},
                "create_time": 1700004000.0,
                "content": {"content_type": "text", "parts": ["One more question"]}
            }
        }
        # Update parent's children
        for node_id, node in extended['mapping'].items():
            if node.get('message') and node['children'] == [] and node_id != new_msg_id:
                if node_id == last_msg_id:
                    node['children'] = [new_msg_id]
        
        # Reimport
        result = extractor.extract_dialogue(extended)
        clean_db_session.commit()
        
        assert result == 'updated'
        
        # Should have more messages now
        new_msg_count = clean_db_session.query(Message).count()
        assert new_msg_count == original_msg_count + 1


class TestUUIDPreservation:
    """Tests for message UUID preservation during updates."""
    
    def test_unchanged_messages_keep_uuids(self, clean_db_session, chatgpt_simple_conversation):
        """Test that unchanged messages keep their UUIDs."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Record original UUIDs
        original_messages = {m.source_id: m.id for m in clean_db_session.query(Message).all()}
        
        # Update with later timestamp but same content
        updated = copy.deepcopy(chatgpt_simple_conversation)
        updated['update_time'] = 1700005000.0
        updated['title'] = "New Title"  # Only title changed, not messages
        
        extractor.extract_dialogue(updated)
        clean_db_session.commit()
        
        # Check UUIDs are preserved
        new_messages = {m.source_id: m.id for m in clean_db_session.query(Message).all()}
        
        for source_id, original_uuid in original_messages.items():
            assert source_id in new_messages, f"Message {source_id} should still exist"
            assert new_messages[source_id] == original_uuid, f"Message {source_id} UUID changed"
    
    def test_changed_message_keeps_uuid_updates_content(self, clean_db_session, chatgpt_simple_conversation):
        """Test that changed messages keep their UUID but update content."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Find a message to modify
        first_user_msg = clean_db_session.query(Message).filter(
            Message.role == 'user'
        ).first()
        original_uuid = first_user_msg.id
        original_source_id = first_user_msg.source_id
        
        # Modify the message content
        updated = copy.deepcopy(chatgpt_simple_conversation)
        updated['update_time'] = 1700005000.0
        
        for node_id, node in updated['mapping'].items():
            msg = node.get('message')
            if msg and msg.get('id') == original_source_id:
                msg['content']['parts'] = ['MODIFIED MESSAGE CONTENT']
                break
        
        extractor.extract_dialogue(updated)
        clean_db_session.commit()
        
        # UUID should be preserved
        modified_msg = clean_db_session.query(Message).filter(
            Message.source_id == original_source_id
        ).first()
        
        assert modified_msg is not None
        assert modified_msg.id == original_uuid, "UUID should be preserved"
        assert 'MODIFIED' in str(modified_msg.source_json), "Content should be updated"
    
    def test_claude_unchanged_messages_keep_uuids(self, clean_db_session, claude_simple_conversation):
        """Test UUID preservation for Claude extractor."""
        extractor = ClaudeExtractor(clean_db_session)
        
        # First import
        extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        
        # Record original UUIDs
        original_messages = {m.source_id: m.id for m in clean_db_session.query(Message).all()}
        
        # Update with later timestamp but same messages
        updated = copy.deepcopy(claude_simple_conversation)
        updated['updated_at'] = "2024-01-20T10:00:00Z"
        updated['name'] = "New Title"
        
        extractor.extract_dialogue(updated)
        clean_db_session.commit()
        
        # Check UUIDs are preserved
        new_messages = {m.source_id: m.id for m in clean_db_session.query(Message).all()}
        
        for source_id, original_uuid in original_messages.items():
            assert source_id in new_messages
            assert new_messages[source_id] == original_uuid


class TestSoftDelete:
    """Tests for soft-delete behavior when messages are removed from source."""
    
    def test_removed_message_soft_deleted(self, clean_db_session, chatgpt_simple_conversation):
        """Test that messages removed from source are soft-deleted."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_count = clean_db_session.query(Message).count()
        
        # Remove a message from the conversation
        truncated = copy.deepcopy(chatgpt_simple_conversation)
        truncated['update_time'] = 1700005000.0
        
        # Remove the last message
        mapping_keys = list(truncated['mapping'].keys())
        last_msg_key = mapping_keys[-1]
        del truncated['mapping'][last_msg_key]
        
        # Update parent to not have children pointing to deleted msg
        for node_id, node in truncated['mapping'].items():
            if last_msg_key in node.get('children', []):
                node['children'].remove(last_msg_key)
        
        extractor.extract_dialogue(truncated)
        clean_db_session.commit()
        
        # Total message count should be the same (soft-deleted, not hard-deleted)
        total_count = clean_db_session.query(Message).count()
        assert total_count == original_count
        
        # Active message count should be one less
        active_count = clean_db_session.query(Message).filter(
            Message.deleted_at.is_(None)
        ).count()
        assert active_count == original_count - 1
        
        # Should have one soft-deleted message
        deleted_count = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).count()
        assert deleted_count == 1
    
    def test_soft_deleted_message_restored_on_reappear(self, clean_db_session, chatgpt_simple_conversation):
        """Test that soft-deleted message is restored if it reappears."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Remove a message
        truncated = copy.deepcopy(chatgpt_simple_conversation)
        truncated['update_time'] = 1700005000.0
        
        mapping_keys = list(truncated['mapping'].keys())
        removed_msg_key = mapping_keys[-1]
        del truncated['mapping'][removed_msg_key]
        
        for node_id, node in truncated['mapping'].items():
            if removed_msg_key in node.get('children', []):
                node['children'].remove(removed_msg_key)
        
        extractor.extract_dialogue(truncated)
        clean_db_session.commit()
        
        # Verify it's soft-deleted
        deleted_msg = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).first()
        assert deleted_msg is not None
        deleted_uuid = deleted_msg.id
        
        # Now restore by importing original again with newer timestamp
        restored = copy.deepcopy(chatgpt_simple_conversation)
        restored['update_time'] = 1700010000.0
        
        extractor.extract_dialogue(restored)
        clean_db_session.commit()
        
        # Message should be restored
        restored_msg = clean_db_session.query(Message).filter(
            Message.id == deleted_uuid
        ).first()
        
        assert restored_msg is not None
        assert restored_msg.deleted_at is None, "Message should be restored (deleted_at = None)"
    
    def test_content_hash_detects_changes(self, clean_db_session, chatgpt_simple_conversation):
        """Test that content hash correctly detects changed messages."""
        extractor = ChatGPTExtractor(clean_db_session)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Get a message's content hash
        msg = clean_db_session.query(Message).filter(Message.role == 'user').first()
        original_hash = msg.content_hash
        
        assert original_hash is not None, "Content hash should be computed"
        
        # Modify the message content
        modified = copy.deepcopy(chatgpt_simple_conversation)
        modified['update_time'] = 1800000000.0  # Much later timestamp to ensure update
        
        for node_id, node in modified['mapping'].items():
            msg_data = node.get('message')
            if msg_data and msg_data.get('id') == msg.source_id:
                msg_data['content']['parts'] = ['Completely different content']
                break
        
        extractor.extract_dialogue(modified)
        clean_db_session.commit()
        
        # Hash should have changed
        clean_db_session.refresh(msg)
        assert msg.content_hash != original_hash, "Content hash should change when content changes"


class TestAssumeImmutableFlag:
    """Tests for assume_immutable optimization flag."""
    
    def test_immutable_mode_skips_hash_check(self, clean_db_session, chatgpt_simple_conversation):
        """Test that immutable mode skips content hash comparison."""
        extractor = ChatGPTExtractor(clean_db_session, assume_immutable=True)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Get a message
        msg = clean_db_session.query(Message).filter(Message.role == 'user').first()
        original_hash = msg.content_hash
        original_uuid = msg.id
        
        # "Modify" content in source (simulating what we'd do in mutable mode)
        # In immutable mode, this shouldn't trigger an update
        modified = copy.deepcopy(chatgpt_simple_conversation)
        modified['update_time'] = 1800000000.0
        
        for node_id, node in modified['mapping'].items():
            msg_data = node.get('message')
            if msg_data and msg_data.get('id') == msg.source_id:
                msg_data['content']['parts'] = ['MODIFIED CONTENT']
                break
        
        extractor.extract_dialogue(modified)
        clean_db_session.commit()
        
        # In immutable mode, hash should NOT change (we didn't check it)
        clean_db_session.refresh(msg)
        assert msg.id == original_uuid, "UUID should be preserved"
        assert msg.content_hash == original_hash, "Hash should NOT change in immutable mode"
    
    def test_mutable_mode_detects_changes(self, clean_db_session, chatgpt_simple_conversation):
        """Test that mutable mode (default) detects content changes."""
        extractor = ChatGPTExtractor(clean_db_session, assume_immutable=False)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Get a message
        msg = clean_db_session.query(Message).filter(Message.role == 'user').first()
        original_hash = msg.content_hash
        
        # Modify content
        modified = copy.deepcopy(chatgpt_simple_conversation)
        modified['update_time'] = 1800000000.0
        
        for node_id, node in modified['mapping'].items():
            msg_data = node.get('message')
            if msg_data and msg_data.get('id') == msg.source_id:
                msg_data['content']['parts'] = ['MODIFIED CONTENT']
                break
        
        extractor.extract_dialogue(modified)
        clean_db_session.commit()
        
        # In mutable mode, hash SHOULD change
        clean_db_session.refresh(msg)
        assert msg.content_hash != original_hash, "Hash SHOULD change in mutable mode"
    
    def test_immutable_mode_still_creates_new_messages(self, clean_db_session, chatgpt_simple_conversation):
        """Test that immutable mode still creates new messages properly."""
        extractor = ChatGPTExtractor(clean_db_session, assume_immutable=True)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_count = clean_db_session.query(Message).count()
        
        # Add a new message
        extended = copy.deepcopy(chatgpt_simple_conversation)
        extended['update_time'] = 1800000000.0
        
        new_msg_id = "new-immutable-msg"
        last_node_id = list(extended['mapping'].keys())[-1]
        
        extended['mapping'][new_msg_id] = {
            "id": new_msg_id,
            "parent": last_node_id,
            "children": [],
            "message": {
                "id": new_msg_id,
                "author": {"role": "user"},
                "create_time": 1700006000.0,
                "content": {"content_type": "text", "parts": ["New message"]}
            }
        }
        
        extractor.extract_dialogue(extended)
        clean_db_session.commit()
        
        # New message should be created
        new_count = clean_db_session.query(Message).count()
        assert new_count == original_count + 1
        
        # And it should have a content hash
        new_msg = clean_db_session.query(Message).filter(
            Message.source_id == new_msg_id
        ).first()
        assert new_msg is not None
        assert new_msg.content_hash is not None
    
    def test_immutable_mode_still_soft_deletes(self, clean_db_session, chatgpt_simple_conversation):
        """Test that immutable mode still soft-deletes removed messages."""
        extractor = ChatGPTExtractor(clean_db_session, assume_immutable=True)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Remove a message
        truncated = copy.deepcopy(chatgpt_simple_conversation)
        truncated['update_time'] = 1800000000.0
        
        mapping_keys = list(truncated['mapping'].keys())
        removed_key = mapping_keys[-1]
        del truncated['mapping'][removed_key]
        
        for node_id, node in truncated['mapping'].items():
            if removed_key in node.get('children', []):
                node['children'].remove(removed_key)
        
        extractor.extract_dialogue(truncated)
        clean_db_session.commit()
        
        # Message should be soft-deleted
        deleted_count = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).count()
        assert deleted_count == 1
    
    def test_immutable_mode_restores_soft_deleted(self, clean_db_session, chatgpt_simple_conversation):
        """Test that immutable mode restores soft-deleted messages without re-hashing."""
        extractor = ChatGPTExtractor(clean_db_session, assume_immutable=True)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Remove and soft-delete a message
        truncated = copy.deepcopy(chatgpt_simple_conversation)
        truncated['update_time'] = 1800000000.0
        
        mapping_keys = list(truncated['mapping'].keys())
        removed_key = mapping_keys[-1]
        del truncated['mapping'][removed_key]
        
        for node_id, node in truncated['mapping'].items():
            if removed_key in node.get('children', []):
                node['children'].remove(removed_key)
        
        extractor.extract_dialogue(truncated)
        clean_db_session.commit()
        
        # Verify soft-deleted
        deleted_msg = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).first()
        assert deleted_msg is not None
        original_hash = deleted_msg.content_hash
        
        # Restore by importing original with newer timestamp
        restored = copy.deepcopy(chatgpt_simple_conversation)
        restored['update_time'] = 1900000000.0
        
        extractor.extract_dialogue(restored)
        clean_db_session.commit()
        
        # Should be restored
        clean_db_session.refresh(deleted_msg)
        assert deleted_msg.deleted_at is None
        # Hash should be unchanged (we didn't re-hash)
        assert deleted_msg.content_hash == original_hash
    
    def test_claude_immutable_mode(self, clean_db_session, claude_simple_conversation):
        """Test that assume_immutable works for Claude extractor too."""
        extractor = ClaudeExtractor(clean_db_session, assume_immutable=True)
        
        # First import
        extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        
        # Get a message
        msg = clean_db_session.query(Message).filter(Message.role == 'user').first()
        original_hash = msg.content_hash
        
        # "Modify" content
        modified = copy.deepcopy(claude_simple_conversation)
        modified['updated_at'] = "2025-01-01T00:00:00Z"
        
        for m in modified['chat_messages']:
            if m.get('uuid') == msg.source_id:
                m['content'] = [{'type': 'text', 'text': 'MODIFIED'}]
                break
        
        extractor.extract_dialogue(modified)
        clean_db_session.commit()
        
        # In immutable mode, hash should NOT change
        clean_db_session.refresh(msg)
        assert msg.content_hash == original_hash


class TestIncrementalMode:
    """Tests for incremental (delta import) mode."""
    
    def test_incremental_mode_skips_soft_delete(self, clean_db_session, chatgpt_simple_conversation):
        """Test that incremental mode doesn't soft-delete missing messages."""
        extractor = ChatGPTExtractor(clean_db_session, incremental=True)
        
        # First import - full conversation
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_count = clean_db_session.query(Message).count()
        
        # Second import - partial conversation (remove a message)
        partial = copy.deepcopy(chatgpt_simple_conversation)
        partial['update_time'] = 1800000000.0
        
        mapping_keys = list(partial['mapping'].keys())
        removed_key = mapping_keys[-1]
        del partial['mapping'][removed_key]
        
        for node_id, node in partial['mapping'].items():
            if removed_key in node.get('children', []):
                node['children'].remove(removed_key)
        
        extractor.extract_dialogue(partial)
        clean_db_session.commit()
        
        # In incremental mode, no messages should be soft-deleted
        deleted_count = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).count()
        assert deleted_count == 0, "Incremental mode should not soft-delete"
        
        # Total count should be unchanged
        assert clean_db_session.query(Message).count() == original_count
    
    def test_non_incremental_mode_does_soft_delete(self, clean_db_session, chatgpt_simple_conversation):
        """Test that non-incremental mode (default) does soft-delete missing messages."""
        extractor = ChatGPTExtractor(clean_db_session, incremental=False)
        
        # First import - full conversation
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_count = clean_db_session.query(Message).count()
        
        # Second import - partial conversation
        partial = copy.deepcopy(chatgpt_simple_conversation)
        partial['update_time'] = 1800000000.0
        
        mapping_keys = list(partial['mapping'].keys())
        removed_key = mapping_keys[-1]
        del partial['mapping'][removed_key]
        
        for node_id, node in partial['mapping'].items():
            if removed_key in node.get('children', []):
                node['children'].remove(removed_key)
        
        extractor.extract_dialogue(partial)
        clean_db_session.commit()
        
        # In non-incremental mode, missing message should be soft-deleted
        deleted_count = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).count()
        assert deleted_count == 1, "Non-incremental mode should soft-delete"
    
    def test_incremental_mode_still_adds_new_messages(self, clean_db_session, chatgpt_simple_conversation):
        """Test that incremental mode still adds new messages."""
        extractor = ChatGPTExtractor(clean_db_session, incremental=True)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_count = clean_db_session.query(Message).count()
        
        # Add a new message
        extended = copy.deepcopy(chatgpt_simple_conversation)
        extended['update_time'] = 1800000000.0
        
        new_msg_id = "new-incremental-msg"
        last_node_id = list(extended['mapping'].keys())[-1]
        
        extended['mapping'][new_msg_id] = {
            "id": new_msg_id,
            "parent": last_node_id,
            "children": [],
            "message": {
                "id": new_msg_id,
                "author": {"role": "user"},
                "create_time": 1700006000.0,
                "content": {"content_type": "text", "parts": ["New message"]}
            }
        }
        
        extractor.extract_dialogue(extended)
        clean_db_session.commit()
        
        # New message should be created
        assert clean_db_session.query(Message).count() == original_count + 1
    
    def test_incremental_mode_still_updates_changed_messages(self, clean_db_session, chatgpt_simple_conversation):
        """Test that incremental mode still updates changed messages."""
        # Use mutable + incremental mode
        extractor = ChatGPTExtractor(clean_db_session, assume_immutable=False, incremental=True)
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        msg = clean_db_session.query(Message).filter(Message.role == 'user').first()
        original_hash = msg.content_hash
        
        # Modify message content
        modified = copy.deepcopy(chatgpt_simple_conversation)
        modified['update_time'] = 1800000000.0
        
        for node_id, node in modified['mapping'].items():
            msg_data = node.get('message')
            if msg_data and msg_data.get('id') == msg.source_id:
                msg_data['content']['parts'] = ['MODIFIED CONTENT']
                break
        
        extractor.extract_dialogue(modified)
        clean_db_session.commit()
        
        # Content should be updated
        clean_db_session.refresh(msg)
        assert msg.content_hash != original_hash
    
    def test_claude_incremental_mode(self, clean_db_session, claude_simple_conversation):
        """Test that incremental mode works for Claude extractor."""
        extractor = ClaudeExtractor(clean_db_session, incremental=True)
        
        # First import
        extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        
        original_count = clean_db_session.query(Message).count()
        
        # Partial import (remove first message)
        partial = copy.deepcopy(claude_simple_conversation)
        partial['updated_at'] = "2025-01-01T00:00:00Z"
        partial['chat_messages'] = partial['chat_messages'][1:]  # Remove first message
        
        extractor.extract_dialogue(partial)
        clean_db_session.commit()
        
        # No messages should be soft-deleted
        deleted_count = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).count()
        assert deleted_count == 0
    
    def test_combined_immutable_and_incremental(self, clean_db_session, chatgpt_simple_conversation):
        """Test combining immutable and incremental modes for fastest delta imports."""
        extractor = ChatGPTExtractor(
            clean_db_session, 
            assume_immutable=True, 
            incremental=True
        )
        
        # First import
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        original_count = clean_db_session.query(Message).count()
        msg = clean_db_session.query(Message).filter(Message.role == 'user').first()
        original_hash = msg.content_hash
        
        # Partial import with "modified" content
        partial = copy.deepcopy(chatgpt_simple_conversation)
        partial['update_time'] = 1800000000.0
        
        # Remove a message
        mapping_keys = list(partial['mapping'].keys())
        removed_key = mapping_keys[-1]
        del partial['mapping'][removed_key]
        
        # "Modify" existing message (should be ignored in immutable mode)
        for node_id, node in partial['mapping'].items():
            msg_data = node.get('message')
            if msg_data and msg_data.get('id') == msg.source_id:
                msg_data['content']['parts'] = ['SHOULD BE IGNORED']
            if removed_key in node.get('children', []):
                node['children'].remove(removed_key)
        
        extractor.extract_dialogue(partial)
        clean_db_session.commit()
        
        # No soft-deletes (incremental mode)
        deleted_count = clean_db_session.query(Message).filter(
            Message.deleted_at.isnot(None)
        ).count()
        assert deleted_count == 0
        
        # Hash unchanged (immutable mode)
        clean_db_session.refresh(msg)
        assert msg.content_hash == original_hash



---
File: tests/integration/test_models.py
---
# tests/integration/test_models.py
"""Integration tests for SQLAlchemy models with database."""

import pytest
from uuid import uuid4
from datetime import datetime, timezone

from llm_archive.models import (
    Dialogue, Message, ContentPart,
)


class TestRawModels:
    """Tests for raw schema models with database persistence."""
    
    def test_create_dialogue(self, db_session):
        """Test creating and persisting a dialogue."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='test-001',
            title='Test Dialogue',
            created_at=datetime.now(timezone.utc),
            source_json={'test': True},
        )
        db_session.add(dialogue)
        db_session.flush()
        
        assert dialogue.id is not None
        assert dialogue.source == 'chatgpt'
    
    def test_create_message_with_parent(self, db_session):
        """Test creating messages with parent relationship."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='test-002',
            source_json={'conversation_id': 'test-002'},
        )
        db_session.add(dialogue)
        db_session.flush()
        
        msg1 = Message(
            dialogue_id=dialogue.id,
            source_id='msg-001',
            role='user',
            source_json={'id': 'msg-001', 'role': 'user'},
        )
        db_session.add(msg1)
        db_session.flush()
        
        msg2 = Message(
            dialogue_id=dialogue.id,
            source_id='msg-002',
            role='assistant',
            parent_id=msg1.id,
            source_json={'id': 'msg-002', 'role': 'assistant'},
        )
        db_session.add(msg2)
        db_session.flush()
        
        assert msg2.parent_id == msg1.id
    
    def test_create_content_part(self, db_session):
        """Test creating content parts."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='test-003',
            source_json={},
        )
        db_session.add(dialogue)
        db_session.flush()
        
        message = Message(
            dialogue_id=dialogue.id,
            source_id='msg-003',
            role='assistant',
            source_json={'id': 'msg-003'},
        )
        db_session.add(message)
        db_session.flush()
        
        part = ContentPart(
            message_id=message.id,
            sequence=0,
            part_type='text',
            text_content='Hello world',
            source_json={'type': 'text'},
        )
        db_session.add(part)
        db_session.flush()
        
        assert part.id is not None
        assert part.message_id == message.id
    
    def test_dialogue_messages_relationship(self, db_session):
        """Test dialogue to messages relationship."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='test-004',
            source_json={},
        )
        db_session.add(dialogue)
        db_session.flush()
        
        msg1 = Message(
            dialogue_id=dialogue.id,
            source_id='m1',
            role='user',
            source_json={},
        )
        msg2 = Message(
            dialogue_id=dialogue.id,
            source_id='m2',
            role='assistant',
            source_json={},
        )
        db_session.add_all([msg1, msg2])
        db_session.flush()
        
        # Refresh to load relationship
        db_session.refresh(dialogue)
        
        assert len(dialogue.messages) == 2



class TestCascadeDeletes:
    """Tests for cascade delete behavior."""
    
    def test_delete_dialogue_cascades_to_messages(self, db_session):
        """Test that deleting dialogue deletes messages."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='test-cascade-001',
            source_json={},
        )
        db_session.add(dialogue)
        db_session.flush()
        
        msg = Message(
            dialogue_id=dialogue.id,
            source_id='m1',
            role='user',
            source_json={},
        )
        db_session.add(msg)
        db_session.flush()
        
        dialogue_id = dialogue.id
        
        # Delete dialogue
        db_session.delete(dialogue)
        db_session.flush()
        
        # Message should be gone
        remaining = db_session.query(Message).filter(
            Message.dialogue_id == dialogue_id
        ).count()
        assert remaining == 0
    
    def test_delete_message_cascades_to_content(self, db_session):
        """Test that deleting message deletes content parts."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='test-cascade-002',
            source_json={},
        )
        db_session.add(dialogue)
        db_session.flush()
        
        msg = Message(
            dialogue_id=dialogue.id,
            source_id='m1',
            role='user',
            source_json={},
        )
        db_session.add(msg)
        db_session.flush()
        
        part = ContentPart(
            message_id=msg.id,
            sequence=0,
            part_type='text',
            text_content='Hello',
            source_json={},
        )
        db_session.add(part)
        db_session.flush()
        
        msg_id = msg.id
        
        # Delete message
        db_session.delete(msg)
        db_session.flush()
        
        # Content part should be gone
        remaining = db_session.query(ContentPart).filter(
            ContentPart.message_id == msg_id
        ).count()
        assert remaining == 0



---
File: tests/integration/test_prompt_response_builder.py
---
# tests/integration/test_prompt_response_builder.py
"""Integration tests for PromptResponseBuilder."""

import pytest
from uuid import UUID

from llm_archive.extractors.chatgpt import ChatGPTExtractor
from llm_archive.extractors.claude import ClaudeExtractor
from llm_archive.builders.prompt_response import PromptResponseBuilder
from llm_archive.models import Dialogue, Message, PromptResponse


class TestPromptResponseBuilderBasic:
    """Basic tests for PromptResponseBuilder."""
    
    def test_build_for_simple_conversation(self, clean_db_session, chatgpt_simple_conversation):
        """Test building prompt-responses for a simple conversation."""
        # Import conversation
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        # Build prompt-responses
        builder = PromptResponseBuilder(clean_db_session)
        stats = builder.build_all()
        
        assert stats['prompt_responses'] > 0
        
        # Verify records exist
        prs = clean_db_session.query(PromptResponse).all()
        assert len(prs) > 0
    
    def test_pairs_user_with_assistant(self, clean_db_session, chatgpt_simple_conversation):
        """Test that user messages are paired with assistant responses."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        builder.build_all()
        
        # Get all prompt-responses
        prs = clean_db_session.query(PromptResponse).all()
        
        for pr in prs:
            prompt_msg = clean_db_session.get(Message, pr.prompt_message_id)
            response_msg = clean_db_session.get(Message, pr.response_message_id)
            
            assert prompt_msg.role == 'user'
            assert response_msg.role == 'assistant'
    
    def test_response_position_ordering(self, clean_db_session, chatgpt_simple_conversation):
        """Test that response_position reflects message order."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        builder.build_all()
        
        dialogue = clean_db_session.query(Dialogue).first()
        prs = clean_db_session.query(PromptResponse).filter(
            PromptResponse.dialogue_id == dialogue.id
        ).order_by(PromptResponse.response_position).all()
        
        # Positions should be monotonically increasing
        positions = [pr.response_position for pr in prs]
        assert positions == sorted(positions)
        assert len(set(positions)) == len(positions)  # No duplicates


class TestPromptResponseBuilderClaude:
    """Tests specific to Claude conversations."""
    
    def test_build_for_claude_conversation(self, clean_db_session, claude_simple_conversation):
        """Test building prompt-responses for Claude conversation."""
        extractor = ClaudeExtractor(clean_db_session)
        extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        stats = builder.build_all()
        
        assert stats['prompt_responses'] > 0
    
    def test_linear_chain_pairing(self, clean_db_session, claude_simple_conversation):
        """Test that linear chains are paired correctly."""
        extractor = ClaudeExtractor(clean_db_session)
        extractor.extract_dialogue(claude_simple_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        builder.build_all()
        
        # Each assistant message should be paired with preceding user message
        prs = clean_db_session.query(PromptResponse).all()
        
        for pr in prs:
            assert pr.prompt_position < pr.response_position


class TestPromptResponseBuilderBranched:
    """Tests for branched conversations."""
    
    def test_build_for_branched_conversation(self, clean_db_session, chatgpt_branched_conversation):
        """Test building prompt-responses for branched conversation."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_branched_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        stats = builder.build_all()
        
        # Should handle branches without error
        assert stats['prompt_responses'] > 0
    
    def test_uses_parent_id_for_pairing(self, clean_db_session, chatgpt_branched_conversation):
        """Test that parent_id is used to find the correct prompt."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_branched_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        builder.build_all()
        
        prs = clean_db_session.query(PromptResponse).all()
        
        for pr in prs:
            response_msg = clean_db_session.get(Message, pr.response_message_id)
            prompt_msg = clean_db_session.get(Message, pr.prompt_message_id)
            
            # If response has a parent, verify the relationship
            if response_msg.parent_id:
                # The prompt should be the parent or an ancestor
                # (For regenerations, multiple responses may share a prompt)
                pass  # Complex to verify without tree traversal


class TestPromptResponseBuilderIdempotency:
    """Tests for idempotent building."""
    
    def test_rebuild_clears_existing(self, clean_db_session, chatgpt_simple_conversation):
        """Test that rebuilding clears and recreates records."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        clean_db_session.commit()
        
        builder = PromptResponseBuilder(clean_db_session)
        
        # Build first time
        stats1 = builder.build_all()
        first_count = stats1['prompt_responses']
        
        # Build again
        stats2 = builder.build_all()
        second_count = stats2['prompt_responses']
        
        # Should have same count (cleared and rebuilt)
        assert first_count == second_count
        
        # Total records should equal one build's worth
        total = clean_db_session.query(PromptResponse).count()
        assert total == first_count
    
    def test_build_for_single_dialogue(self, clean_db_session, chatgpt_simple_conversation, chatgpt_branched_conversation):
        """Test building for a single dialogue doesn't affect others."""
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(chatgpt_simple_conversation)
        extractor.extract_dialogue(chatgpt_branched_conversation)
        clean_db_session.commit()
        
        dialogues = clean_db_session.query(Dialogue).all()
        assert len(dialogues) == 2
        
        builder = PromptResponseBuilder(clean_db_session)
        
        # Build for first dialogue only
        builder.build_for_dialogue(dialogues[0].id)
        
        # Should only have records for first dialogue
        prs = clean_db_session.query(PromptResponse).all()
        dialogue_ids = {pr.dialogue_id for pr in prs}
        
        assert dialogues[0].id in dialogue_ids
        # Second dialogue may or may not be present depending on implementation


class TestPromptResponseBuilderEdgeCases:
    """Edge case tests."""
    
    def test_handles_system_messages(self, clean_db_session):
        """Test handling of conversations with system messages."""
        conversation = {
            'conversation_id': 'conv-system',
            'title': 'System Message Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-sys': {
                    'id': 'node-sys',
                    'message': {
                        'id': 'msg-sys',
                        'author': {'role': 'system'},
                        'content': {'parts': ['You are a helpful assistant.']},
                        'create_time': 1700000000,
                    },
                    'parent': None,
                },
                'node-user': {
                    'id': 'node-user',
                    'message': {
                        'id': 'msg-user',
                        'author': {'role': 'user'},
                        'content': {'parts': ['Hello']},
                        'create_time': 1700000001,
                    },
                    'parent': 'node-sys',
                },
                'node-asst': {
                    'id': 'node-asst',
                    'message': {
                        'id': 'msg-asst',
                        'author': {'role': 'assistant'},
                        'content': {'parts': ['Hi there!']},
                        'create_time': 1700000002,
                    },
                    'parent': 'node-user',
                },
            },
        }
        
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(conversation)
        clean_db_session.commit()
        
        # Get the dialogue we just created
        dialogue = clean_db_session.query(Dialogue).filter(
            Dialogue.source_id == 'conv-system'
        ).one()
        
        builder = PromptResponseBuilder(clean_db_session)
        stats = builder.build_for_dialogue(dialogue.id)
        
        # Should create one prompt-response (user -> assistant)
        # System message should not be part of a pair
        assert stats['prompt_responses'] == 1
        
        prs = clean_db_session.query(PromptResponse).filter(
            PromptResponse.dialogue_id == dialogue.id
        ).all()
        assert len(prs) == 1
        
        pr = prs[0]
        prompt = clean_db_session.get(Message, pr.prompt_message_id)
        assert prompt.role == 'user'
    
    def test_handles_empty_dialogue(self, clean_db_session):
        """Test handling of dialogue with no messages."""
        conversation = {
            'conversation_id': 'conv-empty',
            'title': 'Empty',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {},
        }
        
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(conversation)
        clean_db_session.commit()
        
        # Get the dialogue we just created
        dialogue = clean_db_session.query(Dialogue).filter(
            Dialogue.source_id == 'conv-empty'
        ).one()
        
        builder = PromptResponseBuilder(clean_db_session)
        stats = builder.build_for_dialogue(dialogue.id)
        
        assert stats['prompt_responses'] == 0
    
    def test_handles_user_only_dialogue(self, clean_db_session):
        """Test handling of dialogue with only user messages."""
        conversation = {
            'conversation_id': 'conv-user-only',
            'title': 'User Only',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': {
                        'id': 'msg-1',
                        'author': {'role': 'user'},
                        'content': {'parts': ['Hello?']},
                        'create_time': 1700000000,
                    },
                    'parent': None,
                },
            },
        }
        
        extractor = ChatGPTExtractor(clean_db_session)
        extractor.extract_dialogue(conversation)
        clean_db_session.commit()
        
        # Get the dialogue we just created
        dialogue = clean_db_session.query(Dialogue).filter(
            Dialogue.source_id == 'conv-user-only'
        ).one()
        
        builder = PromptResponseBuilder(clean_db_session)
        stats = builder.build_for_dialogue(dialogue.id)
        
        # No assistant responses means no prompt-response pairs
        assert stats['prompt_responses'] == 0



---
File: tests/unit/__init__.py
---
# tests/unit/__init__.py
"""Unit tests - no database required."""



---
File: tests/unit/conftest.py
---
# tests/unit/conftest.py
"""Fixtures for unit tests - no database required."""

import uuid
from datetime import datetime, timezone
from unittest.mock import MagicMock, patch

import pytest


# ============================================================
# Sample Data Fixtures - ChatGPT Format
# ============================================================

@pytest.fixture
def chatgpt_simple_conversation() -> dict:
    """Simple linear ChatGPT conversation (no branches)."""
    root_id = str(uuid.uuid4())
    msg1_id = str(uuid.uuid4())
    msg2_id = str(uuid.uuid4())
    msg3_id = str(uuid.uuid4())
    msg4_id = str(uuid.uuid4())
    
    return {
        "conversation_id": "conv-simple-001",
        "title": "Simple Test Conversation",
        "create_time": 1700000000.0,
        "update_time": 1700001000.0,
        "mapping": {
            root_id: {
                "id": root_id,
                "parent": None,
                "children": [msg1_id],
                "message": None
            },
            msg1_id: {
                "id": msg1_id,
                "parent": root_id,
                "children": [msg2_id],
                "message": {
                    "id": msg1_id,
                    "author": {"role": "user"},
                    "create_time": 1700000100.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Hello, how are you?"]
                    }
                }
            },
            msg2_id: {
                "id": msg2_id,
                "parent": msg1_id,
                "children": [msg3_id],
                "message": {
                    "id": msg2_id,
                    "author": {"role": "assistant"},
                    "create_time": 1700000200.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["I'm doing well, thank you!"]
                    }
                }
            },
            msg3_id: {
                "id": msg3_id,
                "parent": msg2_id,
                "children": [msg4_id],
                "message": {
                    "id": msg3_id,
                    "author": {"role": "user"},
                    "create_time": 1700000300.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Explain Python decorators."]
                    }
                }
            },
            msg4_id: {
                "id": msg4_id,
                "parent": msg3_id,
                "children": [],
                "message": {
                    "id": msg4_id,
                    "author": {"role": "assistant"},
                    "create_time": 1700000400.0,
                    "content": {
                        "content_type": "text",
                        "parts": ["Python decorators are functions that modify other functions.\n\n```python\ndef decorator(func):\n    pass\n```"]
                    }
                }
            }
        }
    }


@pytest.fixture
def claude_simple_conversation() -> dict:
    """Simple Claude conversation."""
    return {
        "uuid": "claude-conv-001",
        "name": "Claude Test Conversation",
        "created_at": "2024-01-15T10:00:00Z",
        "updated_at": "2024-01-15T10:30:00Z",
        "chat_messages": [
            {
                "uuid": "claude-msg-001",
                "sender": "human",
                "created_at": "2024-01-15T10:00:00Z",
                "content": [
                    {"type": "text", "text": "Hello Claude!"}
                ]
            },
            {
                "uuid": "claude-msg-002",
                "sender": "assistant",
                "created_at": "2024-01-15T10:01:00Z",
                "content": [
                    {"type": "text", "text": "Hello! How can I help you today?"}
                ]
            },
        ]
    }


# ============================================================
# Mock Session Fixture
# ============================================================

@pytest.fixture
def mock_session():
    """Create a mock SQLAlchemy session for unit tests."""
    session = MagicMock()
    session.query.return_value.filter.return_value.first.return_value = None
    session.query.return_value.filter.return_value.all.return_value = []
    session.query.return_value.count.return_value = 0
    return session



---
File: tests/unit/test_annotations.py
---
# tests/unit/test_annotations.py
"""Unit tests for annotation infrastructure."""

import pytest
from uuid import uuid4

from llm_archive.annotations.core import (
    EntityType,
    ValueType,
    AnnotationResult,
    AnnotationWriter,
    AnnotationReader,
)


# ============================================================
# AnnotationResult Tests
# ============================================================

class TestAnnotationResult:
    """Test AnnotationResult dataclass."""
    
    def test_create_flag_result(self):
        """Flag results only need key."""
        result = AnnotationResult(
            key='has_code',
            value_type=ValueType.FLAG,
        )
        assert result.key == 'has_code'
        assert result.value is None
        assert result.value_type == ValueType.FLAG
    
    def test_create_string_result(self):
        """String results need key and value."""
        result = AnnotationResult(
            key='exchange_type',
            value='wiki_article',
            value_type=ValueType.STRING,
            confidence=0.9,
            reason='wiki_links_detected',
        )
        assert result.key == 'exchange_type'
        assert result.value == 'wiki_article'
        assert result.value_type == ValueType.STRING
        assert result.confidence == 0.9
        assert result.reason == 'wiki_links_detected'
    
    def test_create_numeric_result(self):
        """Numeric results need key and numeric value."""
        result = AnnotationResult(
            key='wiki_link_count',
            value=5,
            value_type=ValueType.NUMERIC,
        )
        assert result.key == 'wiki_link_count'
        assert result.value == 5
        assert result.value_type == ValueType.NUMERIC
    
    def test_create_json_result(self):
        """JSON results can store complex data."""
        result = AnnotationResult(
            key='metadata',
            value={'domains': ['example.com', 'test.org']},
            value_type=ValueType.JSON,
        )
        assert result.key == 'metadata'
        assert result.value == {'domains': ['example.com', 'test.org']}
        assert result.value_type == ValueType.JSON
    
    def test_default_value_type_is_string(self):
        """Default value_type should be STRING."""
        result = AnnotationResult(key='title', value='Test Title')
        assert result.value_type == ValueType.STRING
    
    def test_default_source_is_heuristic(self):
        """Default source should be 'heuristic'."""
        result = AnnotationResult(key='test', value='value')
        assert result.source == 'heuristic'


# ============================================================
# EntityType and ValueType Enum Tests
# ============================================================

class TestEnums:
    """Test EntityType and ValueType enums."""
    
    def test_entity_types(self):
        """All expected entity types exist."""
        assert EntityType.CONTENT_PART.value == 'content_part'
        assert EntityType.MESSAGE.value == 'message'
        assert EntityType.PROMPT_RESPONSE.value == 'prompt_response'
        assert EntityType.DIALOGUE.value == 'dialogue'
    
    def test_value_types(self):
        """All expected value types exist."""
        assert ValueType.FLAG.value == 'flag'
        assert ValueType.STRING.value == 'string'
        assert ValueType.NUMERIC.value == 'numeric'
        assert ValueType.JSON.value == 'json'


# ============================================================
# AnnotationWriter Tests (mock-based, no DB)
# ============================================================

class TestAnnotationWriterInterface:
    """Test AnnotationWriter interface without database."""
    
    def test_table_name_generation(self):
        """Test table name generation for entity/value type combos."""
        # Can't instantiate without session, but can test the pattern
        template = "derived.{entity}_annotations_{value_type}"
        
        assert template.format(
            entity='message', value_type='string'
        ) == 'derived.message_annotations_string'
        
        assert template.format(
            entity='content_part', value_type='flag'
        ) == 'derived.content_part_annotations_flag'
        
        assert template.format(
            entity='prompt_response', value_type='numeric'
        ) == 'derived.prompt_response_annotations_numeric'


# ============================================================
# Integration test fixtures (require database)
# ============================================================

@pytest.fixture
def db_session():
    """
    Create a database session for integration tests.
    
    This fixture is a placeholder - actual implementation would
    need a test database setup.
    """
    pytest.skip("Requires database setup")


class TestAnnotationWriterIntegration:
    """Integration tests for AnnotationWriter (require database)."""
    
    def test_write_flag_creates_record(self, db_session):
        """Writing a flag creates a record in flag table."""
        writer = AnnotationWriter(db_session)
        entity_id = uuid4()
        
        result = writer.write_flag(
            entity_type=EntityType.MESSAGE,
            entity_id=entity_id,
            key='has_code',
            source='test',
        )
        
        assert result is True
        # Verify record exists
        reader = AnnotationReader(db_session)
        assert reader.has_flag(EntityType.MESSAGE, entity_id, 'has_code')
    
    def test_write_string_creates_record(self, db_session):
        """Writing a string creates a record in string table."""
        writer = AnnotationWriter(db_session)
        entity_id = uuid4()
        
        result = writer.write_string(
            entity_type=EntityType.MESSAGE,
            entity_id=entity_id,
            key='gizmo_id',
            value='g-12345',
            source='test',
        )
        
        assert result is True
        reader = AnnotationReader(db_session)
        values = reader.get_string(EntityType.MESSAGE, entity_id, 'gizmo_id')
        assert 'g-12345' in values
    
    def test_write_duplicate_flag_returns_false(self, db_session):
        """Writing duplicate flag returns False (no new record)."""
        writer = AnnotationWriter(db_session)
        entity_id = uuid4()
        
        # First write succeeds
        result1 = writer.write_flag(
            entity_type=EntityType.MESSAGE,
            entity_id=entity_id,
            key='has_code',
            source='test',
        )
        assert result1 is True
        
        # Duplicate returns False
        result2 = writer.write_flag(
            entity_type=EntityType.MESSAGE,
            entity_id=entity_id,
            key='has_code',
            source='test',
        )
        assert result2 is False
    
    def test_write_multi_value_string(self, db_session):
        """Can write multiple values for same string key."""
        writer = AnnotationWriter(db_session)
        entity_id = uuid4()
        
        writer.write_string(
            entity_type=EntityType.MESSAGE,
            entity_id=entity_id,
            key='tag',
            value='coding',
            source='test',
        )
        writer.write_string(
            entity_type=EntityType.MESSAGE,
            entity_id=entity_id,
            key='tag',
            value='python',
            source='test',
        )
        
        reader = AnnotationReader(db_session)
        values = reader.get_string(EntityType.MESSAGE, entity_id, 'tag')
        assert set(values) == {'coding', 'python'}


class TestAnnotationReaderIntegration:
    """Integration tests for AnnotationReader (require database)."""
    
    def test_find_entities_with_flag(self, db_session):
        """Can find all entities with a specific flag."""
        writer = AnnotationWriter(db_session)
        
        # Create some flagged entities
        id1, id2, id3 = uuid4(), uuid4(), uuid4()
        writer.write_flag(EntityType.MESSAGE, id1, 'has_code', source='test')
        writer.write_flag(EntityType.MESSAGE, id2, 'has_code', source='test')
        writer.write_flag(EntityType.MESSAGE, id3, 'has_attachment', source='test')
        
        reader = AnnotationReader(db_session)
        results = reader.find_entities_with_flag(EntityType.MESSAGE, 'has_code')
        
        assert id1 in results
        assert id2 in results
        assert id3 not in results
    
    def test_find_entities_with_string_value(self, db_session):
        """Can find entities with specific string value."""
        writer = AnnotationWriter(db_session)
        
        id1, id2 = uuid4(), uuid4()
        writer.write_string(EntityType.MESSAGE, id1, 'gizmo_id', 'g-wiki', source='test')
        writer.write_string(EntityType.MESSAGE, id2, 'gizmo_id', 'g-other', source='test')
        
        reader = AnnotationReader(db_session)
        results = reader.find_entities_with_string(
            EntityType.MESSAGE, 'gizmo_id', 'g-wiki'
        )
        
        assert id1 in results
        assert id2 not in results



---
File: tests/unit/test_chatgpt_extractor_annotations.py
---
# tests/unit/test_chatgpt_extractor_annotations.py
"""Unit tests for ChatGPT extractor annotation integration.

These tests verify that the ChatGPT extractor correctly writes
annotations during ingestion for:
- gizmo_id (string annotation on messages)
- has_gizmo (flag annotation on messages)
- model_slug (string annotation on messages)
- Canvas metadata (annotations on content_parts)
"""

import pytest
from uuid import uuid4


# ============================================================
# Test data fixtures
# ============================================================

def make_message_data(
    msg_id: str = None,
    role: str = 'assistant',
    content: str = 'Test content',
    gizmo_id: str = None,
    model_slug: str = None,
    canvas: dict = None,
) -> dict:
    """Create mock ChatGPT message data."""
    msg_id = msg_id or str(uuid4())
    
    metadata = {}
    if gizmo_id:
        metadata['gizmo_id'] = gizmo_id
    if model_slug:
        metadata['model_slug'] = model_slug
    if canvas:
        metadata['canvas'] = canvas
    
    return {
        'id': msg_id,
        'author': {
            'role': role,
            'name': None,
            'metadata': {},
        },
        'content': {
            'content_type': 'text',
            'parts': [content] if content else [],
        },
        'metadata': metadata,
        'create_time': 1700000000,
        'update_time': 1700000000,
        'status': 'finished',
        'end_turn': True,
    }


def make_canvas_data(
    textdoc_id: str = 'doc-123',
    version: int = 1,
    title: str = 'Test Canvas',
    textdoc_type: str = 'document',
    content: str = 'Canvas content here',
) -> dict:
    """Create mock canvas data."""
    return {
        'textdoc_id': textdoc_id,
        'version': version,
        'title': title,
        'textdoc_type': textdoc_type,
        'content': content,
        'from_version': version - 1 if version > 1 else None,
        'textdoc_content_length': len(content) if content else 0,
        'has_user_edit': False,
    }


# ============================================================
# Unit tests (no database required)
# ============================================================

class TestMessageDataConstruction:
    """Test message data fixture construction."""
    
    def test_basic_message_data(self):
        """Basic message data should have required fields."""
        data = make_message_data()
        
        assert 'id' in data
        assert data['author']['role'] == 'assistant'
        assert data['content']['parts'] == ['Test content']
    
    def test_message_with_gizmo(self):
        """Message with gizmo_id should have it in metadata."""
        data = make_message_data(gizmo_id='g-wiki-generator')
        
        assert data['metadata']['gizmo_id'] == 'g-wiki-generator'
    
    def test_message_with_model(self):
        """Message with model_slug should have it in metadata."""
        data = make_message_data(model_slug='gpt-4')
        
        assert data['metadata']['model_slug'] == 'gpt-4'
    
    def test_message_with_canvas(self):
        """Message with canvas should have canvas in metadata."""
        canvas = make_canvas_data(title='My Document')
        data = make_message_data(canvas=canvas)
        
        assert data['metadata']['canvas']['title'] == 'My Document'


class TestCanvasDataConstruction:
    """Test canvas data fixture construction."""
    
    def test_basic_canvas_data(self):
        """Basic canvas data should have required fields."""
        canvas = make_canvas_data()
        
        assert canvas['textdoc_id'] == 'doc-123'
        assert canvas['version'] == 1
        assert canvas['title'] == 'Test Canvas'
        assert canvas['textdoc_type'] == 'document'
    
    def test_canvas_version_tracking(self):
        """Canvas with version > 1 should have from_version."""
        canvas = make_canvas_data(version=3)
        
        assert canvas['from_version'] == 2
    
    def test_canvas_first_version(self):
        """First version canvas should have from_version=None."""
        canvas = make_canvas_data(version=1)
        
        assert canvas['from_version'] is None


# ============================================================
# Integration tests (require database)
# ============================================================

@pytest.fixture
def db_session():
    """
    Create a database session for integration tests.
    
    This fixture is a placeholder - actual implementation would
    need a test database setup with schema applied.
    """
    pytest.skip("Requires database setup with schema")


class TestChatGPTExtractorGizmoAnnotations:
    """Test gizmo annotation writing during extraction."""
    
    def test_extracts_gizmo_id_annotation(self, db_session):
        """Gizmo ID should be written as message string annotation."""
        from llm_archive.extractors.chatgpt import ChatGPTExtractor
        from llm_archive.annotations import AnnotationReader, EntityType
        
        extractor = ChatGPTExtractor(db_session)
        
        # Create a dialogue with a message using a gizmo
        dialogue_data = {
            'conversation_id': 'test-conv-1',
            'title': 'Test Conversation',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(
                        msg_id='msg-1',
                        gizmo_id='g-wiki-generator',
                    ),
                },
            },
        }
        
        result = extractor.extract_dialogue(dialogue_data)
        db_session.commit()
        
        assert result == 'new'
        
        # Verify annotation was written
        reader = AnnotationReader(db_session)
        # Would need to resolve the message ID to verify
    
    def test_extracts_has_gizmo_flag(self, db_session):
        """has_gizmo flag should be written for messages with gizmo."""
        from llm_archive.extractors.chatgpt import ChatGPTExtractor
        
        extractor = ChatGPTExtractor(db_session)
        
        dialogue_data = {
            'conversation_id': 'test-conv-2',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(
                        msg_id='msg-1',
                        gizmo_id='g-test',
                    ),
                },
            },
        }
        
        result = extractor.extract_dialogue(dialogue_data)
        assert result == 'new'
    
    def test_no_gizmo_annotation_when_missing(self, db_session):
        """Messages without gizmo should not have gizmo annotations."""
        from llm_archive.extractors.chatgpt import ChatGPTExtractor
        
        extractor = ChatGPTExtractor(db_session)
        
        dialogue_data = {
            'conversation_id': 'test-conv-3',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(
                        msg_id='msg-1',
                        gizmo_id=None,  # No gizmo
                    ),
                },
            },
        }
        
        result = extractor.extract_dialogue(dialogue_data)
        assert result == 'new'


class TestChatGPTExtractorCanvasAnnotations:
    """Test canvas annotation writing during extraction."""
    
    def test_extracts_canvas_as_content_part(self, db_session):
        """Canvas should be created as content_part with type='canvas'."""
        from llm_archive.extractors.chatgpt import ChatGPTExtractor
        from llm_archive.models import ContentPart
        
        extractor = ChatGPTExtractor(db_session)
        
        canvas = make_canvas_data(
            textdoc_id='doc-abc',
            version=1,
            title='My Wiki Article',
            content='Article content here',
        )
        
        dialogue_data = {
            'conversation_id': 'test-conv-canvas',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(
                        msg_id='msg-1',
                        content='Here is your document',
                        canvas=canvas,
                    ),
                },
            },
        }
        
        result = extractor.extract_dialogue(dialogue_data)
        db_session.commit()
        
        assert result == 'new'
        
        # Verify canvas content_part was created
        canvas_parts = db_session.query(ContentPart).filter(
            ContentPart.part_type == 'canvas'
        ).all()
        
        assert len(canvas_parts) >= 1
    
    def test_canvas_title_annotation(self, db_session):
        """Canvas title should be written as content_part annotation."""
        from llm_archive.extractors.chatgpt import ChatGPTExtractor
        from llm_archive.annotations import AnnotationReader, EntityType
        
        extractor = ChatGPTExtractor(db_session)
        
        canvas = make_canvas_data(title='Important Document')
        
        dialogue_data = {
            'conversation_id': 'test-conv-canvas-title',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(canvas=canvas),
                },
            },
        }
        
        result = extractor.extract_dialogue(dialogue_data)
        db_session.commit()
        
        # Would verify title annotation exists
    
    def test_canvas_version_annotation(self, db_session):
        """Canvas version should be written as numeric annotation."""
        from llm_archive.extractors.chatgpt import ChatGPTExtractor
        
        extractor = ChatGPTExtractor(db_session)
        
        canvas = make_canvas_data(version=5)
        
        dialogue_data = {
            'conversation_id': 'test-conv-canvas-version',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(canvas=canvas),
                },
            },
        }
        
        result = extractor.extract_dialogue(dialogue_data)
        assert result == 'new'


class TestMarkLatestCanvasVersions:
    """Test the mark_latest_canvas_versions utility."""
    
    def test_marks_single_version_as_latest(self, db_session):
        """Single canvas version should be marked as latest."""
        from llm_archive.extractors.chatgpt import (
            ChatGPTExtractor,
            mark_latest_canvas_versions,
        )
        
        extractor = ChatGPTExtractor(db_session)
        
        canvas = make_canvas_data(textdoc_id='doc-single', version=1)
        
        dialogue_data = {
            'conversation_id': 'test-single-version',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(canvas=canvas),
                },
            },
        }
        
        extractor.extract_dialogue(dialogue_data)
        db_session.commit()
        
        count = mark_latest_canvas_versions(db_session)
        assert count >= 1
    
    def test_marks_highest_version_as_latest(self, db_session):
        """With multiple versions, only highest should be marked latest."""
        from llm_archive.extractors.chatgpt import (
            ChatGPTExtractor,
            mark_latest_canvas_versions,
        )
        
        extractor = ChatGPTExtractor(db_session)
        
        # Create messages with different canvas versions
        mapping = {}
        for i, version in enumerate([1, 3, 2]):  # Out of order
            canvas = make_canvas_data(
                textdoc_id='doc-multi',
                version=version,
            )
            mapping[f'node-{i}'] = {
                'id': f'node-{i}',
                'message': make_message_data(
                    msg_id=f'msg-{i}',
                    canvas=canvas,
                ),
            }
        
        dialogue_data = {
            'conversation_id': 'test-multi-version',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': mapping,
        }
        
        extractor.extract_dialogue(dialogue_data)
        db_session.commit()
        
        count = mark_latest_canvas_versions(db_session)
        
        # Should mark version 3 as latest (only one per textdoc_id)
        # Would need to verify the correct one is marked


class TestFindWikiGizmoMessages:
    """Test the find_wiki_gizmo_messages utility."""
    
    def test_finds_messages_by_gizmo(self, db_session):
        """Should find all messages with specific gizmo_id."""
        from llm_archive.extractors.chatgpt import (
            ChatGPTExtractor,
            find_wiki_gizmo_messages,
        )
        
        extractor = ChatGPTExtractor(db_session)
        
        # Create messages with different gizmos
        dialogue_data = {
            'conversation_id': 'test-find-gizmo',
            'title': 'Test',
            'create_time': 1700000000,
            'update_time': 1700000000,
            'mapping': {
                'node-1': {
                    'id': 'node-1',
                    'message': make_message_data(
                        msg_id='msg-1',
                        gizmo_id='g-wiki',
                    ),
                },
                'node-2': {
                    'id': 'node-2',
                    'message': make_message_data(
                        msg_id='msg-2',
                        gizmo_id='g-other',
                    ),
                },
                'node-3': {
                    'id': 'node-3',
                    'message': make_message_data(
                        msg_id='msg-3',
                        gizmo_id='g-wiki',
                    ),
                },
            },
        }
        
        extractor.extract_dialogue(dialogue_data)
        db_session.commit()
        
        wiki_messages = find_wiki_gizmo_messages(db_session, 'g-wiki')
        other_messages = find_wiki_gizmo_messages(db_session, 'g-other')
        
        assert len(wiki_messages) == 2
        assert len(other_messages) == 1
    
    def test_returns_empty_for_unknown_gizmo(self, db_session):
        """Should return empty list for unknown gizmo_id."""
        from llm_archive.extractors.chatgpt import find_wiki_gizmo_messages
        
        messages = find_wiki_gizmo_messages(db_session, 'g-nonexistent')
        assert messages == []



---
File: tests/unit/test_cli.py
---
# tests/unit/test_cli.py
"""Unit tests for CLI interface."""

import json
import tempfile
from pathlib import Path

import pytest

from llm_archive.cli import CLI


class TestCLIInit:
    """Tests for CLI initialization."""
    
    def test_cli_default_db_url(self):
        """Test CLI uses default database URL."""
        cli = CLI()
        assert 'postgresql://' in cli.db_url
    
    def test_cli_custom_db_url(self):
        """Test CLI accepts custom database URL."""
        custom_url = "postgresql://user:pass@host:5432/mydb"
        cli = CLI(db_url=custom_url)
        assert cli.db_url == custom_url


class TestCLILoadJSON:
    """Tests for JSON loading."""
    
    def test_load_json_valid(self):
        """Test loading valid JSON file."""
        cli = CLI()
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump([{"id": "test"}], f)
            temp_path = f.name
        
        try:
            data = cli._load_json(temp_path)
            assert data == [{"id": "test"}]
        finally:
            Path(temp_path).unlink()
    
    def test_load_json_missing_file(self):
        """Test loading missing file raises error."""
        cli = CLI()
        
        with pytest.raises(FileNotFoundError):
            cli._load_json("/nonexistent/path.json")
    
    def test_load_json_invalid_format(self):
        """Test loading non-array JSON raises error."""
        cli = CLI()
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump({"not": "an array"}, f)
            temp_path = f.name
        
        try:
            with pytest.raises(ValueError, match="Expected JSON array"):
                cli._load_json(temp_path)
        finally:
            Path(temp_path).unlink()
    
    def test_load_json_empty_array(self):
        """Test loading empty array."""
        cli = CLI()
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump([], f)
            temp_path = f.name
        
        try:
            data = cli._load_json(temp_path)
            assert data == []
        finally:
            Path(temp_path).unlink()
    
    def test_load_json_multiple_items(self):
        """Test loading array with multiple items."""
        cli = CLI()
        
        items = [{"id": "1"}, {"id": "2"}, {"id": "3"}]
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(items, f)
            temp_path = f.name
        
        try:
            data = cli._load_json(temp_path)
            assert data == items
            assert len(data) == 3
        finally:
            Path(temp_path).unlink()



---
File: tests/unit/test_content_classification.py
---
# tests/unit/test_content_classification.py
"""Unit tests for content part classification logic."""

import pytest

from llm_archive.extractors.chatgpt import ChatGPTExtractor
from llm_archive.extractors.claude import ClaudeExtractor


class TestChatGPTClassifyContentPart:
    """Tests for ChatGPT content part classification."""
    
    @pytest.fixture
    def extractor(self, mock_session):
        """Create extractor with mock session."""
        return ChatGPTExtractor(mock_session)
    
    def test_classify_string_text(self, extractor):
        """Test classifying a plain string as text."""
        result = extractor._classify_content_part("Hello world")
        
        assert result['part_type'] == 'text'
        assert result['text_content'] == "Hello world"
        assert result['source_json'] == {'text': "Hello world"}
    
    def test_classify_dict_text(self, extractor):
        """Test classifying a dict with text."""
        part = {'text': 'Some text content'}
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'text'
        assert result['text_content'] == 'Some text content'
    
    def test_classify_image(self, extractor):
        """Test classifying image content."""
        part = {
            'content_type': 'image/png',
            'asset_pointer': 'file-service://abc123',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'image'
        assert result['media_type'] == 'image/png'
        assert result['url'] == 'file-service://abc123'
    
    def test_classify_image_with_url(self, extractor):
        """Test classifying image with direct URL."""
        part = {
            'content_type': 'image/jpeg',
            'url': 'https://example.com/image.jpg',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'image'
        assert result['media_type'] == 'image/jpeg'
        assert result['url'] == 'https://example.com/image.jpg'
    
    def test_classify_audio(self, extractor):
        """Test classifying audio content."""
        part = {
            'content_type': 'audio/mp3',
            'url': 'https://example.com/audio.mp3',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'audio'
        assert result['media_type'] == 'audio/mp3'
        assert result['url'] == 'https://example.com/audio.mp3'
    
    def test_classify_video(self, extractor):
        """Test classifying video content."""
        part = {
            'content_type': 'video/mp4',
            'asset_pointer': 'file-service://video123',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'video'
        assert result['media_type'] == 'video/mp4'
        assert result['url'] == 'file-service://video123'
    
    def test_classify_code(self, extractor):
        """Test classifying code content."""
        part = {
            'content_type': 'code',
            'language': 'python',
            'text': 'print("hello")',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'code'
        assert result['language'] == 'python'
        assert result['text_content'] == 'print("hello")'
    
    def test_classify_code_by_language(self, extractor):
        """Test classifying code by presence of language field."""
        part = {
            'language': 'javascript',
            'code': 'console.log("hi")',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'code'
        assert result['language'] == 'javascript'
    
    def test_classify_unknown_type(self, extractor):
        """Test classifying unknown content type."""
        part = {'content_type': 'exotic/type', 'data': 'something'}
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'exotic/type'
    
    def test_classify_non_dict(self, extractor):
        """Test classifying non-dict, non-string content."""
        result = extractor._classify_content_part(12345)
        
        assert result['part_type'] == 'unknown'
        assert result['source_json'] == {'raw': '12345'}


class TestClaudeClassifyContentPart:
    """Tests for Claude content part classification."""
    
    @pytest.fixture
    def extractor(self, mock_session):
        """Create extractor with mock session."""
        return ClaudeExtractor(mock_session)
    
    def test_classify_text(self, extractor):
        """Test classifying text content."""
        part = {'type': 'text', 'text': 'Hello from Claude'}
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'text'
        assert result['text_content'] == 'Hello from Claude'
    
    def test_classify_thinking(self, extractor):
        """Test classifying thinking content."""
        part = {'type': 'thinking', 'thinking': 'Let me consider this...'}
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'thinking'
        assert result['text_content'] == 'Let me consider this...'
    
    def test_classify_tool_use(self, extractor):
        """Test classifying tool_use content."""
        part = {
            'type': 'tool_use',
            'name': 'web_search',
            'id': 'tool-abc123',
            'input': {'query': 'climate change'},
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'tool_use'
        assert result['tool_name'] == 'web_search'
        assert result['tool_use_id'] == 'tool-abc123'
        assert result['tool_input'] == {'query': 'climate change'}
        assert result['text_content'] == 'climate change'  # Extracted from input.query
    
    def test_classify_tool_use_text_input(self, extractor):
        """Test classifying tool_use with text input."""
        part = {
            'type': 'tool_use',
            'name': 'code_executor',
            'id': 'tool-xyz',
            'input': {'text': 'print(1+1)'},
        }
        result = extractor._classify_content_part(part)
        
        assert result['tool_name'] == 'code_executor'
        assert result['text_content'] == 'print(1+1)'
    
    def test_classify_tool_result_string(self, extractor):
        """Test classifying tool_result with string content."""
        part = {
            'type': 'tool_result',
            'tool_use_id': 'tool-abc123',
            'content': 'Search results: AI news...',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'tool_result'
        assert result['tool_use_id'] == 'tool-abc123'
        assert result['text_content'] == 'Search results: AI news...'
    
    def test_classify_tool_result_list(self, extractor):
        """Test classifying tool_result with list content."""
        part = {
            'type': 'tool_result',
            'tool_use_id': 'tool-abc123',
            'content': [
                {'text': 'First result'},
                {'text': 'Second result'},
            ],
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'tool_result'
        assert result['text_content'] == 'First result\nSecond result'
    
    def test_classify_tool_result_mixed_list(self, extractor):
        """Test classifying tool_result with mixed list content."""
        part = {
            'type': 'tool_result',
            'tool_use_id': 'tool-abc123',
            'content': [
                'Plain string',
                {'text': 'Dict with text'},
            ],
        }
        result = extractor._classify_content_part(part)
        
        assert result['text_content'] == 'Plain string\nDict with text'
    
    def test_classify_tool_result_error(self, extractor):
        """Test classifying tool_result with error flag."""
        part = {
            'type': 'tool_result',
            'tool_use_id': 'tool-abc123',
            'is_error': True,
            'content': 'Error: Something went wrong',
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'tool_result'
        assert result['is_error'] is True
    
    def test_classify_image(self, extractor):
        """Test classifying image content."""
        part = {
            'type': 'image',
            'media_type': 'image/png',
            'source': {'type': 'url', 'url': 'https://example.com/img.png'},
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'image'
        assert result['media_type'] == 'image/png'
        assert result['url'] == 'https://example.com/img.png'
    
    def test_classify_image_base64(self, extractor):
        """Test classifying base64 image (no URL)."""
        part = {
            'type': 'image',
            'media_type': 'image/jpeg',
            'source': {'type': 'base64', 'data': 'abc123...'},
        }
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'image'
        assert result['media_type'] == 'image/jpeg'
        assert 'url' not in result or result.get('url') is None
    
    def test_classify_unknown_type(self, extractor):
        """Test classifying unknown content type."""
        part = {'type': 'custom_widget', 'data': {'foo': 'bar'}}
        result = extractor._classify_content_part(part)
        
        assert result['part_type'] == 'custom_widget'



---
File: tests/unit/test_content_part_annotators.py
---
# tests/unit/test_content_part_annotators.py
"""Unit tests for content-part level annotators."""

import pytest
from datetime import datetime, timezone
from uuid import uuid4

from llm_archive.annotators.content_part import (
    ContentPartData,
    ContentPartAnnotator,
    CodeBlockAnnotator,
    ScriptHeaderAnnotator,
    LatexContentAnnotator,
    WikiLinkContentAnnotator,
    CONTENT_PART_ANNOTATORS,
)
from llm_archive.annotations.core import ValueType, EntityType


# ============================================================
# Test Fixtures
# ============================================================

@pytest.fixture
def content_part_id():
    """Generate a content-part ID."""
    return uuid4()


def make_content_part_data(
    text_content: str = "Test content",
    part_type: str = "text",
    language: str | None = None,
    role: str = "assistant",
    content_part_id: uuid4 = None,
) -> ContentPartData:
    """Create ContentPartData for testing."""
    return ContentPartData(
        content_part_id=content_part_id or uuid4(),
        message_id=uuid4(),
        dialogue_id=uuid4(),
        sequence=0,
        part_type=part_type,
        text_content=text_content,
        language=language,
        role=role,
        created_at=datetime.now(timezone.utc),
    )


# ============================================================
# CodeBlockAnnotator Tests
# ============================================================

class TestCodeBlockAnnotator:
    """Test code block detection at content-part level."""
    
    def test_detects_simple_code_block(self, content_part_id):
        """Should detect basic code blocks."""
        data = make_content_part_data(
            text_content="Here's some code:\n```\nprint('hello')\n```",
            content_part_id=content_part_id,
        )
        
        annotator = CodeBlockAnnotator.__new__(CodeBlockAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_code_block' for r in results)
        assert any(r.key == 'code_block_count' for r in results)
        
        count_result = next(r for r in results if r.key == 'code_block_count')
        assert count_result.value == 1
    
    def test_detects_code_block_with_language(self, content_part_id):
        """Should detect code blocks with language specification."""
        data = make_content_part_data(
            text_content="```python\ndef hello():\n    pass\n```",
            content_part_id=content_part_id,
        )
        
        annotator = CodeBlockAnnotator.__new__(CodeBlockAnnotator)
        results = annotator.annotate(data)
        
        lang_results = [r for r in results if r.key == 'code_language']
        assert len(lang_results) == 1
        assert lang_results[0].value == 'python'
    
    def test_counts_multiple_code_blocks(self, content_part_id):
        """Should count multiple code blocks."""
        data = make_content_part_data(
            text_content="```python\ncode1\n```\n\n```javascript\ncode2\n```\n\n```sql\ncode3\n```",
            content_part_id=content_part_id,
        )
        
        annotator = CodeBlockAnnotator.__new__(CodeBlockAnnotator)
        results = annotator.annotate(data)
        
        count_result = next(r for r in results if r.key == 'code_block_count')
        assert count_result.value == 3
        
        lang_results = [r for r in results if r.key == 'code_language']
        langs = {r.value for r in lang_results}
        assert langs == {'python', 'javascript', 'sql'}
    
    def test_no_code_blocks(self, content_part_id):
        """Should return empty for text without code blocks."""
        data = make_content_part_data(
            text_content="This is plain text without any code.",
            content_part_id=content_part_id,
        )
        
        annotator = CodeBlockAnnotator.__new__(CodeBlockAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_skips_non_text_parts(self, content_part_id):
        """Should only process text part_type."""
        # Note: The base class handles this via PART_TYPE_FILTER
        # but the annotate method should also be robust
        data = make_content_part_data(
            text_content="```code```",
            part_type="image",  # Not text
            content_part_id=content_part_id,
        )
        
        # The filter is applied in _iter_content_parts, not annotate
        # So annotate itself will still process, but in real use it won't be called
        annotator = CodeBlockAnnotator.__new__(CodeBlockAnnotator)
        # This should still work since text_content is provided
        results = annotator.annotate(data)
        assert any(r.key == 'has_code_block' for r in results)
    
    def test_empty_text_content(self, content_part_id):
        """Should handle empty text content."""
        data = make_content_part_data(
            text_content="",
            content_part_id=content_part_id,
        )
        
        annotator = CodeBlockAnnotator.__new__(CodeBlockAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_none_text_content(self, content_part_id):
        """Should handle None text content."""
        data = make_content_part_data(
            text_content="placeholder",
            content_part_id=content_part_id,
        )
        data.text_content = None
        
        annotator = CodeBlockAnnotator.__new__(CodeBlockAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0


# ============================================================
# ScriptHeaderAnnotator Tests
# ============================================================

class TestScriptHeaderAnnotator:
    """Test script header detection."""
    
    def test_detects_python_shebang(self, content_part_id):
        """Should detect Python shebang."""
        data = make_content_part_data(
            text_content="#!/usr/bin/env python3\nimport sys",
            content_part_id=content_part_id,
        )
        
        annotator = ScriptHeaderAnnotator.__new__(ScriptHeaderAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_script_header' for r in results)
        
        type_result = next(r for r in results if r.key == 'script_type')
        assert type_result.value == 'python3'
    
    def test_detects_bash_shebang(self, content_part_id):
        """Should detect Bash shebang."""
        data = make_content_part_data(
            text_content="#!/bin/bash\necho hello",
            content_part_id=content_part_id,
        )
        
        annotator = ScriptHeaderAnnotator.__new__(ScriptHeaderAnnotator)
        results = annotator.annotate(data)
        
        type_result = next(r for r in results if r.key == 'script_type')
        assert type_result.value == 'bash'
    
    def test_detects_c_include(self, content_part_id):
        """Should detect C/C++ includes."""
        data = make_content_part_data(
            text_content='#include <stdio.h>\nint main() {}',
            content_part_id=content_part_id,
        )
        
        annotator = ScriptHeaderAnnotator.__new__(ScriptHeaderAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_script_header' for r in results)
        
        type_result = next(r for r in results if r.key == 'script_type')
        assert type_result.value == 'c'
    
    def test_detects_c_include_quotes(self, content_part_id):
        """Should detect C includes with quotes."""
        data = make_content_part_data(
            text_content='#include "myheader.h"',
            content_part_id=content_part_id,
        )
        
        annotator = ScriptHeaderAnnotator.__new__(ScriptHeaderAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_script_header' for r in results)
    
    def test_detects_php_tag(self, content_part_id):
        """Should detect PHP opening tag."""
        data = make_content_part_data(
            text_content="<?php\necho 'Hello';",
            content_part_id=content_part_id,
        )
        
        annotator = ScriptHeaderAnnotator.__new__(ScriptHeaderAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_script_header' for r in results)
        
        type_result = next(r for r in results if r.key == 'script_type')
        assert type_result.value == 'php'
    
    def test_no_script_header(self, content_part_id):
        """Should not detect in plain text."""
        data = make_content_part_data(
            text_content="Just some plain text about programming.",
            content_part_id=content_part_id,
        )
        
        annotator = ScriptHeaderAnnotator.__new__(ScriptHeaderAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0


# ============================================================
# LatexContentAnnotator Tests
# ============================================================

class TestLatexContentAnnotator:
    """Test LaTeX detection at content-part level."""
    
    def test_detects_display_math(self, content_part_id):
        """Should detect $$ display math."""
        data = make_content_part_data(
            text_content="The equation is: $$E = mc^2$$",
            content_part_id=content_part_id,
        )
        
        annotator = LatexContentAnnotator.__new__(LatexContentAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_latex' for r in results)
        latex_types = {r.value for r in results if r.key == 'latex_type'}
        assert 'display' in latex_types
    
    def test_detects_inline_math(self, content_part_id):
        """Should detect inline $ math."""
        data = make_content_part_data(
            text_content="The value $x = 5$ is the solution.",
            content_part_id=content_part_id,
        )
        
        annotator = LatexContentAnnotator.__new__(LatexContentAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_latex' for r in results)
        latex_types = {r.value for r in results if r.key == 'latex_type'}
        assert 'inline' in latex_types
    
    def test_detects_latex_commands(self, content_part_id):
        """Should detect LaTeX commands."""
        data = make_content_part_data(
            text_content="Use \\frac{a}{b} for fractions.",
            content_part_id=content_part_id,
        )
        
        annotator = LatexContentAnnotator.__new__(LatexContentAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_latex' for r in results)
        latex_types = {r.value for r in results if r.key == 'latex_type'}
        assert 'commands' in latex_types
    
    def test_multiple_latex_types(self, content_part_id):
        """Should detect multiple LaTeX types."""
        data = make_content_part_data(
            text_content="Inline $x$ and display $$\\sum_{i=1}^n i$$",
            content_part_id=content_part_id,
        )
        
        annotator = LatexContentAnnotator.__new__(LatexContentAnnotator)
        results = annotator.annotate(data)
        
        latex_types = {r.value for r in results if r.key == 'latex_type'}
        assert len(latex_types) >= 2
    
    def test_no_latex(self, content_part_id):
        """Should not detect in plain text."""
        data = make_content_part_data(
            text_content="The price is $100 or maybe $200.",
            content_part_id=content_part_id,
        )
        
        annotator = LatexContentAnnotator.__new__(LatexContentAnnotator)
        results = annotator.annotate(data)
        
        # Single $ with numbers shouldn't match inline math pattern
        # (inline pattern requires non-$ chars inside)
        # But this test may be tricky - adjust based on actual behavior
        pass  # May or may not detect - depends on pattern specifics


# ============================================================
# WikiLinkContentAnnotator Tests
# ============================================================

class TestWikiLinkContentAnnotator:
    """Test wiki link detection at content-part level."""
    
    def test_detects_wiki_links(self, content_part_id):
        """Should detect [[wiki links]]."""
        data = make_content_part_data(
            text_content="The [[cat]] is a [[mammal]].",
            content_part_id=content_part_id,
        )
        
        annotator = WikiLinkContentAnnotator.__new__(WikiLinkContentAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_wiki_links' for r in results)
        
        count_result = next(r for r in results if r.key == 'wiki_link_count')
        assert count_result.value == 2
    
    def test_counts_many_wiki_links(self, content_part_id):
        """Should count multiple wiki links."""
        data = make_content_part_data(
            text_content="[[One]] [[Two]] [[Three]] [[Four]] [[Five]]",
            content_part_id=content_part_id,
        )
        
        annotator = WikiLinkContentAnnotator.__new__(WikiLinkContentAnnotator)
        results = annotator.annotate(data)
        
        count_result = next(r for r in results if r.key == 'wiki_link_count')
        assert count_result.value == 5
    
    def test_no_wiki_links(self, content_part_id):
        """Should not detect in plain text."""
        data = make_content_part_data(
            text_content="Regular text with [single brackets] and no wiki links.",
            content_part_id=content_part_id,
        )
        
        annotator = WikiLinkContentAnnotator.__new__(WikiLinkContentAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0


# ============================================================
# ContentPartAnnotator Base Class Tests
# ============================================================

class TestContentPartAnnotatorBase:
    """Test base class attributes and behavior."""
    
    def test_entity_type(self):
        """All content-part annotators should use CONTENT_PART entity type."""
        for annotator_cls in CONTENT_PART_ANNOTATORS:
            assert annotator_cls.ENTITY_TYPE == EntityType.CONTENT_PART
    
    def test_annotators_have_annotation_key(self):
        """All annotators should have ANNOTATION_KEY defined."""
        for annotator_cls in CONTENT_PART_ANNOTATORS:
            assert annotator_cls.ANNOTATION_KEY, f"{annotator_cls.__name__} missing ANNOTATION_KEY"
    
    def test_annotators_have_priority(self):
        """All annotators should have PRIORITY defined."""
        for annotator_cls in CONTENT_PART_ANNOTATORS:
            assert hasattr(annotator_cls, 'PRIORITY')
            assert isinstance(annotator_cls.PRIORITY, int)


# ============================================================
# Registry Tests
# ============================================================

class TestContentPartAnnotatorRegistry:
    """Test the content-part annotator registry."""
    
    def test_all_annotators_in_registry(self):
        """All annotators should be in CONTENT_PART_ANNOTATORS."""
        assert CodeBlockAnnotator in CONTENT_PART_ANNOTATORS
        assert ScriptHeaderAnnotator in CONTENT_PART_ANNOTATORS
        assert LatexContentAnnotator in CONTENT_PART_ANNOTATORS
        assert WikiLinkContentAnnotator in CONTENT_PART_ANNOTATORS
    
    def test_registry_count(self):
        """Registry should have expected number of annotators."""
        assert len(CONTENT_PART_ANNOTATORS) == 4



---
File: tests/unit/test_extractor_utils.py
---
# tests/unit/test_extractor_utils.py
"""Unit tests for extractor utility functions."""

import pytest
from datetime import datetime, timezone

from llm_archive.extractors.base import parse_timestamp, normalize_role, safe_get, compute_content_hash


class TestParseTimestamp:
    """Tests for timestamp parsing."""
    
    def test_parse_epoch_int(self):
        """Test parsing integer epoch timestamp."""
        result = parse_timestamp(1700000000)
        assert result is not None
        assert result.tzinfo is not None
        assert result.year == 2023
    
    def test_parse_epoch_float(self):
        """Test parsing float epoch timestamp."""
        result = parse_timestamp(1700000000.123)
        assert result is not None
        assert result.tzinfo is not None
    
    def test_parse_iso_string(self):
        """Test parsing ISO 8601 string."""
        result = parse_timestamp("2024-01-15T10:00:00Z")
        assert result is not None
        assert result.tzinfo is not None
        assert result.year == 2024
        assert result.month == 1
        assert result.day == 15
    
    def test_parse_iso_string_with_offset(self):
        """Test parsing ISO 8601 with timezone offset."""
        result = parse_timestamp("2024-01-15T10:00:00+05:00")
        assert result is not None
        assert result.tzinfo is not None
    
    def test_parse_none(self):
        """Test parsing None returns None."""
        result = parse_timestamp(None)
        assert result is None
    
    def test_parse_invalid_string(self):
        """Test parsing invalid string returns None."""
        result = parse_timestamp("not a timestamp")
        assert result is None
    
    def test_parse_negative_epoch(self):
        """Test parsing negative epoch (before 1970)."""
        result = parse_timestamp(-1000000)
        assert result is not None
        assert result.year < 1970


class TestNormalizeRole:
    """Tests for role normalization."""
    
    def test_normalize_user(self):
        """Test 'user' stays 'user'."""
        assert normalize_role("user", "chatgpt") == "user"
    
    def test_normalize_assistant(self):
        """Test 'assistant' stays 'assistant'."""
        assert normalize_role("assistant", "chatgpt") == "assistant"
    
    def test_normalize_human_to_user(self):
        """Test 'human' becomes 'user' (Claude format)."""
        assert normalize_role("human", "claude") == "user"
    
    def test_normalize_human_uppercase(self):
        """Test uppercase 'HUMAN' becomes 'user'."""
        assert normalize_role("HUMAN", "claude") == "user"
    
    def test_normalize_system(self):
        """Test 'system' stays 'system'."""
        assert normalize_role("system", "chatgpt") == "system"
    
    def test_normalize_none(self):
        """Test None becomes 'unknown'."""
        assert normalize_role(None, "chatgpt") == "unknown"


class TestSafeGet:
    """Tests for safe dictionary traversal."""
    
    def test_simple_get(self):
        """Test simple key access."""
        data = {"key": "value"}
        assert safe_get(data, "key") == "value"
    
    def test_nested_get(self):
        """Test nested key access."""
        data = {"level1": {"level2": {"level3": "value"}}}
        assert safe_get(data, "level1", "level2", "level3") == "value"
    
    def test_missing_key(self):
        """Test missing key returns default."""
        data = {"key": "value"}
        assert safe_get(data, "missing") is None
        assert safe_get(data, "missing", default="default") == "default"
    
    def test_missing_nested_key(self):
        """Test missing nested key returns default."""
        data = {"level1": {"level2": "value"}}
        assert safe_get(data, "level1", "level2", "level3") is None
    
    def test_non_dict_intermediate(self):
        """Test non-dict intermediate value returns default."""
        data = {"level1": "not a dict"}
        assert safe_get(data, "level1", "level2") is None
    
    def test_none_intermediate(self):
        """Test None intermediate value returns default."""
        data = {"level1": None}
        assert safe_get(data, "level1", "level2") is None


class TestTimestampEdgeCases:
    """Edge case tests for timestamp parsing."""
    
    def test_zero_epoch(self):
        """Test epoch 0 (1970-01-01)."""
        result = parse_timestamp(0)
        assert result is not None
        assert result.year == 1970
    
    def test_very_large_epoch(self):
        """Test very large epoch value."""
        # Year 2100
        result = parse_timestamp(4102444800)
        assert result is not None
        assert result.year == 2100
    
    def test_iso_without_timezone(self):
        """Test ISO string without timezone gets UTC."""
        result = parse_timestamp("2024-01-15T10:00:00")
        assert result is not None
        assert result.tzinfo is not None


class TestComputeContentHash:
    """Tests for content hash computation."""
    
    def test_hash_dict(self):
        """Test hashing a dictionary."""
        data = {'text': 'Hello world', 'role': 'user'}
        result = compute_content_hash(data)
        
        assert result is not None
        assert len(result) == 64  # SHA-256 hex string
    
    def test_hash_string(self):
        """Test hashing a plain string."""
        result = compute_content_hash('Hello world')
        
        assert result is not None
        assert len(result) == 64
    
    def test_hash_is_deterministic(self):
        """Test that same content produces same hash."""
        data = {'message': 'test', 'value': 123}
        
        hash1 = compute_content_hash(data)
        hash2 = compute_content_hash(data)
        
        assert hash1 == hash2
    
    def test_hash_is_order_independent(self):
        """Test that key order doesn't affect hash."""
        data1 = {'a': 1, 'b': 2}
        data2 = {'b': 2, 'a': 1}
        
        hash1 = compute_content_hash(data1)
        hash2 = compute_content_hash(data2)
        
        assert hash1 == hash2
    
    def test_different_content_different_hash(self):
        """Test that different content produces different hash."""
        data1 = {'text': 'Hello'}
        data2 = {'text': 'World'}
        
        hash1 = compute_content_hash(data1)
        hash2 = compute_content_hash(data2)
        
        assert hash1 != hash2
    
    def test_hash_nested_dict(self):
        """Test hashing nested dictionary."""
        data = {
            'content': {
                'parts': ['Hello', {'type': 'code', 'text': 'print(1)'}]
            },
            'metadata': {'author': 'user'}
        }
        
        result = compute_content_hash(data)
        assert len(result) == 64
    
    def test_hash_list(self):
        """Test hashing a list."""
        data = [{'text': 'message 1'}, {'text': 'message 2'}]
        
        result = compute_content_hash(data)
        assert len(result) == 64



---
File: tests/unit/test_models.py
---
# tests/unit/test_models.py
"""Unit tests for SQLAlchemy models - no database required."""

import pytest
from uuid import uuid4
from datetime import datetime, timezone

from llm_archive.models import (
    Dialogue,
    Message,
    ContentPart,
)


class TestDialogueModel:
    """Tests for Dialogue model instantiation."""
    
    def test_create_dialogue_instance(self):
        """Test creating a Dialogue instance."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='conv-001',
            title='Test Conversation',
            source_json={'test': True},
        )
        
        assert dialogue.source == 'chatgpt'
        assert dialogue.source_id == 'conv-001'
        assert dialogue.title == 'Test Conversation'
        assert dialogue.source_json == {'test': True}
    
    def test_dialogue_with_timestamps(self):
        """Test Dialogue with timestamp fields."""
        now = datetime.now(timezone.utc)
        dialogue = Dialogue(
            source='claude',
            source_id='conv-002',
            created_at=now,
            updated_at=now,
            source_json={},
        )
        
        assert dialogue.created_at == now
        assert dialogue.updated_at == now
    
    def test_dialogue_minimal_fields(self):
        """Test Dialogue with only required fields."""
        dialogue = Dialogue(
            source='chatgpt',
            source_id='conv-003',
            source_json={},
        )
        
        assert dialogue.source == 'chatgpt'
        assert dialogue.source_id == 'conv-003'
        assert dialogue.title is None
        assert dialogue.created_at is None


class TestMessageModel:
    """Tests for Message model instantiation."""
    
    def test_create_message_instance(self):
        """Test creating a Message instance."""
        dialogue_id = uuid4()
        message = Message(
            dialogue_id=dialogue_id,
            source_id='msg-001',
            role='user',
            source_json={'content': 'Hello'},
        )
        
        assert message.dialogue_id == dialogue_id
        assert message.source_id == 'msg-001'
        assert message.role == 'user'
        assert message.source_json == {'content': 'Hello'}
    
    def test_message_with_parent(self):
        """Test Message with parent reference."""
        dialogue_id = uuid4()
        parent_id = uuid4()
        
        message = Message(
            dialogue_id=dialogue_id,
            source_id='msg-002',
            role='assistant',
            parent_id=parent_id,
            source_json={},
        )
        
        assert message.parent_id == parent_id
    
    def test_message_with_author(self):
        """Test Message with author fields."""
        message = Message(
            dialogue_id=uuid4(),
            source_id='msg-003',
            role='user',
            author_id='user-123',
            author_name='John Doe',
            source_json={},
        )
        
        assert message.author_id == 'user-123'
        assert message.author_name == 'John Doe'
    
    def test_message_with_content_hash(self):
        """Test Message with content hash for change detection."""
        message = Message(
            dialogue_id=uuid4(),
            source_id='msg-004',
            role='user',
            content_hash='a' * 64,  # SHA-256 hash
            source_json={},
        )
        
        assert message.content_hash == 'a' * 64
    
    def test_message_with_deleted_at(self):
        """Test Message with soft delete timestamp."""
        now = datetime.now(timezone.utc)
        message = Message(
            dialogue_id=uuid4(),
            source_id='msg-005',
            role='user',
            deleted_at=now,
            source_json={},
        )
        
        assert message.deleted_at == now
    
    def test_message_not_deleted_by_default(self):
        """Test that deleted_at is None by default."""
        message = Message(
            dialogue_id=uuid4(),
            source_id='msg-006',
            role='user',
            source_json={},
        )
        
        assert message.deleted_at is None


class TestContentPartModel:
    """Tests for ContentPart model instantiation."""
    
    def test_create_text_content_part(self):
        """Test creating a text ContentPart."""
        message_id = uuid4()
        part = ContentPart(
            message_id=message_id,
            sequence=0,
            part_type='text',
            text_content='Hello, world!',
            source_json={'type': 'text'},
        )
        
        assert part.message_id == message_id
        assert part.sequence == 0
        assert part.part_type == 'text'
        assert part.text_content == 'Hello, world!'
    
    def test_create_code_content_part(self):
        """Test creating a code ContentPart with language."""
        part = ContentPart(
            message_id=uuid4(),
            sequence=1,
            part_type='code',
            text_content='print("hello")',
            language='python',
            source_json={'type': 'code', 'language': 'python'},
        )
        
        assert part.part_type == 'code'
        assert part.language == 'python'
        assert part.text_content == 'print("hello")'
    
    def test_create_image_content_part(self):
        """Test creating an image ContentPart with media type and URL."""
        part = ContentPart(
            message_id=uuid4(),
            sequence=0,
            part_type='image',
            media_type='image/png',
            url='https://example.com/image.png',
            source_json={'type': 'image'},
        )
        
        assert part.part_type == 'image'
        assert part.media_type == 'image/png'
        assert part.url == 'https://example.com/image.png'
    
    def test_create_tool_use_content_part(self):
        """Test creating a tool_use ContentPart."""
        part = ContentPart(
            message_id=uuid4(),
            sequence=0,
            part_type='tool_use',
            tool_name='web_search',
            tool_use_id='tool-123',
            tool_input={'query': 'test search'},
            source_json={'type': 'tool_use'},
        )
        
        assert part.part_type == 'tool_use'
        assert part.tool_name == 'web_search'
        assert part.tool_use_id == 'tool-123'
        assert part.tool_input == {'query': 'test search'}
    
    def test_create_tool_result_content_part(self):
        """Test creating a tool_result ContentPart."""
        part = ContentPart(
            message_id=uuid4(),
            sequence=1,
            part_type='tool_result',
            tool_use_id='tool-123',
            text_content='Search results: ...',
            is_error=False,
            source_json={'type': 'tool_result'},
        )
        
        assert part.part_type == 'tool_result'
        assert part.tool_use_id == 'tool-123'
        assert part.is_error is False


class TestModelTableNames:
    """Tests for model table name configuration."""
    
    def test_dialogue_table_name(self):
        """Test Dialogue uses raw schema."""
        assert Dialogue.__tablename__ == 'dialogues'
        assert Dialogue.__table__.schema == 'raw'
    
    def test_message_table_name(self):
        """Test Message uses raw schema."""
        assert Message.__tablename__ == 'messages'
        assert Message.__table__.schema == 'raw'
    
    def test_content_part_table_name(self):
        """Test ContentPart uses raw schema."""
        assert ContentPart.__tablename__ == 'content_parts'
        assert ContentPart.__table__.schema == 'raw'
    



---
File: tests/unit/test_prompt_response.py
---
# tests/unit/test_prompt_response.py
"""Unit tests for prompt-response builders and annotators."""

import pytest
from datetime import datetime, timezone
from uuid import uuid4

from llm_archive.annotators.prompt_response import (
    PromptResponseData,
    PromptResponseAnnotator,
    WikiCandidateAnnotator,
    NaiveTitleAnnotator,
    HasCodeAnnotator,
    HasLatexAnnotator,
)
from llm_archive.annotations.core import ValueType, EntityType, AnnotationResult


# ============================================================
# Test Fixtures
# ============================================================

@pytest.fixture
def pr_id():
    """Generate a prompt-response ID."""
    return uuid4()


def make_pr_data(
    prompt_text: str = "Test prompt",
    response_text: str = "Test response",
    pr_id: uuid4 = None,
    response_role: str = 'assistant',
    prompt_role: str = 'user',
) -> PromptResponseData:
    """Create PromptResponseData for testing."""
    return PromptResponseData(
        prompt_response_id=pr_id or uuid4(),
        dialogue_id=uuid4(),
        prompt_message_id=uuid4(),
        response_message_id=uuid4(),
        prompt_text=prompt_text,
        response_text=response_text,
        prompt_word_count=len(prompt_text.split()) if prompt_text else 0,
        response_word_count=len(response_text.split()) if response_text else 0,
        prompt_role=prompt_role,
        response_role=response_role,
        created_at=datetime.now(timezone.utc),
    )


# ============================================================
# WikiCandidateAnnotator Tests
# ============================================================

class TestWikiCandidateAnnotator:
    """Test wiki article detection."""
    
    def test_detects_wiki_links(self, pr_id):
        """Should detect responses with wiki links."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="# Cats\n\nCats are [[mammals]] that are [[domesticated]].",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        # Should have both exchange_type and wiki_link_count annotations
        assert len(results) == 2
        
        exchange_type_result = next(r for r in results if r.key == 'exchange_type')
        assert exchange_type_result.value == 'wiki_article'
        assert exchange_type_result.value_type == ValueType.STRING
        assert exchange_type_result.reason == 'wiki_links_detected'
        
        count_result = next(r for r in results if r.key == 'wiki_link_count')
        assert count_result.value == 2
        assert count_result.value_type == ValueType.NUMERIC
    
    def test_high_confidence_multiple_links(self, pr_id):
        """Should have higher confidence with 3+ wiki links."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="[[Cats]] are [[mammals]]. They eat [[mice]] and [[birds]].",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        exchange_type_result = next(r for r in results if r.key == 'exchange_type')
        assert exchange_type_result.confidence == 0.95
    
    def test_lower_confidence_single_link(self, pr_id):
        """Should have lower confidence with just 1-2 links."""
        data = make_pr_data(
            prompt_text="Tell me about cats",
            response_text="Cats are [[mammals]].",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        exchange_type_result = next(r for r in results if r.key == 'exchange_type')
        assert exchange_type_result.confidence == 0.8
    
    def test_no_wiki_links(self, pr_id):
        """Should not detect if no wiki links."""
        data = make_pr_data(
            prompt_text="Tell me about cats",
            response_text="Cats are mammals. They are cute.",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_skips_non_assistant(self, pr_id):
        """Should skip non-assistant responses."""
        data = make_pr_data(
            prompt_text="Write [[wiki]] style",
            response_text="Here's [[content]]",
            pr_id=pr_id,
            response_role='user',  # Not assistant
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_counts_links_correctly(self, pr_id):
        """Should count wiki links correctly."""
        data = make_pr_data(
            response_text="[[One]] [[Two]] [[Three]] [[Four]] [[Five]]",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        count_result = next(r for r in results if r.key == 'wiki_link_count')
        assert count_result.value == 5
    
    def test_handles_empty_brackets(self, pr_id):
        """Should count empty brackets as potential links."""
        data = make_pr_data(
            response_text="Empty [[]] brackets and [[valid]] link",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        count_result = next(r for r in results if r.key == 'wiki_link_count')
        assert count_result.value == 2


# ============================================================
# NaiveTitleAnnotator Tests
# ============================================================

class TestNaiveTitleAnnotator:
    """Test naive title extraction."""
    
    def test_extracts_markdown_h1(self, pr_id):
        """Should extract # Title."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="# The Domestic Cat\n\n[[Cats]] are mammals...",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 1
        assert results[0].value == 'The Domestic Cat'
        assert results[0].key == 'proposed_title'
        assert results[0].value_type == ValueType.STRING
        assert results[0].reason == 'markdown_header'
    
    def test_extracts_markdown_h2(self, pr_id):
        """Should extract ## Title."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="## Feline History\n\nThe history of cats...",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 1
        assert results[0].value == 'Feline History'
        assert results[0].reason == 'markdown_header'
    
    def test_extracts_markdown_h3(self, pr_id):
        """Should extract ### Title."""
        data = make_pr_data(
            response_text="### Deep Section\n\nContent here...",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 1
        assert results[0].value == 'Deep Section'
    
    def test_extracts_bold_title(self, pr_id):
        """Should extract **Title**."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="**The Domestic Cat**\n\n[[Cats]] are mammals...",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 1
        assert results[0].value == 'The Domestic Cat'
        assert results[0].reason == 'bold_header'
    
    def test_extracts_bold_with_subtitle(self, pr_id):
        """Should extract **Title** - Subtitle pattern."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="**Felis catus** - The Domestic Cat\n\nContent...",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 1
        assert results[0].value == 'Felis catus'
        assert results[0].reason == 'bold_header_with_suffix'
    
    def test_no_title_preamble(self, pr_id):
        """Should return nothing if first line is preamble."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="Sure, here's an article about cats:\n\n# The Domestic Cat\n\n...",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        # This is expected - naive extractor misses the title
        # because first line is a preamble
        assert len(results) == 0
    
    def test_no_title_plain_text(self, pr_id):
        """Should return nothing if no clear title format."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="Cats have been domesticated for thousands of years...",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_skips_non_assistant(self, pr_id):
        """Should skip non-assistant responses."""
        data = make_pr_data(
            prompt_text="# My Title",
            response_text="# Another Title",
            pr_id=pr_id,
            response_role='user',
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_empty_response(self, pr_id):
        """Should handle empty response."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_none_response(self, pr_id):
        """Should handle None response."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="placeholder",
            pr_id=pr_id,
        )
        data.response_text = None  # Override
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_whitespace_only_first_line(self, pr_id):
        """Should skip whitespace-only first lines."""
        data = make_pr_data(
            response_text="   \n# Real Title\n\nContent",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        # .strip() is called on the document, so the title should be extracted.
        assert len(results) == 1
    
    def test_strips_title_whitespace(self, pr_id):
        """Should strip whitespace from extracted title."""
        data = make_pr_data(
            response_text="#   Spaced Title   \n\nContent",
            pr_id=pr_id,
        )
        
        annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 1
        assert results[0].value == 'Spaced Title'


# ============================================================
# Annotation Filter Tests (class attributes)
# ============================================================

class TestAnnotatorFilters:
    """Test annotation filter attributes."""
    
    def test_wiki_candidate_has_no_requirements(self):
        """WikiCandidateAnnotator should have no prerequisites."""
        assert WikiCandidateAnnotator.REQUIRES_FLAGS == []
        assert WikiCandidateAnnotator.REQUIRES_STRINGS == []
        assert WikiCandidateAnnotator.SKIP_IF_FLAGS == []
        assert WikiCandidateAnnotator.SKIP_IF_STRINGS == []
    
    def test_naive_title_requires_wiki(self):
        """NaiveTitleAnnotator should require wiki_article."""
        assert ('exchange_type', 'wiki_article') in NaiveTitleAnnotator.REQUIRES_STRINGS
    
    def test_annotator_metadata(self):
        """Check annotator class metadata."""
        assert WikiCandidateAnnotator.ENTITY_TYPE == EntityType.PROMPT_RESPONSE
        assert WikiCandidateAnnotator.ANNOTATION_KEY == 'exchange_type'
        assert WikiCandidateAnnotator.VALUE_TYPE == ValueType.STRING
        
        assert NaiveTitleAnnotator.ENTITY_TYPE == EntityType.PROMPT_RESPONSE
        assert NaiveTitleAnnotator.ANNOTATION_KEY == 'proposed_title'
        assert NaiveTitleAnnotator.VALUE_TYPE == ValueType.STRING
        
        # Wiki detection should run before title extraction
        assert WikiCandidateAnnotator.PRIORITY > NaiveTitleAnnotator.PRIORITY
    
    def test_custom_annotator_with_filters(self):
        """Test defining custom annotator with filters."""
        
        class PreambleDetector(PromptResponseAnnotator):
            ANNOTATION_KEY = 'has_preamble'
            VALUE_TYPE = ValueType.FLAG
            REQUIRES_STRINGS = [('exchange_type', 'wiki_article')]
            SKIP_IF_FLAGS = ['preamble_checked']
            
            def annotate(self, data):
                return []
        
        assert PreambleDetector.REQUIRES_STRINGS == [('exchange_type', 'wiki_article')]
        assert PreambleDetector.SKIP_IF_FLAGS == ['preamble_checked']


# ============================================================
# AnnotationResult Tests
# ============================================================

class TestAnnotationResult:
    """Test AnnotationResult dataclass behavior."""
    
    def test_result_with_reason(self, pr_id):
        """Results should include reason when provided."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="# Cats\n\n[[Cats]] are mammals.",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        exchange_type_result = next(r for r in results if r.key == 'exchange_type')
        assert exchange_type_result.reason == 'wiki_links_detected'
    
    def test_key_is_required(self, pr_id):
        """Key should always be set on results."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="# Title\n\n[[link]]",
            pr_id=pr_id,
        )
        
        wiki_annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        wiki_results = wiki_annotator.annotate(data)
        
        title_annotator = NaiveTitleAnnotator.__new__(NaiveTitleAnnotator)
        title_results = title_annotator.annotate(data)
        
        for result in wiki_results + title_results:
            assert result.key is not None
    
    def test_value_type_is_set(self, pr_id):
        """Results should have explicit value_type."""
        data = make_pr_data(
            prompt_text="Write about cats",
            response_text="# Title\n\n[[link1]] [[link2]] [[link3]]",
            pr_id=pr_id,
        )
        
        annotator = WikiCandidateAnnotator.__new__(WikiCandidateAnnotator)
        results = annotator.annotate(data)
        
        # Should have string and numeric results
        string_results = [r for r in results if r.value_type == ValueType.STRING]
        numeric_results = [r for r in results if r.value_type == ValueType.NUMERIC]
        
        assert len(string_results) >= 1
        assert len(numeric_results) >= 1


# ============================================================
# PromptResponseData Tests
# ============================================================

class TestPromptResponseData:
    """Test PromptResponseData dataclass."""
    
    def test_all_fields_accessible(self):
        """All fields should be accessible."""
        data = make_pr_data(
            prompt_text="Hello",
            response_text="World",
        )
        
        assert data.prompt_text == "Hello"
        assert data.response_text == "World"
        assert data.prompt_role == 'user'
        assert data.response_role == 'assistant'
        assert isinstance(data.prompt_response_id, type(uuid4()))
        assert isinstance(data.dialogue_id, type(uuid4()))
    
    def test_word_counts_calculated(self):
        """Word counts should be calculated from text."""
        data = make_pr_data(
            prompt_text="one two three",
            response_text="four five six seven",
        )
        
        assert data.prompt_word_count == 3
        assert data.response_word_count == 4
    
    def test_handles_none_text(self):
        """Should handle None text gracefully."""
        data = PromptResponseData(
            prompt_response_id=uuid4(),
            dialogue_id=uuid4(),
            prompt_message_id=uuid4(),
            response_message_id=uuid4(),
            prompt_text=None,
            response_text=None,
            prompt_word_count=0,
            response_word_count=0,
            prompt_role='user',
            response_role='assistant',
            created_at=datetime.now(timezone.utc),
        )
        
        assert data.prompt_text is None
        assert data.response_text is None


# ============================================================
# HasCodeAnnotator Tests
# ============================================================

class TestHasCodeAnnotator:
    """Test code detection annotator."""
    
    def test_detects_code_blocks(self, pr_id):
        """Should detect ``` code blocks."""
        data = make_pr_data(
            response_text="Here's some code:\n```python\nprint('hello')\n```",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        flag_results = [r for r in results if r.key == 'has_code']
        assert len(flag_results) == 1
        assert flag_results[0].value_type == ValueType.FLAG
        assert flag_results[0].confidence >= 0.9
        
        evidence_results = [r for r in results if r.key == 'code_evidence']
        evidence_values = {r.value for r in evidence_results}
        assert 'code_block' in evidence_values
    
    def test_detects_shebang(self, pr_id):
        """Should detect shebang lines."""
        data = make_pr_data(
            response_text="#!/usr/bin/env python\nprint('hello')",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_code' for r in results)
        evidence_values = {r.value for r in results if r.key == 'code_evidence'}
        assert 'shebang' in evidence_values
    
    def test_detects_c_include(self, pr_id):
        """Should detect C/C++ includes."""
        data = make_pr_data(
            response_text='#include <stdio.h>\nint main() { return 0; }',
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_code' for r in results)
        evidence_values = {r.value for r in results if r.key == 'code_evidence'}
        assert 'c_include' in evidence_values
    
    def test_detects_python_function(self, pr_id):
        """Should detect Python function definitions."""
        data = make_pr_data(
            response_text="def hello_world():\n    print('hello')",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_code' for r in results)
        evidence_values = {r.value for r in results if r.key == 'code_evidence'}
        assert 'python_function' in evidence_values
    
    def test_detects_js_function(self, pr_id):
        """Should detect JavaScript function definitions."""
        data = make_pr_data(
            response_text="function hello() {\n  console.log('hello');\n}",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_code' for r in results)
        evidence_values = {r.value for r in results if r.key == 'code_evidence'}
        assert 'js_function' in evidence_values
    
    def test_detects_arrow_function(self, pr_id):
        """Should detect arrow functions."""
        data = make_pr_data(
            response_text="const hello = (name) => console.log(`Hello ${name}`);",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_code' for r in results)
        evidence_values = {r.value for r in results if r.key == 'code_evidence'}
        assert 'arrow_function' in evidence_values
    
    def test_detects_python_import(self, pr_id):
        """Should detect Python imports."""
        data = make_pr_data(
            response_text="import pandas as pd\nfrom datetime import datetime",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_code' for r in results)
        evidence_values = {r.value for r in results if r.key == 'code_evidence'}
        assert 'python_import' in evidence_values
    
    def test_no_code_in_plain_text(self, pr_id):
        """Should not detect code in plain text."""
        data = make_pr_data(
            response_text="Cats are wonderful pets. They like to sleep and play.",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_skips_non_assistant(self, pr_id):
        """Should skip non-assistant responses."""
        data = make_pr_data(
            response_text="```python\nprint('hello')\n```",
            pr_id=pr_id,
            response_role='user',
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_multiple_evidence_types(self, pr_id):
        """Should detect multiple evidence types."""
        data = make_pr_data(
            response_text="```python\nimport os\ndef main():\n    pass\n```",
            pr_id=pr_id,
        )
        
        annotator = HasCodeAnnotator.__new__(HasCodeAnnotator)
        results = annotator.annotate(data)
        
        evidence_results = [r for r in results if r.key == 'code_evidence']
        assert len(evidence_results) >= 2  # code_block + python_function + python_import


# ============================================================
# HasLatexAnnotator Tests
# ============================================================

class TestHasLatexAnnotator:
    """Test LaTeX detection annotator."""
    
    def test_detects_display_math(self, pr_id):
        """Should detect display math $$...$$."""
        data = make_pr_data(
            response_text="The equation is: $$E = mc^2$$",
            pr_id=pr_id,
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_latex' for r in results)
        latex_types = {r.value for r in results if r.key == 'latex_type'}
        assert 'display' in latex_types
    
    def test_detects_bracket_display_math(self, pr_id):
        """Should detect \\[...\\] display math."""
        data = make_pr_data(
            response_text="The integral is: \\[\\int_0^\\infty e^{-x} dx = 1\\]",
            pr_id=pr_id,
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_latex' for r in results)
    
    def test_detects_latex_commands(self, pr_id):
        """Should detect LaTeX commands."""
        data = make_pr_data(
            response_text="Use \\frac{a}{b} for fractions and \\sqrt{x} for roots.",
            pr_id=pr_id,
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_latex' for r in results)
        latex_types = {r.value for r in results if r.key == 'latex_type'}
        assert 'commands' in latex_types
    
    def test_detects_greek_letters(self, pr_id):
        """Should detect Greek letter commands."""
        data = make_pr_data(
            response_text="The angle \\theta is measured from \\alpha to \\omega.",
            pr_id=pr_id,
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        assert any(r.key == 'has_latex' for r in results)
    
    def test_no_latex_in_plain_text(self, pr_id):
        """Should not detect LaTeX in plain text."""
        data = make_pr_data(
            response_text="The value of pi is approximately 3.14159.",
            pr_id=pr_id,
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_skips_non_assistant(self, pr_id):
        """Should skip non-assistant responses."""
        data = make_pr_data(
            response_text="$$E = mc^2$$",
            pr_id=pr_id,
            response_role='user',
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        assert len(results) == 0
    
    def test_high_confidence_for_display_math(self, pr_id):
        """Should have high confidence for display math."""
        data = make_pr_data(
            response_text="$$\\sum_{i=1}^n i = \\frac{n(n+1)}{2}$$",
            pr_id=pr_id,
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        flag_result = next(r for r in results if r.key == 'has_latex')
        assert flag_result.confidence >= 0.9
    
    def test_lower_confidence_for_commands_only(self, pr_id):
        """Should have lower confidence for commands without display math."""
        data = make_pr_data(
            response_text="Use \\alpha and \\beta for parameters.",
            pr_id=pr_id,
        )
        
        annotator = HasLatexAnnotator.__new__(HasLatexAnnotator)
        results = annotator.annotate(data)
        
        flag_result = next(r for r in results if r.key == 'has_latex')
        assert flag_result.confidence < 0.9


# ============================================================
# Annotator Registry Tests
# ============================================================

class TestAnnotatorRegistry:
    """Test the annotator registry and runner."""
    
    def test_all_annotators_in_registry(self):
        """All annotators should be in PROMPT_RESPONSE_ANNOTATORS."""
        from llm_archive.annotators.prompt_response import PROMPT_RESPONSE_ANNOTATORS
        
        assert WikiCandidateAnnotator in PROMPT_RESPONSE_ANNOTATORS
        assert NaiveTitleAnnotator in PROMPT_RESPONSE_ANNOTATORS
        assert HasCodeAnnotator in PROMPT_RESPONSE_ANNOTATORS
        assert HasLatexAnnotator in PROMPT_RESPONSE_ANNOTATORS
    
    def test_annotators_have_unique_keys(self):
        """Each annotator should have a unique ANNOTATION_KEY."""
        from llm_archive.annotators.prompt_response import PROMPT_RESPONSE_ANNOTATORS
        
        keys = [cls.ANNOTATION_KEY for cls in PROMPT_RESPONSE_ANNOTATORS]
        assert len(keys) == len(set(keys)), "Duplicate ANNOTATION_KEYs found"
    
    def test_priority_ordering(self):
        """Annotators should have distinct priorities for deterministic ordering."""
        from llm_archive.annotators.prompt_response import PROMPT_RESPONSE_ANNOTATORS
        
        priorities = [cls.PRIORITY for cls in PROMPT_RESPONSE_ANNOTATORS]
        # Note: Priorities don't have to be unique, but it helps with debugging
        assert len(priorities) == len(PROMPT_RESPONSE_ANNOTATORS)


